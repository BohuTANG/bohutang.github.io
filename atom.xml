<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>虎哥的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://bohutang.me/"/>
  <updated>2020-09-04T03:10:18.270Z</updated>
  <id>https://bohutang.me/</id>
  
  <author>
    <name>BohuTANG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【置顶】ClickHouse MaterializeMySQL实时同步MySQL汇总</title>
    <link href="https://bohutang.me/3030/12/12/clickhouse-and-friends-mysql-replication-materializemysql/"/>
    <id>https://bohutang.me/3030/12/12/clickhouse-and-friends-mysql-replication-materializemysql/</id>
    <published>3030-12-11T16:00:00.000Z</published>
    <updated>2020-09-04T03:10:18.270Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-04</b></p><h2 id="1-如何下载"><a href="#1-如何下载" class="headerlink" title="1. 如何下载"></a><b>1. 如何下载</b></h2><p>体验版将在 20.8 stable 发布。</p><p>目前任何 master 分支的 build 都可以试用，比如 <a href="https://clickhouse-builds.s3.yandex.net/0/2b8ad576cc3892d2d760f3f8b670adf17db0c2a0/clickhouse_build_check/report.html" target="_blank" rel="noopener">ClickHouse Build Check for master-20.9.1</a> 下载安装。</p><h2 id="2-如何使用"><a href="#2-如何使用" class="headerlink" title="2, 如何使用"></a><b>2, 如何使用</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE ckdb ENGINE &#x3D; MaterializeMySQL(&#39;[mysql-host]:[mysql-port]&#39;, &#39;[mysql-database]&#39;, &#39;[mysql-user]&#39;, &#39;[mysql-password]&#39;);</span><br></pre></td></tr></table></figure><h2 id="3-更新汇总"><a href="#3-更新汇总" class="headerlink" title="3. 更新汇总"></a><b>3. 更新汇总</b></h2><ul><li><input disabled="" type="checkbox"> 支持 decimal 类型 <a href="https://github.com/ClickHouse/ClickHouse/pull/11512" target="_blank" rel="noopener">WIP PR#11512</a></li><li><input disabled="" type="checkbox"> 支持 DATETIME(x)  fractional seconds precision [WIP]</li><li><input disabled="" type="checkbox"> 支持 YEAR, TIME 类型 (TODO)</li><li><input disabled="" type="checkbox"> 支持 DROP TABLE database.table, table_2, … 语法  (TODO)</li><li><input disabled="" type="checkbox"> 支持 gc thread 清理已经 deleted 的数据  (TODO)</li><li><input disabled="" type="checkbox"> 支持复制状态查询 (TODO)</li></ul><h2 id="4-遇到问题"><a href="#4-遇到问题" class="headerlink" title=" 4. 遇到问题"></a><b> 4. 遇到问题</b></h2><ul><li><p><a href="https://github.com/ClickHouse/ClickHouse/issues/new?assignees=&labels=bug&template=bug-report.md&title=" target="_blank" rel="noopener">提bug</a></p></li><li><p><a href="https://github.com/ClickHouse/ClickHouse/issues/new?assignees=&labels=question&template=question.md&title=" target="_blank" rel="noopener">提问题/需求</a></p></li><li><p>加入 ClickHouse 交流社群，请在 <a href="/about/">about</a> 添加博主后提申请</p></li></ul><h2 id="5-问题线索采集"><a href="#5-问题线索采集" class="headerlink" title="5. 问题线索采集"></a><b>5. 问题线索采集</b></h2><h3 id="5-1-错误日志"><a href="#5-1-错误日志" class="headerlink" title=" 5.1 错误日志"></a><b> 5.1 错误日志</b></h3><p>clickhouse-server log 错误日志</p><h3 id="5-2-执行的SQL操作"><a href="#5-2-执行的SQL操作" class="headerlink" title="5.2 执行的SQL操作"></a><b>5.2 执行的SQL操作</b></h3><ul><li><p>MySQL 表结构</p></li><li><p>MySQL 侧配置</p></li><li><p>MySQL 侧执行过的 SQL</p></li></ul><h2 id="6-相关博文"><a href="#6-相关博文" class="headerlink" title="6. 相关博文"></a><b>6. 相关博文</b></h2><p><a href="/2020/07/26/clickhouse-and-friends-mysql-replication/">ClickHouse和他的朋友们（9）MySQL实时复制与实现</a><br><a href="/2020/08/26/clickhouse-and-friends-mysql-gtid-replication/">ClickHouse和他的朋友们（11）MySQL实时复制之GTID模式</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mysql" scheme="https://bohutang.me/tags/mysql/"/>
    
      <category term="replication" scheme="https://bohutang.me/tags/replication/"/>
    
      <category term="GTID" scheme="https://bohutang.me/tags/GTID/"/>
    
      <category term="materializemysql" scheme="https://bohutang.me/tags/materializemysql/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（12）神奇的物化视图(Materialized View)与原理</title>
    <link href="https://bohutang.me/2020/08/31/clickhouse-and-friends-materialized-view/"/>
    <id>https://bohutang.me/2020/08/31/clickhouse-and-friends-materialized-view/</id>
    <published>2020-08-30T16:00:00.000Z</published>
    <updated>2020-09-01T14:44:59.219Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materializeview.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-08-31</b></p><p>在 ClickHouse 里，物化视图(Materialized View)可以说是一个神奇且强大的东西，用途别具一格。</p><p>本文从底层机制进行分析，看看 ClickHouse 的 Materalized View 是怎么工作的，以方便更好的使用它。</p><h2 id="什么是物化视图"><a href="#什么是物化视图" class="headerlink" title="什么是物化视图"></a><b>什么是物化视图</b></h2><p>对大部分人来说，物化视图这个概念会比较抽象，物化？视图？。。。</p><p>为了更好的理解它，我们先看一个场景。</p><p>假设你是 *hub 一个“幸福”的小程序员，某天产品经理有个需求：实时统计每小时视频下载量。</p><p>用户下载明细表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download LIMIT 10;</span><br><span class="line">+---------------------+--------+--------+</span><br><span class="line">| when                | userid | bytes  |</span><br><span class="line">+---------------------+--------+--------+</span><br><span class="line">| 2020-08-31 18:22:06 |     19 | 530314 |</span><br><span class="line">| 2020-08-31 18:22:06 |     19 | 872957 |</span><br><span class="line">| 2020-08-31 18:22:06 |     19 | 107047 |</span><br><span class="line">| 2020-08-31 18:22:07 |     19 | 214876 |</span><br><span class="line">| 2020-08-31 18:22:07 |     19 | 820943 |</span><br><span class="line">| 2020-08-31 18:22:07 |     19 | 693959 |</span><br><span class="line">| 2020-08-31 18:22:08 |     19 | 882151 |</span><br><span class="line">| 2020-08-31 18:22:08 |     19 | 644223 |</span><br><span class="line">| 2020-08-31 18:22:08 |     19 | 199800 |</span><br><span class="line">| 2020-08-31 18:22:09 |     19 | 511439 |</span><br><span class="line"></span><br><span class="line">... ....</span><br></pre></td></tr></table></figure><p>计算每小时下载量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT toStartOfHour(when) AS hour, userid, count() as downloads, sum(bytes) AS bytes FROM download GROUP BY userid, hour ORDER BY userid, hour;</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| hour                | userid | downloads | bytes      |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| 2020-08-31 18:00:00 |     19 |      6822 | 3378623036 |</span><br><span class="line">| 2020-08-31 19:00:00 |     19 |     10800 | 5424173178 |</span><br><span class="line">| 2020-08-31 20:00:00 |     19 |     10800 | 5418656068 |</span><br><span class="line">| 2020-08-31 21:00:00 |     19 |     10800 | 5404309443 |</span><br><span class="line">| 2020-08-31 22:00:00 |     19 |     10800 | 5354077456 |</span><br><span class="line">| 2020-08-31 23:00:00 |     19 |     10800 | 5390852563 |</span><br><span class="line">| 2020-09-01 00:00:00 |     19 |     10800 | 5369839540 |</span><br><span class="line">| 2020-09-01 01:00:00 |     19 |     10800 | 5384161012 |</span><br><span class="line">| 2020-09-01 02:00:00 |     19 |     10800 | 5404581759 |</span><br><span class="line">| 2020-09-01 03:00:00 |     19 |      6778 | 3399557322 |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">10 rows in set (0.13 sec)</span><br></pre></td></tr></table></figure><p>很容易嘛，不过有个问题：<br>每次都要以 <code>download</code> 表为基础数据进行计算，*hub 数据量太大，无法忍受。</p><p>想到一个办法：如果对 <code>download</code> 进行预聚合，把结果保存到一个新表 <code>download_hour_mv</code>，并随着 <code>download</code> 增量实时更新，每次去查询<code>download_hour_mv</code> 不就可以了。</p><p>这个新表可以看做是一个物化视图，它在 ClickHouse 是一个普通表。</p><h2 id="创建物化视图"><a href="#创建物化视图" class="headerlink" title="创建物化视图"></a><b>创建物化视图</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; CREATE MATERIALIZED VIEW download_hour_mv</span><br><span class="line">ENGINE &#x3D; SummingMergeTree</span><br><span class="line">PARTITION BY toYYYYMM(hour) ORDER BY (userid, hour)</span><br><span class="line">AS SELECT</span><br><span class="line">  toStartOfHour(when) AS hour,</span><br><span class="line">  userid,</span><br><span class="line">  count() as downloads,</span><br><span class="line">  sum(bytes) AS bytes</span><br><span class="line">FROM download WHERE when &gt;&#x3D; toDateTime(&#39;2020-09-01 04:00:00&#39;)</span><br><span class="line">GROUP BY userid, hour</span><br></pre></td></tr></table></figure><p>这个语句主要做了：</p><ul><li>创建一个引擎为 <code>SummingMergeTree</code> 的物化视图 <code>download_hour_mv</code></li><li>物化视图的数据来源于 <code>download</code> 表，并根据 <code>select</code> 语句中的表达式进行相应“物化”操作</li><li>选取一个未来时间(当前时间是 <code>2020-08-31 18:00:00</code>)作为开始点 <code>WHERE when &gt;= toDateTime(&#39;2020-09-01 04:00:00&#39;)</code>，表示在<code>2020-09-01 04:00:00</code> 之后的数据才会被同步到 <code>download_hour_mv</code></li></ul><p>这样，目前 <code>download_hour_mv</code> 是一个空表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY userid, hour;</span><br><span class="line">Empty set (0.02 sec)</span><br></pre></td></tr></table></figure><p>注意：官方有 <a href="https://clickhouse.tech/docs/en/sql-reference/statements/create/view/#materialized" target="_blank" rel="noopener">POPULATE</a> 关键字，但是不建议使用，因为视图创建期间 <code>download</code> 如果有写入数据会丢失，这也是我们加一个 <code>WHERE</code> 作为数据同步点的原因。</p><p>那么，我们如何让源表数据可以一致性的同步到 <code>download_hour_mv</code> 呢？</p><h2 id="物化全量数据"><a href="#物化全量数据" class="headerlink" title="物化全量数据"></a><b>物化全量数据</b></h2><p>在<code>2020-09-01 04:00:00</code>之后，我们可以通过一个带 <code>WHERE</code> 快照的<code>INSERT INTO SELECT...</code> 对 <code>download</code> 历史数据进行物化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; INSERT INTO download_hour_mv</span><br><span class="line">SELECT</span><br><span class="line">  toStartOfHour(when) AS hour,</span><br><span class="line">  userid,</span><br><span class="line">  count() as downloads,</span><br><span class="line">  sum(bytes) AS bytes</span><br><span class="line">FROM download WHERE when &lt; toDateTime(&#39;2020-09-01 04:00:00&#39;)</span><br><span class="line">GROUP BY userid, hour</span><br></pre></td></tr></table></figure><p>查询物化视图：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY hour, userid, downloads DESC;</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| hour                | userid | downloads | bytes      |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| 2020-08-31 18:00:00 |     19 |      6822 | 3378623036 |</span><br><span class="line">| 2020-08-31 19:00:00 |     19 |     10800 | 5424173178 |</span><br><span class="line">| 2020-08-31 20:00:00 |     19 |     10800 | 5418656068 |</span><br><span class="line">| 2020-08-31 21:00:00 |     19 |     10800 | 5404309443 |</span><br><span class="line">| 2020-08-31 22:00:00 |     19 |     10800 | 5354077456 |</span><br><span class="line">| 2020-08-31 23:00:00 |     19 |     10800 | 5390852563 |</span><br><span class="line">| 2020-09-01 00:00:00 |     19 |     10800 | 5369839540 |</span><br><span class="line">| 2020-09-01 01:00:00 |     19 |     10800 | 5384161012 |</span><br><span class="line">| 2020-09-01 02:00:00 |     19 |     10800 | 5404581759 |</span><br><span class="line">| 2020-09-01 03:00:00 |     19 |      6778 | 3399557322 |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">10 rows in set (0.05 sec)</span><br></pre></td></tr></table></figure><p>可以看到数据已经“物化”到 <code>download_hour_mv</code>。</p><h2 id="物化增量数据"><a href="#物化增量数据" class="headerlink" title="物化增量数据"></a><b>物化增量数据</b></h2><p>写一些数据到 <code>download</code>表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; INSERT INTO download</span><br><span class="line">       SELECT</span><br><span class="line">         toDateTime(&#39;2020-09-01 04:00:00&#39;) + number*(1&#x2F;3) as when,</span><br><span class="line">         19,</span><br><span class="line">         rand() % 1000000</span><br><span class="line">       FROM system.numbers</span><br><span class="line">       LIMIT 10;</span><br></pre></td></tr></table></figure><p>查询物化视图 <code>download_hour_mv</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY hour, userid, downloads;</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| hour                | userid | downloads | bytes      |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| 2020-08-31 18:00:00 |     19 |      6822 | 3378623036 |</span><br><span class="line">| 2020-08-31 19:00:00 |     19 |     10800 | 5424173178 |</span><br><span class="line">| 2020-08-31 20:00:00 |     19 |     10800 | 5418656068 |</span><br><span class="line">| 2020-08-31 21:00:00 |     19 |     10800 | 5404309443 |</span><br><span class="line">| 2020-08-31 22:00:00 |     19 |     10800 | 5354077456 |</span><br><span class="line">| 2020-08-31 23:00:00 |     19 |     10800 | 5390852563 |</span><br><span class="line">| 2020-09-01 00:00:00 |     19 |     10800 | 5369839540 |</span><br><span class="line">| 2020-09-01 01:00:00 |     19 |     10800 | 5384161012 |</span><br><span class="line">| 2020-09-01 02:00:00 |     19 |     10800 | 5404581759 |</span><br><span class="line">| 2020-09-01 03:00:00 |     19 |      6778 | 3399557322 |</span><br><span class="line">| 2020-09-01 04:00:00 |     19 |        10 |    5732600 |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">11 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>可以看到最后一条数据就是我们增量的一个物化聚合，已经实时同步，这是如何做到的呢？</p><h2 id="物化视图原理"><a href="#物化视图原理" class="headerlink" title="物化视图原理"></a><b>物化视图原理</b></h2><p>ClickHouse 的物化视图原理并不复杂，在 <code>download</code> 表有新的数据写入时，如果检测到有物化视图跟它关联，会针对这批写入的数据进行物化操作。</p><p>比如上面新增数据是通过以下 SQL 生成的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT</span><br><span class="line">    -&gt;          toDateTime(&#39;2020-09-01 04:00:00&#39;) + number*(1&#x2F;3) as when,</span><br><span class="line">    -&gt;          19,</span><br><span class="line">    -&gt;          rand() % 1000000</span><br><span class="line">    -&gt;        FROM system.numbers</span><br><span class="line">    -&gt;        LIMIT 10;</span><br><span class="line">+---------------------+------+-------------------------+</span><br><span class="line">| when                | 19   | modulo(rand(), 1000000) |</span><br><span class="line">+---------------------+------+-------------------------+</span><br><span class="line">| 2020-09-01 04:00:00 |   19 |                  870495 |</span><br><span class="line">| 2020-09-01 04:00:00 |   19 |                  322270 |</span><br><span class="line">| 2020-09-01 04:00:00 |   19 |                  983422 |</span><br><span class="line">| 2020-09-01 04:00:01 |   19 |                  759708 |</span><br><span class="line">| 2020-09-01 04:00:01 |   19 |                  975636 |</span><br><span class="line">| 2020-09-01 04:00:01 |   19 |                  365507 |</span><br><span class="line">| 2020-09-01 04:00:02 |   19 |                  865569 |</span><br><span class="line">| 2020-09-01 04:00:02 |   19 |                  975742 |</span><br><span class="line">| 2020-09-01 04:00:02 |   19 |                   85827 |</span><br><span class="line">| 2020-09-01 04:00:03 |   19 |                  992779 |</span><br><span class="line">+---------------------+------+-------------------------+</span><br><span class="line">10 rows in set (0.02 sec)</span><br></pre></td></tr></table></figure><p>物化视图执行的语句类似：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO download_hour_mv</span><br><span class="line">SELECT</span><br><span class="line">  toStartOfHour(when) AS hour,</span><br><span class="line">  userid,</span><br><span class="line">  count() as downloads,</span><br><span class="line">  sum(bytes) AS bytes</span><br><span class="line">FROM [新增的10条数据] WHERE when &gt;&#x3D; toDateTime(&#39;2020-09-01 04:00:00&#39;)</span><br><span class="line">GROUP BY userid, hour</span><br></pre></td></tr></table></figure><p>代码导航：</p><ol><li><p>添加视图 OutputStream， <a href="https://github.com/ClickHouse/ClickHouse/blob/cb4644ea6d04b3d5900868b4f8d686a03082379a/src/Interpreters/InterpreterInsertQuery.cpp#L313" target="_blank" rel="noopener">InterpreterInsertQuery.cpp</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (table-&gt;noPushingToViews() &amp;&amp; !no_destination)</span><br><span class="line">    out &#x3D; table-&gt;write(query_ptr, metadata_snapshot, context);</span><br><span class="line">else</span><br><span class="line">    out &#x3D; std::make_shared&lt;PushingToViewsBlockOutputStream&gt;(table, metadata_snapshot, context, query_ptr, no_destination);</span><br></pre></td></tr></table></figure></li><li><p>构造 Insert ， <a href="https://github.com/ClickHouse/ClickHouse/blob/cb4644ea6d04b3d5900868b4f8d686a03082379a/src/DataStreams/PushingToViewsBlockOutputStream.cpp#L85" target="_blank" rel="noopener">PushingToViewsBlockOutputStream.cpp</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ASTPtr insert_query_ptr(insert.release());</span><br><span class="line">InterpreterInsertQuery interpreter(insert_query_ptr, *insert_context);</span><br><span class="line">BlockIO io &#x3D; interpreter.execute();</span><br><span class="line">out &#x3D; io.out;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li>物化新增数据：<a href="https://github.com/ClickHouse/ClickHouse/blob/cb4644ea6d04b3d5900868b4f8d686a03082379a/src/DataStreams/PushingToViewsBlockOutputStream.cpp#L331" target="_blank" rel="noopener">PushingToViewsBlockOutputStream.cpp</a></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Context local_context &#x3D; *select_context;</span><br><span class="line">local_context.addViewSource(</span><br><span class="line">    StorageValues::create(</span><br><span class="line">        storage-&gt;getStorageID(), metadata_snapshot-&gt;getColumns(), block, storage-&gt;getVirtuals()));</span><br><span class="line">select.emplace(view.query, local_context, SelectQueryOptions());</span><br><span class="line">in &#x3D; std::make_shared&lt;MaterializingBlockInputStream&gt;(select-&gt;execute().getInputStream()</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>物化视图的用途较多。</p><p>比如可以解决表索引问题，我们可以用物化视图创建另外一种物理序，来满足某些条件下的查询问题。</p><p>还有就是通过物化视图的实时同步数据能力，我们可以做到更加灵活的表结构变更。</p><p>更强大的地方是它可以借助 MergeTree 家族引擎(SummingMergeTree、Aggregatingmergetree等)，得到一个实时的预聚合，满足快速查询。</p><p>原理是把增量的数据根据 <code>AS SELECT ...</code> 对其进行处理并写入到物化视图表，物化视图是一种普通表，可以直接读取和写入。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materializeview.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="materialized view" scheme="https://bohutang.me/tags/materialized-view/"/>
    
      <category term="物化视图" scheme="https://bohutang.me/tags/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（11）MySQL实时复制之GTID模式</title>
    <link href="https://bohutang.me/2020/08/26/clickhouse-and-friends-mysql-gtid-replication/"/>
    <id>https://bohutang.me/2020/08/26/clickhouse-and-friends-mysql-gtid-replication/</id>
    <published>2020-08-25T16:00:00.000Z</published>
    <updated>2020-09-03T15:05:16.838Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-03</b></p><p> <a href="/2020/07/26/clickhouse-and-friends-mysql-replication/">MySQL实时复制原理篇</a></p><p>几天前 ClickHouse 官方发布了 <a href="https://github.com/ClickHouse/ClickHouse/releases/tag/v20.8.1.4447-testing" target="_blank" rel="noopener">v20.8.1.4447-testing</a>，这个版本已经包含了 MaterializeMySQL 引擎，实现了 ClickHouse 实时复制 MySQL 数据的能力，感兴趣的朋友可以通过官方安装包来做体验，安装方式参考: <a href="https://clickhouse.tech/#quick-start" target="_blank" rel="noopener">https://clickhouse.tech/#quick-start</a>，需要注意的是要选择 testing 分支。</p><h2 id="基于位点同步"><a href="#基于位点同步" class="headerlink" title="基于位点同步"></a><b>基于位点同步</b></h2><p>MaterializeMySQL 在 v20.8.1.4447-testing 版本是基于 binlog 位点模式进行同步的。</p><p>每次消费完一批 binlog event，就会记录 event 的位点信息到 .metadata 文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Version:1</span><br><span class="line">Binlog File:mysql-bin.000002</span><br><span class="line">Binlog Position:328</span><br><span class="line">Data Version:1</span><br></pre></td></tr></table></figure><p>这样当 ClickHouse 再次启动时，它会把 {‘mysql-bin.000002’, 328} 二元组通过协议告知 MySQL Server，MySQL 从这个位点开始发送数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1&gt; ClickHouse 发送 &#123;&#39;mysql-bin.000002&#39;, 328&#125; 位点信息给 MySQL</span><br><span class="line">s2&gt; MySQL 找到本地 mysql-bin.000002 文件并定位到 328 偏移位置，读取下一个 event 发送给 ClickHouse</span><br><span class="line">s3&gt; ClickHouse 接收 binlog event 并更新 .metadata位点</span><br></pre></td></tr></table></figure><p>看起来不错哦，但是有个问题：<br>如果 MySQL Server 是一个集群(比如１主２从)，通过 VIP 对外服务，MaterializeMySQL 的 host 指向的是这个 vip。<br>当集群主从发生切换后，{binlog-name, binlog-position} 二元组其实是不准确的，因为集群里主从 binlog 不一定是完全一致的(binlog 可以做 reset 操作)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1&gt; ClickHouse 发送 &#123;&#39;mysql-bin.000002&#39;, 328&#125; 给集群新主 MySQL</span><br><span class="line">s2&gt; 新主 MySQL 发现本地没有 mysql-bin.000002 文件，因为它做过 reset master 操作，binlog 文件是 mysql-bin.000001</span><br><span class="line">... oops ...</span><br></pre></td></tr></table></figure><p>为了解决这个问题，我们开发了 GTID 同步模式，废弃了不安全的位点同步模式，目前已被 upstream merged <a href="https://github.com/ClickHouse/ClickHouse/pull/13820" target="_blank" rel="noopener">#PR13820</a>，下一个 testing 版本即可体验。</p><p>着急的话可以自己编译或通过 <a href="https://clickhouse-builds.s3.yandex.net/0/2b8ad576cc3892d2d760f3f8b670adf17db0c2a0/clickhouse_build_check/report.html" target="_blank" rel="noopener">ClickHouse Build Check for master-20.9.1</a> 下载安装。</p><h2 id="基于GTID同步"><a href="#基于GTID同步" class="headerlink" title="基于GTID同步"></a><b>基于GTID同步</b></h2><p>GTID 是 MySQL 复制增强版，从 MySQL 5.6 版本开始支持，目前已经是 MySQL 主流复制模式。</p><p>它为每个 event 分配一个全局唯一ID和序号，我们可以不用关心 MySQL 集群主从拓扑结构，直接告知 MySQL 这个 GTID 即可，.metadata变为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Version:2</span><br><span class="line">Executed GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5</span><br><span class="line">Data Version:1</span><br></pre></td></tr></table></figure><p><code>f4aee41e-e36f-11ea-8b37-0242ac110002</code> 是生成 event的主机UUID，<code>1-5</code>是已经同步的event区间。</p><p>这样流程就变为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1&gt; ClickHouse 发送 GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5 给 MySQL</span><br><span class="line">s2&gt; MySQL 根据 GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5 找到本地位点，读取下一个 event 发送给 ClickHouse</span><br><span class="line">s3&gt; ClickHouse 接收 binlog event 并更新 .metadata GTID信息</span><br></pre></td></tr></table></figure><h2 id="MySQL开启GTID"><a href="#MySQL开启GTID" class="headerlink" title=" MySQL开启GTID"></a><b> MySQL开启GTID</b></h2><p>那么，MySQL 侧怎么开启 GTID 呢？增加以下两个参数即可:</p><p><code>--gtid-mode=ON --enforce-gtid-consistency</code></p><p>比如启动一个启用 GTID 的 MySQL docker：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -e MYSQL_ROOT_PASSWORD&#x3D;123 mysql:5.7 mysqld --datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql --server-id&#x3D;1 --log-bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin.log --gtid-mode&#x3D;ON --enforce-gtid-consistency</span><br></pre></td></tr></table></figure><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><b>注意事项</b></h2><p>启用 GTID 复制模式后，metadata Version 会变为 2，也就是老版本启动时会直接报错，database 需要重建。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>MaterializeMySQL 引擎还处于不停迭代中，对于它我们有一个初步的规划：</p><ul><li><p><b>稳定性保证</b><br>这块需要更多测试，更多试用反馈</p></li><li><p><b>索引优化</b><br>OLTP 索引一般不是为 OLAP 设计，目前索引转换还是依赖 MySQL 表结构，需要更加智能化</p></li><li><p><b>可观测性</b><br>在 ClickHouse 侧可以方便的查看当前同步信息，类似 MySQL <code>show slave status</code></p></li><li><p><b>数据一致性校验</b><br>需要提供方式可以校验 MySQL 和 ClickHouse 数据一致性</p></li></ul><p>MaterializeMySQL 已经是社区功能，仍然有不少的工作要做。期待更多的力量加入，我们的征途不止星辰大海。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="MySQL" scheme="https://bohutang.me/tags/MySQL/"/>
    
      <category term="replication" scheme="https://bohutang.me/tags/replication/"/>
    
      <category term="GTID" scheme="https://bohutang.me/tags/GTID/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（10）MergeTree Write-Ahead Log</title>
    <link href="https://bohutang.me/2020/08/18/clickhouse-and-friends-merge-tree-wal/"/>
    <id>https://bohutang.me/2020/08/18/clickhouse-and-friends-merge-tree-wal/</id>
    <published>2020-08-17T16:00:00.000Z</published>
    <updated>2020-08-26T13:29:42.849Z</updated>
    
    <content type="html"><![CDATA[<p><b>最后更新: 2020-08-26</b></p><p>数据库系统为了提高写入性能，会把数据先写到内存，等“攒”到一定程度后再回写到磁盘，比如 MySQL 的 buffer pool 机制。</p><p>因为数据先写到内存，为了数据的安全性，我们需要一个 Write-Ahead Log (WAL) 来保证内存数据的安全性。</p><p>今天我们来看看 ClickHouse 新增的 <a href="https://github.com/ClickHouse/ClickHouse/pull/8290" target="_blank" rel="noopener">MergeTreeWriteAheadLog</a> 模块，它到底解决了什么问题。</p><h2 id="高频写问题"><a href="#高频写问题" class="headerlink" title="高频写问题"></a><b>高频写问题</b></h2><p>对于 ClickHouse MergeTree 引擎，每次写入(即使１条数据)都会在磁盘生成一个分区目录(part)，等着 merge 线程合并。</p><p>如果有多个客户端，每个客户端写入的数据量较少、次数较频繁的情况下，就会引发 <code>DB::Exception: Too many parts</code> 错误。</p><p>这样就对客户端有一定的要求，比如需要做 batch 写入。</p><p>或者，写入到 Buffer 引擎，定时的刷回 MergeTree，缺点是在宕机时可能会丢失数据。</p><h2 id="MergeTree-WAL"><a href="#MergeTree-WAL" class="headerlink" title="MergeTree WAL"></a><b>MergeTree WAL</b></h2><h3 id="1-默认模式"><a href="#1-默认模式" class="headerlink" title="1. 默认模式"></a><b>1. 默认模式</b></h3><p>我们先看看在没有 WAL 情况下，MergeTree 是如何写入的：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/mergetree-part-raw.png" align="center" style="zoom:50%;" /><p>每次写入 MergeTree 都会直接在磁盘上创建分区目录，并生成分区数据，这种模式其实就是 WAL + 数据的融合。</p><p>很显然，这种模式不适合频繁写操作的情况，否则会生成非常多的分区目录和文件，引发 <code>Too many parts</code> 错误。</p><h3 id="2-WAL模式"><a href="#2-WAL模式" class="headerlink" title="2. WAL模式"></a><b>2. WAL模式</b></h3><p>设置SETTINGS: <code>min_rows_for_compact_part=2</code>，分别执行２条写 SQL，数据会先写到 wal.bin 文件：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/mergetree-part-wal.png" align="center" style="zoom:50%;" /><p>当满足  <code>min_rows_for_compact_part=2</code> 后，merger 线程触发合并操作，生成  <code>1_1_2_1</code> 分区，也就是完成了 wal.bin 里的 <code>1_1_1_0</code> 和 <code>1_2_2_0</code> 两个分区的合并操作。当我们执行第三条 SQL 写入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into default.mt(a,b,c) values(1,3,3)</span><br></pre></td></tr></table></figure><p>数据块(分区)会继续追加到 wal.bin 尾部：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/mergetree-part-wal-merge.png" align="center" style="zoom:50%;" /><p>此时，3 条数据分布在两个地方：分区 <code>1_1_2_1</code>， wal.bin 里的 <code>1_3_3_0</code>。</p><p>这样就有一个问题：当我们执行查询的时候，数据是怎么合并的呢？</p><p>MergeTree 使用全局结构 <code>data_parts_indexes</code> 维护分区信息，当服务启动的时候，<code>MergeTreeData::loadDataParts</code>方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. data_parts_indexes.insert(1_1_2_1)</span><br><span class="line">2. 读取 wal.bin，通过 getActiveContainingPart 判断分区是否已经merge到磁盘：1_1_1_0 已经存在, 1_2_2_0 已经存在，data_parts_indexes.insert(1_3_3_0)</span><br><span class="line">3. data_parts_indexes:&#123;1_1_2_1,1_3_3_0&#125;</span><br></pre></td></tr></table></figure><p>这样，它总是能维护全局的分区信息。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>WAL 功能在 <a href="https://github.com/ClickHouse/ClickHouse/pull/8290" target="_blank" rel="noopener">PR＃8290</a> 实现，通过以下两个 SETTINGS 开启，目前还是 Experimental：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">min_bytes_for_compact_part</span><br><span class="line">&quot;Experimental. Minimal uncompressed size in bytes to create part in compact format instead of saving it in RAM&quot;</span><br><span class="line"></span><br><span class="line">min_rows_for_compact_part</span><br><span class="line">&quot;Experimental. Minimal number of rows to create part in compact format instead of saving it in RAM&quot;</span><br></pre></td></tr></table></figure><p>MergeTree 通过 WAL 来保护客户端的高频、少量写机制，减少服务端目录和文件数量，让客户端操作尽可能简单、高效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;b&gt;最后更新: 2020-08-26&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;数据库系统为了提高写入性能，会把数据先写到内存，等“攒”到一定程度后再回写到磁盘，比如 MySQL 的 buffer pool 机制。&lt;/p&gt;
&lt;p&gt;因为数据先写到内存，为了数据的安全性，我们需要一个 Write
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mergetree" scheme="https://bohutang.me/tags/mergetree/"/>
    
      <category term="WAL" scheme="https://bohutang.me/tags/WAL/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（9）MySQL实时复制与实现</title>
    <link href="https://bohutang.me/2020/07/26/clickhouse-and-friends-mysql-replication/"/>
    <id>https://bohutang.me/2020/07/26/clickhouse-and-friends-mysql-replication/</id>
    <published>2020-07-25T16:00:00.000Z</published>
    <updated>2020-09-07T23:52:58.642Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-08</b></p><p>很多人看到标题还以为自己走错了夜场，其实没有。</p><p>ClickHouse  可以挂载为 MySQL 的一个从库 ，先全量再增量的实时同步 MySQL 数据，这个功能可以说是今年最亮眼、最刚需的功能，基于它我们可以轻松的打造一套企业级解决方案，让 OLTP 和 OLAP 的融合从此不再头疼。</p><p>目前支持 MySQL 5.6/5.7/8.0 版本，兼容 Delete/Update 语句，及大部分常用的 DDL 操作。<br><a href="https://github.com/ClickHouse/ClickHouse/pull/10851" target="_blank" rel="noopener">代码</a>已经合并到 upstream master 分支，预计在20.8版本作为experimental 功能发布。</p><p>毕竟是两个异构生态的融合，仍然有不少的工作要做，同时也期待着社区用户的反馈，以加速迭代。</p><h2 id="代码获取"><a href="#代码获取" class="headerlink" title="代码获取"></a><b>代码获取</b></h2><p>获取 <a href="https://github.com/ClickHouse/ClickHouse" target="_blank" rel="noopener">clickhouse/master</a> 代码编译即可，方法见 <a href="/2020/06/05/clickhouse-and-friends-development/">ClickHouse和他的朋友们（1）编译、开发、测试</a>…</p><h2 id="MySQL-Master"><a href="#MySQL-Master" class="headerlink" title=" MySQL Master"></a><b> MySQL Master</b></h2><p>我们需要一个开启 binlog 的 MySQL 作为 master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -e MYSQL_ROOT_PASSWORD&#x3D;123 mysql:5.7 mysqld --datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql --server-id&#x3D;1 --log-bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin.log --gtid-mode&#x3D;ON --enforce-gtid-consistency</span><br></pre></td></tr></table></figure><p>创建数据库和表，并写入数据:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database ckdb;</span><br><span class="line">mysql&gt; use ckdb;</span><br><span class="line">mysql&gt; create table t1(a int not null primary key, b int);</span><br><span class="line">mysql&gt; insert into t1 values(1,1),(2,2);</span><br><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 1 |    1 |</span><br><span class="line">| 2 |    2 |</span><br><span class="line">+---+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h2 id="ClickHouse-Slave"><a href="#ClickHouse-Slave" class="headerlink" title="ClickHouse Slave "></a><b>ClickHouse Slave </b></h2><p>目前以 database 为单位进行复制，不同的 database 可以来自不同的 MySQL master，这样就可以实现多个 MySQL 源数据同步到一个 ClickHouse 做 OLAP 分析功能。</p><p>首先开启体验开关:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) SET allow_experimental_database_materialize_mysql&#x3D;1;</span><br></pre></td></tr></table></figure><p>创建一个复制通道：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) CREATE DATABASE ckdb ENGINE &#x3D; MaterializeMySQL(&#39;172.17.0.2:3306&#39;, &#39;ckdb&#39;, &#39;root&#39;, &#39;123&#39;);</span><br><span class="line">clickhouse :) use ckdb;</span><br><span class="line">clickhouse :) show tables;</span><br><span class="line">┌─name─┐</span><br><span class="line">│ t1   │</span><br><span class="line">└──────┘</span><br><span class="line">clickhouse :) select * from t1;</span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 1 │ 1 │</span><br><span class="line">└───┴───┘</span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 2 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">2 rows in set. Elapsed: 0.017 sec.</span><br></pre></td></tr></table></figure><p>看下 ClickHouse 的同步位点：<br>cat ckdatas/metadata/ckdb/.metadata</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Version:1</span><br><span class="line">Binlog File:mysql-bin.000001</span><br><span class="line">Binlog Position:913</span><br><span class="line">Data Version:0</span><br></pre></td></tr></table></figure><h2 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a><b>Delete</b></h2><p>首先在 MySQL Master 上执行一个删除操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from t1 where a&#x3D;1;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br></pre></td></tr></table></figure><p>然后在 ClickHouse Slave 侧查看记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select * from t1;</span><br><span class="line"></span><br><span class="line">SELECT *</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 2 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">1 rows in set. Elapsed: 0.032 sec.</span><br></pre></td></tr></table></figure><p>此时的 metadata 里 Data Version 已经递增到 2:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat ckdatas&#x2F;metadata&#x2F;ckdb&#x2F;.metadata </span><br><span class="line">Version:1</span><br><span class="line">Binlog File:mysql-bin.000001</span><br><span class="line">Binlog Position:1171</span><br><span class="line">Data Version:2</span><br></pre></td></tr></table></figure><h2 id="Update"><a href="#Update" class="headerlink" title="Update"></a><b>Update</b></h2><p>MySQL Master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 2 |    2 |</span><br><span class="line">+---+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; update t1 set b&#x3D;b+1;</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 2 |    3 |</span><br><span class="line">+---+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>ClickHouse Slave:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select * from t1;</span><br><span class="line"></span><br><span class="line">SELECT *</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 3 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">1 rows in set. Elapsed: 0.023 sec.</span><br></pre></td></tr></table></figure><h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a><b>性能测试</b></h2><h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a><b>测试环境</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MySQL          8C16G 云主机, 192.168.0.3，基础数据 10188183 条记录</span><br><span class="line">ClickHouse     8C16G 云主机, 192.168.0.4</span><br><span class="line">benchyou       8C8G  云主机,  192.168.0.5, 256并发写, https:&#x2F;&#x2F;github.com&#x2F;xelabs&#x2F;benchyou</span><br></pre></td></tr></table></figure><p>性能测试跟硬件环境有较大关系，这里使用的是云主机模式，数据供参考。</p><h3 id="全量性能"><a href="#全量性能" class="headerlink" title="全量性能"></a><b>全量性能</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">8c16G-vm :) create database sbtest engine&#x3D;MaterializeMySQL(&#39;192.168.0.3:3306&#39;, &#39;sbtest&#39;, &#39;test&#39;, &#39;123&#39;);</span><br><span class="line"></span><br><span class="line">8c16G-vm :) watch lv1;</span><br><span class="line"></span><br><span class="line">WATCH lv1</span><br><span class="line"></span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│       0 │ 2020-07-29 06:36:04 │        1 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 1113585 │ 2020-07-29 06:36:05 │        2 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 2227170 │ 2020-07-29 06:36:07 │        3 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 3340755 │ 2020-07-29 06:36:10 │        4 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 4454340 │ 2020-07-29 06:36:13 │        5 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 5567925 │ 2020-07-29 06:36:16 │        6 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 6681510 │ 2020-07-29 06:36:18 │        7 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 7795095 │ 2020-07-29 06:36:22 │        8 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 8908680 │ 2020-07-29 06:36:25 │        9 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌──count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 10022265 │ 2020-07-29 06:36:28 │       10 │</span><br><span class="line">└──────────┴─────────────────────┴──────────┘</span><br><span class="line">┌──count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 10188183 │ 2020-07-29 06:36:28 │       11 │</span><br><span class="line">└──────────┴─────────────────────┴──────────┘</span><br><span class="line">← Progress: 11.00 rows, 220.00 B (0.16 rows&#x2F;s., 3.17 B&#x2F;s.)</span><br></pre></td></tr></table></figure><p>在这个硬件环境下，全量同步性能大概是 <b>424507/s</b>，<b>42w</b> 事务每秒。<br>因为全量的数据之间没有依赖关系，可以进一步优化成并行，加速同步。<br>全量的性能直接决定 ClickHouse slave 坏掉后重建的速度，如果你的 MySQL 有<b> 10 亿</b>条数据，大概<b> 40 分钟</b>就可以重建完成。</p><h3 id="增量性能-实时同步"><a href="#增量性能-实时同步" class="headerlink" title="增量性能(实时同步)"></a><b>增量性能(实时同步)</b></h3><p>在当前配置下，ClickHouse slave 单线程回放消费能力大于 MySQL master 256 并发下生产能力，通过测试可以看到它们保持<b>实时同步</b>。</p><p>benchyou 压测数据，<b>2.1w</b> 事务/秒(MySQL 在当前环境下TPS上不去):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;benchyou --mysql-host&#x3D;192.168.0.3 --mysql-user&#x3D;test --mysql-password&#x3D;123 --oltp-tables-count&#x3D;1 --write-threads&#x3D;256 --read-threads&#x3D;0</span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[13s]        [r:0,w:256,u:0,d:0]  19962    19962   0    </span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[14s]        [r:0,w:256,u:0,d:0]  20415    20415   0 </span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[15s]        [r:0,w:256,u:0,d:0]  21131    21131   0</span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[16s]        [r:0,w:256,u:0,d:0]  21606    21606   0</span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[17s]        [r:0,w:256,u:0,d:0]  22505    22505   0</span><br></pre></td></tr></table></figure><p>ClickHouse 侧单线程回放能力，<b>2.1w</b> 事务/秒，实时同步：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  150732 │ 2020-07-30 05:17:15 │       17 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  155477 │ 2020-07-30 05:17:16 │       18 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  160222 │ 2020-07-30 05:17:16 │       19 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  164967 │ 2020-07-30 05:17:16 │       20 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  169712 │ 2020-07-30 05:17:16 │       21 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  174457 │ 2020-07-30 05:17:16 │       22 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  179202 │ 2020-07-30 05:17:17 │       23 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  183947 │ 2020-07-30 05:17:17 │       24 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  188692 │ 2020-07-30 05:17:17 │       25 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  193437 │ 2020-07-30 05:17:17 │       26 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  198182 │ 2020-07-30 05:17:17 │       27 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br></pre></td></tr></table></figure><h2 id="实现机制"><a href="#实现机制" class="headerlink" title="实现机制"></a><b>实现机制</b></h2><p>在探讨机制之前，首先需要了解下 MySQL 的 binlog event ，主要有以下几种类型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. MYSQL_QUERY_EVENT　　　　-- DDL</span><br><span class="line">2. MYSQL_WRITE_ROWS_EVENT　-- insert数据</span><br><span class="line">3. MYSQL_UPDATE_ROWS_EVENT -- update数据</span><br><span class="line">4. MYSQL_DELETE_ROWS_EVENT -- delete数据</span><br></pre></td></tr></table></figure><p>当一个事务提交后，MySQL 会把执行的 SQL 处理成相应的 binlog event，并持久化到 binlog 文件。</p><p>binlog 是 MySQL 对外输出的重要途径，只要你实现 MySQL Replication Protocol，就可以流式的消费MySQL 生产的 binlog event，具体协议见 <a href="https://dev.mysql.com/doc/internals/en/replication-protocol.html" target="_blank" rel="noopener">Replication Protocol</a>。</p><p>由于历史原因，协议繁琐而诡异，这不是本文重点。</p><p>对于 ClickHouse 消费 MySQL binlog 来说，主要有以下３个难点：</p><ul><li>DDL 兼容</li><li>Delete/Update 支持</li><li>Query 过滤</li></ul><h3 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a><b>DDL</b></h3><p>DDL 兼容花费了大量的代码去实现。</p><p>首先，我们看看 MySQL 的表复制到 ClickHouse 后会变成什么样子。</p><p>MySQL master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show create table t1\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">       Table: t1</span><br><span class="line">Create Table: CREATE TABLE &#96;t1&#96; (</span><br><span class="line">  &#96;a&#96; int(11) NOT NULL,</span><br><span class="line">  &#96;b&#96; int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (&#96;a&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;latin1</span><br></pre></td></tr></table></figure><p> ClickHouse slave:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ATTACH TABLE t1</span><br><span class="line">(</span><br><span class="line">    &#96;a&#96; Int32,</span><br><span class="line">    &#96;b&#96; Nullable(Int32),</span><br><span class="line">    &#96;_sign&#96; Int8,</span><br><span class="line">    &#96;_version&#96; UInt64</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; ReplacingMergeTree(_version)</span><br><span class="line">PARTITION BY intDiv(a, 4294967)</span><br><span class="line">ORDER BY tuple(a)</span><br><span class="line">SETTINGS index_granularity &#x3D; 8192</span><br></pre></td></tr></table></figure><p>可以看到：</p><ul><li>默认增加了 2 个隐藏字段：_sign(-1删除, 1写入) 和 _version(数据版本) </li><li>引擎转换成了 ReplacingMergeTree，以 _version 作为 column version</li><li>原主键字段 a 作为排序和分区键</li></ul><p>这只是一个表的复制，其他还有非常多的DDL处理，比如增加列、索引等，感兴趣可以观摩 Parsers/MySQL 下代码。</p><h3 id="索引转换"><a href="#索引转换" class="headerlink" title="索引转换"></a><b>索引转换</b></h3><p>MySQL 对应的主键/索引如何对应到 MaterializeMySQL 表结构呢？</p><p>首先针对 MySQL 建表语句进行key扫描，<a href="https://github.com/ClickHouse/ClickHouse/blob/1076a42cf599c7eedd7d0cda2246f9afaf6434a4/src/Interpreters/MySQL/InterpretersMySQLDDLQuery.cpp#L157" target="_blank" rel="noopener">InterpretersMySQLDDLQuery::getKeys</a>：</p><ol><li>扫描 unique_key</li><li>扫描 primary_key</li><li>扫描 auto_increment</li></ol><p>其次根据 key 按照以下顺序生成 OrderBy tuple 表达式，<a href="https://github.com/ClickHouse/ClickHouse/blob/1076a42cf599c7eedd7d0cda2246f9afaf6434a4/src/Interpreters/MySQL/InterpretersMySQLDDLQuery.cpp#L288" target="_blank" rel="noopener">InterpretersMySQLDDLQuery::getOrderByPolicy</a>：</p><ol><li>primary_key[not increment]</li><li>key[not increment]</li><li>unique[not increment]</li><li>unique[increment]</li><li>key[increment]</li><li>primary_key[increment]</li></ol><p>ClickHouse 的物理序目前只有一种(多物理序有一定的规划)，是有 OrderBy 决定，所以 MaterializeMySQL 索引如果利用不佳，可以使用物化视图建立新的物理序解决。</p><h3 id="Update和Delete"><a href="#Update和Delete" class="headerlink" title="Update和Delete"></a><b>Update和Delete</b></h3><p>当我们在 MySQL master 执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from t1 where a&#x3D;1;</span><br><span class="line">mysql&gt; update t1 set b&#x3D;b+1;</span><br></pre></td></tr></table></figure><p>ClickHouse t1数据（把  _sign 和 _version 一并查询）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select a,b,_sign, _version from t1;</span><br><span class="line"></span><br><span class="line">SELECT </span><br><span class="line">    a,</span><br><span class="line">    b,</span><br><span class="line">    _sign,</span><br><span class="line">    _version</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │     1 │        1 │</span><br><span class="line">│ 2 │ 2 │     1 │        1 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br></pre></td></tr></table></figure><p>根据返回结果，可以看到是由 3 个 part 组成。</p><p>part1 由 <code>mysql&gt; insert into t1 values(1,1),(2,2)</code> 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │     1 │        1 │</span><br><span class="line">│ 2 │ 2 │     1 │        1 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br></pre></td></tr></table></figure><p>part2 由 <code>mysql&gt; delete from t1 where a=1</code> 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">说明：</span><br><span class="line">_sign &#x3D; -1表明处于删除状态</span><br></pre></td></tr></table></figure><p>part3 由 <code>update t1 set b=b+1</code> 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br></pre></td></tr></table></figure><p>使用 final 查询：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select a,b,_sign,_version from t1 final;</span><br><span class="line"></span><br><span class="line">SELECT </span><br><span class="line">    a,</span><br><span class="line">    b,</span><br><span class="line">    _sign,</span><br><span class="line">    _version</span><br><span class="line">FROM t1</span><br><span class="line">FINAL</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line"></span><br><span class="line">2 rows in set. Elapsed: 0.016 sec.</span><br></pre></td></tr></table></figure><p>可以看到 ReplacingMergeTree 已经根据 _version 和 OrderBy 对记录进行去重。</p><h3 id="Query"><a href="#Query" class="headerlink" title="Query"></a><b>Query</b></h3><p>MySQL master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 2 |    3 |</span><br><span class="line">+---+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>ClickHouse slave:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select * from t1;</span><br><span class="line"></span><br><span class="line">SELECT *</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 3 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">clickhouse :) select *,_sign,_version from t1;</span><br><span class="line"></span><br><span class="line">SELECT </span><br><span class="line">    *,</span><br><span class="line">    _sign,</span><br><span class="line">    _version</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">说明：这里还有一条删除记录，_sign为-1</span><br></pre></td></tr></table></figure><p>MaterializeMySQL 被定义成一种存储引擎，所以在读取的时候，会根据 _sign 状态进行判断，如果是-1则是已经删除，进行过滤。</p><h2 id="并行回放"><a href="#并行回放" class="headerlink" title="并行回放"></a><b>并行回放</b></h2><p>为什么 MySQL 需要并行回放？</p><p>假设 MySQL master 有 1024 个并发同时写入、更新数据，瞬间产生大量的 binlog event ，MySQL slave 上只有一个线程一个 event 接着一个 event 式回放，于是 MySQL 实现了并行回放功能！</p><p>那么，MySQL slave 回放时能否完全(或接近)模拟出 master 当时的 1024 并发行为呢？</p><p>要想并行首先要解决的就是依赖问题：我们需要 master 标记出哪些 event 可以并行，哪些 event 有先后关系，因为它是第一现场。</p><p>MySQL 通过在 binlog 里增加:</p><ul><li>last_committed，相同则可以并行</li><li>sequece_number，较小先执行，描述先后依赖</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">last_committed&#x3D;3   sequece_number&#x3D;4   -- event1</span><br><span class="line">last_committed&#x3D;4   sequece_number&#x3D;5   -- event2</span><br><span class="line">last_committed&#x3D;4   sequece_number&#x3D;6   -- event3</span><br><span class="line">last_committed&#x3D;5   sequece_number&#x3D;7   -- event4</span><br></pre></td></tr></table></figure><p>event2 和 event3 则可以并行，event4 需要等待前面 event 完成才可以回放。<br>以上只是一个大体原理，目前 MySQL 有３种并行模式可以选择：</p><ol><li>基于 database 并行</li><li>基于 group commit 并行</li><li>基于主键不冲突的 write set 并行</li></ol><p>最大程度上让 MySQL slave加速回放，整套机制还是异常复杂的。</p><p>回到 ClickHouse slave 问题，我们采用的单线程回放，延迟已经不是主要问题，这是由它们的机制决定的：<br>MySQL slave 回放时，需要把 binlog event 转换成  SQL，然后模拟 master 的写入，这种逻辑复制是导致性能低下的最重要原因。<br>而 ClickHouse 在回放上，直接把 binlog event 转换成 底层 block 结构，然后直接写入底层的存储引擎，接近于物理复制，可以理解为把 binlog event 直接回放到 InnoDB 的 page。</p><h2 id="读取最新"><a href="#读取最新" class="headerlink" title="读取最新"></a><b>读取最新</b></h2><p>虽然 ClickHouse slave 回放非常快，接近于实时，如何在ClickHouse slave上总是读取到最新的数据呢？</p><p>其实非常简单，借助 MySQL binlog GTID 特性，每次读的时候，我们跟 ｍaster 做一次 executed_gtid 同步，然后等待这些 executed_gtid 回放完毕即可。</p><h2 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a><b>数据一致性</b></h2><p>对一致性要求较高的场景，我们怎么验证 MySQL master 的数据和 ClickHouse slave 的数据一致性呢？</p><p>这块初步想法是提供一个兼容 MySQL checksum 算法的函数，我们只需对比两边的 checksum 值即可。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>ClickHouse 实时复制同步 MySQL 数据是 upstream 2020 的一个 roadmap，在整体构架上比较有挑战一直无人接单，挑战主要来自两方面：</p><ul><li>对 MySQL 复制通道与协议非常熟悉</li><li>对 ClickHouse 整体机制非常熟悉</li></ul><p>这样，在两个本来有点遥远的山头中间架起了一座高速，这条 <a href="https://github.com/ClickHouse/ClickHouse/pull/10851" target="_blank" rel="noopener">10851号</a> 高速由 zhang1024(ClickHouse侧) 和 BohuTANG(MySQL复制) 两个修路工联合承建，目前已经合并到 upstream 分支。</p><p>关于同步 MySQL 的数据，目前大家的方案基本都是在中间安置一个 binlog 消费工具，这个工具对 event 进行解析，然后再转换成 ClickHouse 的 SQL 语句，写到 ClickHouse server，链路较长，性能损耗较大。</p><p> <a href="https://github.com/ClickHouse/ClickHouse/pull/10851" target="_blank" rel="noopener">10851号</a> 高速是在 ClickHouse 内部实现一套 binlog 消费方案，然后根据 event 解析成 ClickHouse 内部的 block 结构，再直接回写到底层存储引擎，几乎是最高效的一种实现方式，实现与 MySQL 实时同步的能力，让分析更接近现实。</p><p>基于 database 级的复制，实现了多源复制的功能，如果复制通道坏掉，我们只需在 ClickHouse 侧删掉 database 再重建一次即可，非常快速、方便，OLTP+OLAP 就是这么简单！</p><p>要想富，先修路！</p><p><a href="/3030/12/12/clickhouse-and-friends-mysql-replication-materializemysql/">【置顶】ClickHouse MaterializeMySQL实时同步MySQL汇总</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="MySQL" scheme="https://bohutang.me/tags/MySQL/"/>
    
      <category term="replication" scheme="https://bohutang.me/tags/replication/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（８）纯手工打造的SQL解析器</title>
    <link href="https://bohutang.me/2020/07/25/clickhouse-and-friends-parser/"/>
    <id>https://bohutang.me/2020/07/25/clickhouse-and-friends-parser/</id>
    <published>2020-07-24T16:00:00.000Z</published>
    <updated>2020-08-10T07:17:16.088Z</updated>
    
    <content type="html"><![CDATA[<p>现实生活中的物品一旦被标记为“纯手工打造”，给人的第一感觉就是“上乘之品”，一个字“贵”，比如北京老布鞋。</p><p>但是在计算机世界里，如果有人告诉你 ClickHouse 的 SQL 解析器是纯手工打造的，是不是很惊讶！<br>这个问题引起了不少网友的关注，所以本篇聊聊 ClickHouse 的纯手工解析器，看看它们的底层工作机制及优缺点。</p><p>枯燥先从一个 SQL 开始:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN SELECT a,b FROM t1</span><br></pre></td></tr></table></figure><h2 id="token"><a href="#token" class="headerlink" title="token "></a><b>token </b></h2><p>首先对 SQL 里的字符逐个做判断，然后根据其关联性做 token 分割：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/parser.png" align="center" style="zoom:40%;" /><p>比如连续的 WordChar，那它就是 BareWord，解析函数在 <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/Lexer.cpp#L61" target="_blank" rel="noopener">Lexer::nextTokenImpl()</a>，解析调用栈：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DB::Lexer::nextTokenImpl() Lexer.cpp:63</span><br><span class="line">DB::Lexer::nextToken() Lexer.cpp:52</span><br><span class="line">DB::Tokens::operator[](unsigned long) TokenIterator.h:36</span><br><span class="line">DB::TokenIterator::get() TokenIterator.h:62</span><br><span class="line">DB::TokenIterator::operator-&gt;() TokenIterator.h:64</span><br><span class="line">DB::tryParseQuery(DB::IParser&amp;, char const*&amp;, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;&amp;, bool, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, bool, unsigned long, unsigned long) parseQuery.cpp:224</span><br><span class="line">DB::parseQueryAndMovePosition(DB::IParser&amp;, char const*&amp;, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, bool, unsigned long, unsigned long) parseQuery.cpp:314</span><br><span class="line">DB::parseQuery(DB::IParser&amp;, char const*, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, unsigned long, unsigned long) parseQuery.cpp:332</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:272</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:731</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:150</span><br></pre></td></tr></table></figure><h2 id="ast"><a href="#ast" class="headerlink" title="ast"></a><b>ast</b></h2><p> token 是最基础的元组，他们之间没有任何关联，只是一堆生冷的词组与符号，所以我们还需对其进行<b>语法解析</b>，让这些 token 之间建立一定的关系，达到一个可描述的活力。</p><p>ClickHouse 在解每一个 token 的时候，会根据当前的 token 进行状态空间进行预判(parse 返回 true 则进入子状态空间继续)，然后决定状态跳转，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN  -- TokenType::BareWord</span><br></pre></td></tr></table></figure><p>逻辑首先会进入Parsers/ParserQuery.cpp 的 <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserQuery.cpp#L26" target="_blank" rel="noopener">ParserQuery::parseImpl</a> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bool res &#x3D; query_with_output_p.parse(pos, node, expected)</span><br><span class="line">    || insert_p.parse(pos, node, expected)</span><br><span class="line">    || use_p.parse(pos, node, expected)</span><br><span class="line">    || set_role_p.parse(pos, node, expected)</span><br><span class="line">    || set_p.parse(pos, node, expected)</span><br><span class="line">    || system_p.parse(pos, node, expected)</span><br><span class="line">    || create_user_p.parse(pos, node, expected)</span><br><span class="line">    || create_role_p.parse(pos, node, expected)</span><br><span class="line">    || create_quota_p.parse(pos, node, expected)</span><br><span class="line">    || create_row_policy_p.parse(pos, node, expected)</span><br><span class="line">    || create_settings_profile_p.parse(pos, node, expected)</span><br><span class="line">    || drop_access_entity_p.parse(pos, node, expected)</span><br><span class="line">    || grant_p.parse(pos, node, expected);</span><br></pre></td></tr></table></figure><p>这里会对所有 query 类型进行 parse 方法的调用，直到有分支返回 true。</p><p>我们来看<b>第一层</b> query_with_output_p.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserQueryWithOutput.cpp#L31" target="_blank" rel="noopener">Parsers/ParserQueryWithOutput.cpp</a>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">bool parsed &#x3D;</span><br><span class="line">       explain_p.parse(pos, query, expected)</span><br><span class="line">    || select_p.parse(pos, query, expected)</span><br><span class="line">    || show_create_access_entity_p.parse(pos, query, expected)</span><br><span class="line">    || show_tables_p.parse(pos, query, expected)</span><br><span class="line">    || table_p.parse(pos, query, expected)</span><br><span class="line">    || describe_table_p.parse(pos, query, expected)</span><br><span class="line">    || show_processlist_p.parse(pos, query, expected)</span><br><span class="line">    || create_p.parse(pos, query, expected)</span><br><span class="line">    || alter_p.parse(pos, query, expected)</span><br><span class="line">    || rename_p.parse(pos, query, expected)</span><br><span class="line">    || drop_p.parse(pos, query, expected)</span><br><span class="line">    || check_p.parse(pos, query, expected)</span><br><span class="line">    || kill_query_p.parse(pos, query, expected)</span><br><span class="line">    || optimize_p.parse(pos, query, expected)</span><br><span class="line">    || watch_p.parse(pos, query, expected)</span><br><span class="line">    || show_access_p.parse(pos, query, expected)</span><br><span class="line">    || show_access_entities_p.parse(pos, query, expected)</span><br><span class="line">    || show_grants_p.parse(pos, query, expected)</span><br><span class="line">    || show_privileges_p.parse(pos, query, expected</span><br></pre></td></tr></table></figure><p>跳进<b>第二层</b> explain_p.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserExplainQuery.cpp#L10" target="_blank" rel="noopener">ParserExplainQuery::parseImpl</a>状态空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">bool ParserExplainQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ASTExplainQuery::ExplainKind kind;</span><br><span class="line">    bool old_syntax &#x3D; false;</span><br><span class="line"></span><br><span class="line">    ParserKeyword s_ast(&quot;AST&quot;);</span><br><span class="line">    ParserKeyword s_analyze(&quot;ANALYZE&quot;);</span><br><span class="line">    ParserKeyword s_explain(&quot;EXPLAIN&quot;);</span><br><span class="line">    ParserKeyword s_syntax(&quot;SYNTAX&quot;);</span><br><span class="line">    ParserKeyword s_pipeline(&quot;PIPELINE&quot;);</span><br><span class="line">    ParserKeyword s_plan(&quot;PLAN&quot;);</span><br><span class="line"></span><br><span class="line">    ... ...</span><br><span class="line">    else if (s_explain.ignore(pos, expected))</span><br><span class="line">    &#123;</span><br><span class="line">       ... ...</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ... ...</span><br><span class="line">    </span><br><span class="line">    ParserSelectWithUnionQuery select_p;</span><br><span class="line">    ASTPtr query;</span><br><span class="line">    if (!select_p.parse(pos, query, expected))</span><br><span class="line">        return false;</span><br><span class="line">    ... ...</span><br></pre></td></tr></table></figure><p>s_explain.ignore 方法会进行一个 keyword 解析，解析出 ast node:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN -- keyword</span><br></pre></td></tr></table></figure><p>跃进<b>第三层</b> select_p.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserSelectWithUnionQuery.cpp#L26" target="_blank" rel="noopener">ParserSelectWithUnionQuery::parseImpl</a>状态空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bool ParserSelectWithUnionQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ASTPtr list_node;</span><br><span class="line"></span><br><span class="line">    ParserList parser(std::make_unique&lt;ParserUnionQueryElement&gt;(), std::make_unique&lt;ParserKeyword&gt;(&quot;UNION ALL&quot;), false);</span><br><span class="line">    if (!parser.parse(pos, list_node, expected))</span><br><span class="line">        return false;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>parser.parse 里又调用<b>第四层</b> <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserSelectQuery.cpp#L24" target="_blank" rel="noopener">ParserSelectQuery::parseImpl</a> 状态空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">bool ParserSelectQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    auto select_query &#x3D; std::make_shared&lt;ASTSelectQuery&gt;();</span><br><span class="line">    node &#x3D; select_query;</span><br><span class="line"></span><br><span class="line">    ParserKeyword s_select(&quot;SELECT&quot;);</span><br><span class="line">    ParserKeyword s_distinct(&quot;DISTINCT&quot;);</span><br><span class="line">    ParserKeyword s_from(&quot;FROM&quot;);</span><br><span class="line">    ParserKeyword s_prewhere(&quot;PREWHERE&quot;);</span><br><span class="line">    ParserKeyword s_where(&quot;WHERE&quot;);</span><br><span class="line">    ParserKeyword s_group_by(&quot;GROUP BY&quot;);</span><br><span class="line">    ParserKeyword s_with(&quot;WITH&quot;);</span><br><span class="line">    ParserKeyword s_totals(&quot;TOTALS&quot;);</span><br><span class="line">    ParserKeyword s_having(&quot;HAVING&quot;);</span><br><span class="line">    ParserKeyword s_order_by(&quot;ORDER BY&quot;);</span><br><span class="line">    ParserKeyword s_limit(&quot;LIMIT&quot;);</span><br><span class="line">    ParserKeyword s_settings(&quot;SETTINGS&quot;);</span><br><span class="line">    ParserKeyword s_by(&quot;BY&quot;);</span><br><span class="line">    ParserKeyword s_rollup(&quot;ROLLUP&quot;);</span><br><span class="line">    ParserKeyword s_cube(&quot;CUBE&quot;);</span><br><span class="line">    ParserKeyword s_top(&quot;TOP&quot;);</span><br><span class="line">    ParserKeyword s_with_ties(&quot;WITH TIES&quot;);</span><br><span class="line">    ParserKeyword s_offset(&quot;OFFSET&quot;);</span><br><span class="line"></span><br><span class="line">    ParserNotEmptyExpressionList exp_list(false);</span><br><span class="line">    ParserNotEmptyExpressionList exp_list_for_with_clause(false);</span><br><span class="line">    ParserNotEmptyExpressionList exp_list_for_select_clause(true);  </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">            if (!exp_list_for_select_clause.parse(pos, select_expression_list, expected))</span><br><span class="line">            return false;</span><br></pre></td></tr></table></figure><p><b>第五层</b> exp_list_for_select_clause.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ExpressionListParsers.cpp#L520" target="_blank" rel="noopener">ParserExpressionList::parseImpl</a>状态空间继续：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bool ParserExpressionList::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    return ParserList(</span><br><span class="line">        std::make_unique&lt;ParserExpressionWithOptionalAlias&gt;(allow_alias_without_as_keyword),</span><br><span class="line">        std::make_unique&lt;ParserToken&gt;(TokenType::Comma))</span><br><span class="line">        .parse(pos, node, expected);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>… … 写不下去个鸟！</p><p>可以发现，ast parser 的时候，预先构造好状态空间，比如 select 的状态空间:</p><ol><li>expression list</li><li>from tables</li><li>where</li><li>group by</li><li>with …</li><li>order by</li><li>limit</li></ol><p>在一个状态空间內，还可以根据 parse 返回的 bool 判断是否继续进入子状态空间，一直递归解析出整个 ast。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>手工 parser 的好处是代码清晰简洁，每个细节可防可控，以及友好的错误处理，改动起来不会一发动全身。<br>缺点是手工成本太高，需要大量的测试来保证其正确性，还需要一些fuzz来保证可靠性。<br>好在ClickHouse 已经实现的比较全面，即使有新的需求，在现有基础上修修补补即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现实生活中的物品一旦被标记为“纯手工打造”，给人的第一感觉就是“上乘之品”，一个字“贵”，比如北京老布鞋。&lt;/p&gt;
&lt;p&gt;但是在计算机世界里，如果有人告诉你 ClickHouse 的 SQL 解析器是纯手工打造的，是不是很惊讶！&lt;br&gt;这个问题引起了不少网友的关注，所以本篇
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（6）MergeTree存储结构</title>
    <link href="https://bohutang.me/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/"/>
    <id>https://bohutang.me/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/</id>
    <published>2020-06-25T16:00:00.000Z</published>
    <updated>2020-08-11T13:37:05.365Z</updated>
    
    <content type="html"><![CDATA[<p>上篇的 <a href="/2020/06/20/clickhouse-and-friends-merge-tree-algo/">存储引擎技术进化与MergeTree</a> 介绍了存储算法的演进。</p><p>存储引擎是一个数据库的底盘，一定要稳和动力澎湃。</p><p>接下来我们将一起来探索下 ClickHouse MergeTree 列式存储引擎，解构下这台“跑车”最重要的部件。</p><p>所有的存储引擎，无论精良与粗制滥造，最终都是要把数据回写到磁盘，来满足存储和索引目的。</p><p>磁盘文件的构造可以说是算法的物理体现，我们甚至可以通过这些存储结构反推出其算法实现。</p><p>所以，要想深入了解一个存储引擎，最好的入手点是它的磁盘存储结构，然后再反观它的读、写机制就会有一种水到渠成的感觉。</p><p>如果这个分析顺序搞反了，会有一种生硬的感觉，网上大部分教程都是这种“生硬”式教学，本文将直击灵魂从最底层谈起，彻底搞明白４个问题：</p><ol><li><p>MergeTree 有哪些文件？</p></li><li><p>MergeTree 数据如何分布？</p></li><li><p>MergeTree 索引如何组织？</p></li><li><p>MergeTree 如何利用索引加速？</p></li></ol><p>话不多说，上表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE default.mt</span><br><span class="line">(</span><br><span class="line">    &#96;a&#96; Int32,</span><br><span class="line">    &#96;b&#96; Int32,</span><br><span class="line">    &#96;c&#96; Int32,</span><br><span class="line">    INDEX &#96;idx_c&#96; (c) TYPE minmax GRANULARITY 1</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; MergeTree</span><br><span class="line">PARTITION BY a </span><br><span class="line">ORDER BY b</span><br><span class="line">SETTINGS index_granularity&#x3D;3</span><br></pre></td></tr></table></figure><p>造点数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">insert into default.mt(a,b,c) values(1,1,1);</span><br><span class="line">insert into default.mt(a,b,c) values(5,2,2),(5,3,3);</span><br><span class="line">insert into default.mt(a,b,c) values(3,10,4),(3,9,5),(3,8,6),(3,7,7),(3,6,8),(3,5,9),(3,4,10);</span><br></pre></td></tr></table></figure><h2 id="磁盘文件"><a href="#磁盘文件" class="headerlink" title="磁盘文件"></a><b>磁盘文件</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls ckdatas&#x2F;data&#x2F;default&#x2F;mt&#x2F;</span><br><span class="line">1_4_4_0  3_6_6_0  5_5_5_0  detached  format_version.txt</span><br></pre></td></tr></table></figure><p>可以看到，生成了 3 个数据目录，每个目录在 ClickHouse 里称作一个分区(part)，目录名的前缀正是我们写入时字段 a 的值: 1,3,5，因为表分区是这样定位的：<code>PARTITION BY a</code>。</p><p>现在我们看看 a=3 分区：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls ckdatas&#x2F;data&#x2F;default&#x2F;mt&#x2F;3_6_6_0&#x2F;</span><br><span class="line">a.bin  a.mrk2  b.bin  b.mrk2  c.bin  checksums.txt  c.mrk2  columns.txt  count.txt  minmax_a.idx  partition.dat  primary.idx  skp_idx_idx_c.idx  skp_idx_idx_c.mrk2</span><br></pre></td></tr></table></figure><ul><li>*.bin 是列数据文件，按主键排序(ORDER BY)，这里是按照字段 b 进行排序</li><li>*.mrk2 mark 文件，目的是快速定位 bin 文件数据位置</li><li>minmax_a.idx 分区键 min-max 索引文件，目的是加速分区键 a 查找</li><li>primay.idx 主键索引文件，目的是加速主键 b 查找</li><li>skp_idx_idx_c.* 字段 c 索引文件，目的是加速 c 的查找</li></ul><p>在磁盘上，MergeTree 只有一种物理排序，就是 ORDER BY 的主键序，其他文件(比如 .mrk/.idx)是一种逻辑加速，围绕仅有的一份物理排序，要解决的问题是：</p><p><b>在以字段 b 物理排序上，如何实现字段 a、字段 c 的快速查找？</b></p><p>MergeTree 引擎概括起来很简单：<br>整个数据集通过分区字段被划分为多个物理分区，每个分区內又通过逻辑文件围绕仅有的一种物理排序进行加速查找。</p><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a><b>存储结构</b></h2><h3 id="数据文件"><a href="#数据文件" class="headerlink" title="数据文件"></a><b>数据文件</b></h3><p>对于单个物理分区內的存储结构，首先要明确一点，MergeTree 的数据只有一份：*.bin。</p><p>a.bin 是字段 a 的数据，b.bin 是字段 b 的数据，c.bin 是字段 c 的数据，也就是大家熟悉的列存储。</p><p>各个 bin 文件以 b.bin排序对齐（b 是排序键），如图：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-without-granule.png" align="center" style="zoom:45%;" /><p>这样会有一个比较严重的问题：<br>如果 *.bin 文件较大，即使读取一行数据，也要加载整个 bin 文件，浪费了大量的 IO，没法忍。</p><h3 id="granule"><a href="#granule" class="headerlink" title="granule"></a><b>granule</b></h3><p>高、黑科技来了，ClickHouse MergeTree 把 bin 文件根据颗粒度(GRANULARITY)划分为多个颗粒(granule)，每个 granule 单独压缩存储。</p><p><code>SETTINGS index_granularity=3</code>  表示每 ３ 行数据为一个 granule，分区目前只有 ７ 条数据，所以被划分成 3 个 granule(三个色块)：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-granule.png" align="center" style="zoom:50%;" /><p>为方便读取某个 granule，使用 *.mrk 文件记录每个 granule 的 offset，每个 granule 的 header 里会记录一些元信息，用于读取解析:</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-marker.png" align="center" style="zoom:50%;" /><p>这样，我们就可以根据 ｍark 文件，直接定位到想要的 granule，然后对这个单独的 granule 进行读取、校验。</p><p>目前，我们还有缺少一种映射：每个 mark 与字段值之间的对应，哪些值区间落在 mark0，哪些落在 mark1 …？</p><p>有了这个映射，就可以实现最小化读取 granule 来加速查询：</p><ol><li>根据查询条件确定需要哪些 mark</li><li>根据 mark 读取相应的 granule</li></ol><h3 id="存储排序"><a href="#存储排序" class="headerlink" title="存储排序"></a><b>存储排序</b></h3><p>在了解 MergeTree 索引机制之前，需要明白以下两点：</p><ol><li><p>只有一份全量数据，存储在 *.bin 文件</p></li><li><p>*.bin 按照 ORDER BY 字段降序存储</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-orderby-sort.png" align="center" style="zoom:45%;" /></li></ol><h3 id="稀疏索引"><a href="#稀疏索引" class="headerlink" title="稀疏索引"></a><b>稀疏索引</b></h3><p>因为数据只有一份且只有一种物理排序，MergeTree在索引设计上选择了简单、高效的稀疏索引模式。</p><p>什么是稀疏索引呢？就是从已经排序的全量数据里，间隔性的选取一些点，并记录这些点属于哪个 mark。</p><h4 id="1-primary-index"><a href="#1-primary-index" class="headerlink" title="1. primary index"></a><b>1. primary index</b></h4><p>主键索引，可通过<code>[PRIMARY KEY expr]</code>指定，默认是 ORDER BY 字段值。</p><p>注意 ClickHouse primary index 跟 MySQL primary key 不是一个概念。</p><p>在稀疏点的选择上，取每个 granule 最小值：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-primary-key.png" align="center" style="zoom:45%;" /><h4 id="2-skipping-index"><a href="#2-skipping-index" class="headerlink" title="2. skipping index"></a><b>2. skipping index</b></h4><p>普通索引。</p><p><code>INDEX</code>idx_c<code>(c) TYPE minmax GRANULARITY 1</code> 针对字段 c 创建一个 minmax 模式索引。</p><p><code>GRANULARITY</code> 是稀疏点选择上的 granule 颗粒度，<code>GRANULARITY 1</code> 表示每 1 个 granule 选取一个：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-skipping-index-g1.png" align="center" style="zoom:40%;" /><p>如果定义为<code>GRANULARITY 2</code> ，则 2 个 granule 选取一个：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-skipping-index-g2.png" align="center" style="zoom:40%;" /><h4 id="3-partition-minmax-index"><a href="#3-partition-minmax-index" class="headerlink" title="3. partition minmax index"></a><b>3. partition minmax index</b></h4><p>针对分区键，MergeTree 还会创建一个 min/max 索引，来加速分区选择。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-minmax-idx.png" align="center" style="zoom:40%;" /><h4 id="4-全景图"><a href="#4-全景图" class="headerlink" title="4. 全景图 "></a><b>4. 全景图 </b></h4><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-layout.png" align="center" style="zoom:95%;" /><h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a><b>查询优化</b></h2><p>现在熟悉了 MergeTree 的存储结构，我们通过几个查询来体验下。</p><h3 id="1-分区键查询"><a href="#1-分区键查询" class="headerlink" title="1. 分区键查询"></a><b>1. 分区键查询</b></h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from default.mt where a&#x3D;3</span><br></pre></td></tr></table></figure><p>查询会直接根据 <code>a=3</code> 定位到单个分区:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;Debug&gt; InterpreterSelectQuery: MergeTreeWhereOptimizer: condition &quot;a &#x3D; 3&quot; moved to PREWHERE</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Key condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: (column 0 in [3, 3])</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Selected 1 parts by a, 1 parts by key, 3 marks by primary key, 3 marks to read from 1 ranges</span><br><span class="line">┌─a─┬──b─┬──c─┐</span><br><span class="line">│ 3 │  4 │ 10 │</span><br><span class="line">│ 3 │  5 │  9 │</span><br><span class="line">│ 3 │  6 │  8 │</span><br><span class="line">│ 3 │  7 │  7 │</span><br><span class="line">│ 3 │  8 │  6 │</span><br><span class="line">│ 3 │  9 │  5 │</span><br><span class="line">│ 3 │ 10 │  4 │</span><br><span class="line">└───┴────┴────┘</span><br></pre></td></tr></table></figure><h3 id="2-主键索引查询"><a href="#2-主键索引查询" class="headerlink" title="2. 主键索引查询"></a><b>2. 主键索引查询</b></h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from default.mt where b&#x3D;5</span><br></pre></td></tr></table></figure><p>查询会先从 3 个分区读取 prmary.idx，然后定位到只有一个分区符合条件，找到要读取的 mark:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Key condition: (column 0 in [5, 5])</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Selected 3 parts by a, 1 parts by key, 1 marks by primary key, 1 marks to read from 1 ranges</span><br><span class="line">┌─a─┬─b─┬─c─┐</span><br><span class="line">│ 3 │ 5 │ 9 │</span><br><span class="line">└───┴───┴───┘</span><br></pre></td></tr></table></figure><h3 id="3-索引查询"><a href="#3-索引查询" class="headerlink" title="3. 索引查询"></a><b>3. 索引查询</b></h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from default.mt where b&#x3D;5</span><br></pre></td></tr></table></figure><p>查询会先从 3 个分区读取 prmary.idx 和 skp_idx_idx_c.idx 进行 granule 过滤（没用的 drop 掉），然后定位到只有 3_x_x_x 分区的一个 granule 符合条件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;Debug&gt; InterpreterSelectQuery: MergeTreeWhereOptimizer: condition &quot;c &#x3D; 5&quot; moved to PREWHERE</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Key condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Index &#96;idx_c&#96; has dropped 1 &#x2F; 1 granules.</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Index &#96;idx_c&#96; has dropped 1 &#x2F; 1 granules.</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Index &#96;idx_c&#96; has dropped 2 &#x2F; 3 granules.</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Selected 3 parts by a, 1 parts by key, 5 marks by primary key, 1 marks to read from 1 ranges</span><br><span class="line">┌─a─┬─b─┬─c─┐</span><br><span class="line">│ 3 │ 9 │ 5 │</span><br><span class="line">└───┴───┴───┘</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>本文从磁盘存储结构入手，分析 ClickHouse MergeTree 的存储、索引设计。</p><p>只有了解了这些底层机制，我们才好对自己的 SQL 和表结构进行优化，使其执行更加高效。</p><p>ClickHouse MergeTree 设计简单、高效，它首要解决的问题是：在一种物理排序上，如何实现快速查找。</p><p>针对这个问题，ClickHouse使用稀疏索引来解决。</p><p>在官方 roadmap 上，列举了一个有意思的索引方向：Z-Order Indexing，目的是把多个维度编码到一维存储，当我们给出多维度条件的时候，可以快速定位到这个条件点集的空间位置，目前 ClickHouse 针对这个索引设计暂无进展。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上篇的 &lt;a href=&quot;/2020/06/20/clickhouse-and-friends-merge-tree-algo/&quot;&gt;存储引擎技术进化与MergeTree&lt;/a&gt; 介绍了存储算法的演进。&lt;/p&gt;
&lt;p&gt;存储引擎是一个数据库的底盘，一定要稳和动力澎湃。&lt;/p&gt;

      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="lsm" scheme="https://bohutang.me/tags/lsm/"/>
    
      <category term="mergetree" scheme="https://bohutang.me/tags/mergetree/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（5）存储引擎技术进化与MergeTree</title>
    <link href="https://bohutang.me/2020/06/20/clickhouse-and-friends-merge-tree-algo/"/>
    <id>https://bohutang.me/2020/06/20/clickhouse-and-friends-merge-tree-algo/</id>
    <published>2020-06-19T16:00:00.000Z</published>
    <updated>2020-08-11T13:37:05.365Z</updated>
    
    <content type="html"><![CDATA[<p>21 世纪的第二个 10 年，虎哥已经在存储引擎一线奋战近 10 年，由于强大的兴趣驱动，这么多年来几乎不放过 arXiv 上与存储相关的每一篇 paper。<br>尤其是看到带有 draft 的 paper 时，有一种乞丐听到“叮当”响时的愉悦。<br>看paper这玩意就像鉴宝，多数是“赝品”，需要你有“鉴真”的本领，否则今天是张三的算法超越xx，明儿又是王二的硬件提升了yy，让你永远跟不上节奏zz，湮灭在这些没有营养的技术垃圾中，浪费大好青春。</p><p>言归正传，接下来的3篇，跟 ClickHouse 的 MergeTree 引擎有关：<br><b>上篇介绍存储引擎的技术演进史</b>，从”远古”的 B-tree 出发推演到目前主流的技术架构。<br><b><a href="/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/">中篇会从存储结构介绍 MergeTree 原理</a></b>，对 ClickHouse MergeTree 有一个深入的认识，如何合理设计来进行科学加速。<br><b>下篇会从MergeTree代码出发</b>，看看 ClickHouse MergeTree 如何实现读、写。</p><p>本文为上篇，先来个热身，相信本篇大部分内容对大家来说都比较陌生，很少人写过。</p><h2 id="地位"><a href="#地位" class="headerlink" title="地位"></a><b>地位</b></h2><p>存储引擎(事务型)在一个数据库(DBMS)中的地位如何呢？</p><p>MySQL 的商业成功可以说大部分来自于 InnoDB 引擎，Oracle 收购 InnoDB 比 MySQL 早好几年呢！<br>20年前，能亲手撸一套 <a href="https://en.wikipedia.org/wiki/Algorithms_for_Recovery_and_Isolation_Exploiting_Semantics" target="_blank" rel="noopener">ARIES (Algorithms for Recovery and Isolation Exploiting Semantics)</a> 规范引擎，实力还是相当震撼的，相信 Oracle 收购的不仅是 InnoDB 这个引擎，更重要的是人， InnoDB 作者在哪里，在干什么？！<br>Fork 出来的 MariaDB 这么多年一直找不到自己的灵魂，在 Server 层磨磨蹭蹭可谓是江河日下，只能四处收购碰碰运气，当年 TokuDB 战斗过的 commit 依在，但这些已经是历史了。<br>另，WiredTiger 被 MongoDB 收购并使用，对整个生态所起的作用也是无可估量的，这些发动机引擎对于一辆汽车是非常重要的。</p><p>有人问道，都已经 2020 年了，开发一个存储引擎还这么难吗？不难，但是造出来的未必有 RocksDB 好用？！<br>如大家所见，很多的分布式存储引擎都是基于 RocksDB 研发，可谓短期内还算明智的选择。<br>从工程角度来看，一个 ACID 引擎要打磨的东西非常之多，到处充斥着人力、钱力、耐心的消耗，一种可能是写到一半就停滞了(如 <a href="https://github.com/BohuTANG/nessDB" target="_blank" rel="noopener">nessDB</a>)，还有一种可能是写着写着发现跟xx很像，沃茨法克。<br>当然，这里并不是鼓励大家都去基于 RocksDB 去构建自己的产品，而是要根据自己的情况去做选择。</p><h2 id="B-tree"><a href="#B-tree" class="headerlink" title="B-tree"></a><b>B-tree</b></h2><p>首先要尊称一声大爷，这个大爷年方 50，目前支撑着数据库产业的半壁江山。</p><p>50 年来不变而且人们还没有改变它的意向，这个大爷厉害的很！<br>鉴定一个算法的优劣，有一个学派叫 <b>IO复杂度分析</b>，简单推演真假便知。<br>下面就用此法分析下 B-tree(traditional b-tree) 的 IO 复杂度，对读、写 IO 一目了然，真正明白读为什么快，写为什么慢，如何优化。<br>为了可以愉快的阅读，本文不会做任何公式推导，复杂度分析怎么可能没有公式呢！</p><h3 id="读IO分析"><a href="#读IO分析" class="headerlink" title="读IO分析"></a><b>读IO分析</b></h3><p>这里有一个 3-level 的 B-tree，每个方块代表一个 page，数字代表 page ID。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-read.png" align="center" style="zoom:40%;" /><p>上图 B-tree 结构是<b>内存</b>的一个表现形式，如果我们要读取的记录在 leaf-8上，read-path 如蓝色箭头所示:<br>root-9 –&gt; branch-6 –&gt; leaf-8</p><p>下图是 B-tree 在<b>磁盘</b>上的存储形式，meta page 是起点:<br><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-read-disk.png" align="center" style="zoom:40%;" /></p><p>这样读取的随机 IO (假设内存里没有 page 缓存且 page 存储是随机的)总数就是(蓝色箭头):</p><p>1(meta-10)IO +  1(root-9)IO + 1(branch-6)IO + 1(leaf-8)IO = 4次 IO，这里忽略一直缓存的 meta 和 root，就是 <b>2</b> 次随机 IO。<br>如果磁盘 seek 是 1ms，读取延迟就是 <b>2ms</b>。</p><p>通过推演就会发现，B-tree 是一种读优化(Read-Optimized)的数据结构，无论 LSM-tree 还是 Fractal-tree 等在读上只能比它慢，因为读放大(Read Amplification)问题。<br>存储引擎算法可谓日新月异，但是大部分都是在跟写优化(Write-Optimized)做斗争，那怕是一个常数项的优化那就是突破，自从 Fractal-tree 突破后再无来者了！</p><h3 id="写IO分析"><a href="#写IO分析" class="headerlink" title="写IO分析"></a><b>写IO分析</b></h3><p>现在写一条记录到 leaf-8。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-update-raw.png" align="center" style="zoom:40%;" /><p>可以发现，每次写都需要先读取一遍，如上图蓝色路径所示。</p><p>假设这次写入导致 root, branch 都发生了变化，这种 in-place 的更新反映到磁盘上就是：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-update-raw-disk.png" align="center" style="zoom:40%;" /><p>基本是 <b>2</b> 次读 IO和写 <b>2</b> 次写 IO+WAL fsync，粗略为 <b>4</b> 次随机 IO。<br>通过分析发现，B-tree 对写操作不太友好，随机 IO 次数较多，而且 in-place 更新必须增加一个 page 级的 WAL 保证失败回滚，简直是要命。</p><h3 id="Write-Optimized-B-tree"><a href="#Write-Optimized-B-tree" class="headerlink" title="Write-Optimized B-tree"></a><b>Write-Optimized B-tree</b></h3><p>说到写优化，在机械盘的年代，大家的方向基本是把随机 IO 转换为顺序 IO，充分发挥磁盘的机械优势，于是出现一种 Append-only B-tree：<br><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-aof.png" align="center" style="zoom:40%;" /></p><ol><li>更新生成新的 page(蓝色)</li><li>page 回写磁盘时 append only 到文件末尾</li><li>无需 page WAL，数据不 overwrite，有写放大(Write Amplification)问题，需要做空洞重利用机制</li></ol><p>Append-only B-tree 节省了回写时的 2 次随机 IO，转换为常数级(constant)的1次顺序 IO，写性能大幅提升，总结起来就是：<br><b>随机变顺序，空间换时间</b><br>LSM-tree, Fractal-tree 等写优化算法的核心思想也是这个，只不过其实现机制不同。</p><h2 id="LSM-trees"><a href="#LSM-trees" class="headerlink" title="LSM-trees"></a><b>LSM-trees</b></h2><p>随着 LevelDB 的问世，LSM-tree 逐渐被大家所熟知。<br>LSM-tree 更像一种思想，模糊了 B-tree 里 tree 的严肃性，通过文件组织成一个更加松散的 tree。<br>这里不谈一个具体的 LSM-tree 是 Leveled 还是 Size-tiered，只谈大体思想。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/lsm-tree.png" align="center" style="zoom:40%;" /><h3 id="写入"><a href="#写入" class="headerlink" title="写入"></a><b>写入</b></h3><ol><li><p>先写入内存的 C0</p></li><li><p>后台线程根据规则(Leveled/Sized)进行 merge，C0 –&gt; C1, C1 –&gt; C2 … CL</p></li><li><p>写入 C0 即可返回，IO 放到后台的 Merge 过程</p></li><li><p>每次 Merge 是硬伤，动作大就抖，动作小性能不好，每次 Merge 的数据流向不明确</p></li><li><p>写放大问题</p></li></ol><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a><b>读取</b></h3><ol><li>读取 C0</li><li>读取 C1 .. CL</li><li>合并记录返回</li><li>读放大问题</li></ol><h2 id="Fractal-tree"><a href="#Fractal-tree" class="headerlink" title="Fractal-tree"></a><b>Fractal-tree</b></h2><p>终于发展到了“终极”优化(目前最先进的索引算法)，Fractal-tree。<br>它是在 Append-only B-tree 的基础上，对每个 branch 节点增加了一个 message buffer 作为缓冲，可以看做是 LSM-tree 和 Append-only B-tree 完美合体。</p><p>相对于 LSM-tree 它的优势非常明显:<br>Merge 更加有序，数据流向非常分明，消除了 Merge 的抖动问题，大家一直寻找的 compaction 防抖方案一直存在的！</p><p>这个高科技目前只有 <a href="https://github.com/xelabs/tokudb" target="_blank" rel="noopener">TokuDB</a> 在使用，这个算法可以开篇新介，这里不做累述，感兴趣的可以参考原型实现 <a href="https://github.com/BohuTANG/nessDB" target="_blank" rel="noopener">nessDB</a>。</p><h2 id="Cache-oblivious"><a href="#Cache-oblivious" class="headerlink" title="Cache-oblivious"></a><b>Cache-oblivious</b></h2><p>这个词对于大部分人都是陌生的，不过别怕。<br>在存储引擎里，有一个数据结构非常非常重要，它负责 page 数据有序性维护，比如在一个 page 里怎么快速定位到我要的记录。<br>在 LevelDB 里使用 skiplist，但大部分引擎使用的是一个有序数组来表示，比如 [1, 2, 3, … 100]，然后使用二分查找。</p><p>大概 10 年前一位内核开发者发表了一篇 &lt;<a href="https://queue.acm.org/detail.cfm?id=1814327" target="_blank" rel="noopener">You’re Doing It Wrong</a>&gt;，这个小文讲了一个很有意思的事情:<br>数据的组织形式对性能有很大的影响，因为 CPU有 cache line。</p><p>抛开这篇文章不谈，咱们来看一张“神仙”图：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/veb-layout.png" align="center" style="zoom:40%;" /><p>这是一个 binary-tree 的 4 种 layout 表示形式，那么哪种 layout 对 CPU cache line 最友好？</p><p>也许你已经猜对了，那就是 van Emde Boas，简称 vEB。<br>因为它的相邻数据“扎堆”存储，point-query 和 range-query 的 cache line 可以最大化共享，skiplist 对 cache line 是非常不友好的，还可以更快！<br>对于 cache oblivious 数据结构，这里有一个简单的原型实现: <a href="https://github.com/BohuTANG/omt" target="_blank" rel="noopener">omt</a></p><h2 id="B-tree优化魔力象限"><a href="#B-tree优化魔力象限" class="headerlink" title="B-tree优化魔力象限"></a><b>B-tree优化魔力象限</b></h2><p>写优化算法从原生的 B-tree 到 Append-only B-tree(代表作 LMDB)，又到 LSM-tree(LevelDB/RocksDB 等)，最后进化到目前最先进的 Fractal-tree (TokuDB)。<br>这些算法耗费了很多年才在工程上实现并被认可，研发一款存储引擎缺的不是算法而是“鉴宝”的能力，这个“宝”可能已经躺了几十年了。</p><p>其实，”科学家”们已经总结出一个 B-tree 优化魔力象限:</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-optimal-curve.png" align="center" style="zoom:50%;" /><p>横坐标是写性能，纵坐标是读性能，B-tree 和 Logging 数据结构分布在曲线的两个极端。<br>B-tree 的读性能非常好，但是写性能差。<br>Logging 的写性能非常好，但是读性能差(想想我们每次写都把数据追加到文件末尾，是不是很快？但是读…)。</p><p>在它们中间有一个优化曲度(Optimal Curve)。<br>在这个曲度上，你可以通过增加/减少一个常数(1-epsilon)来做读和写优化组合，LSM-tree/Fractal-tree 都在这个曲度之上。  </p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-epsilon.png" align="center" style="zoom:20%;" /><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>本文主要讨论事务性引擎的技术演进，其中包含了 IO 复杂度分析，其实这个分析是基于一个 DAM(Disk Access Machine)模型，这里不再展开。<br>这个模型要解决什么问题呢？<br>如果工程中涉及硬件层级关系，比如 Disk / Memory / CPU，数据在Disk，读取(以 block 为单位)到 Memory，查找计算(cache-line)在 CPU，<br>不同介质间性能差距又非常之大，我们怎么做才能让整体性能更优的问题。<br>和当今的硬件相融合，这个模型也一样适用。</p><p>最后回到 ClickHouse 的 MergeTree 引擎，它只使用了本文中的部分优化，实现也比较简洁、高效，毕竟没有事务，撸起来也没啥心理负担。<br><strong>随机变顺序，空间换时间</strong>， MergeTree 原理，请听下回分解。</p><h2 id="References"><a href="#References" class="headerlink" title=" References"></a><b> References</b></h2><p>[1] <a href="https://www.cs.au.dk/~gerth/papers/cacheoblivious05.pdf" target="_blank" rel="noopener">Cache-Oblivious Data Structures</a><br>[2] <a href="https://www3.cs.stonybrook.edu/~bender/talks/2013-BenderKuszmaul-xldb-tutorial.pdf" target="_blank" rel="noopener">Data Structures and Algorithms for Big Databases</a><br>[3] <a href="https://link.springer.com/chapter/10.1007%2F3-540-60220-8_74" target="_blank" rel="noopener">The buffer tree: A new technique for optimal I/O-algorithms</a><br>[4] <a href="http://www.bzero.se/ldapd/btree.html" target="_blank" rel="noopener">how the append-only btree works</a><br>[5] <a href="https://www.douban.com/note/269741273/" target="_blank" rel="noopener">写优化的数据结构(1):AOF和b-tree之间</a><br>[6] <a href="https://www.douban.com/note/269744617/" target="_blank" rel="noopener">写优化的数据结构(2):buffered tree</a><br>[7] <a href="https://www.douban.com/note/304123656/" target="_blank" rel="noopener">存储引擎数据结构优化(1):cpu bound</a><br>[8] <a href="https://www.douban.com/note/304349195/" target="_blank" rel="noopener">存储引擎数据结构优化(2):io bound</a><br>[9] <a href="https://github.com/BohuTANG/nessDB" target="_blank" rel="noopener">nessDB</a><br>[10] <a href="https://github.com/BohuTANG/omt" target="_blank" rel="noopener">omt</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;21 世纪的第二个 10 年，虎哥已经在存储引擎一线奋战近 10 年，由于强大的兴趣驱动，这么多年来几乎不放过 arXiv 上与存储相关的每一篇 paper。&lt;br&gt;尤其是看到带有 draft 的 paper 时，有一种乞丐听到“叮当”响时的愉悦。&lt;br&gt;看paper这玩意
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="lsm" scheme="https://bohutang.me/tags/lsm/"/>
    
      <category term="btree" scheme="https://bohutang.me/tags/btree/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（4）Pipeline处理器和调度器</title>
    <link href="https://bohutang.me/2020/06/11/clickhouse-and-friends-processor/"/>
    <id>https://bohutang.me/2020/06/11/clickhouse-and-friends-processor/</id>
    <published>2020-06-10T16:00:00.000Z</published>
    <updated>2020-08-22T02:41:40.084Z</updated>
    
    <content type="html"><![CDATA[<p><b>最后更新: 2020-08-15</b></p><p>本文谈下 ClickHouse 核心科技：处理器 Processor 和有向无环调度器 DAG Scheduler。</p><p>这些概念并不是 ClickHouse 首创，感兴趣的同学可以关注下 <a href="https://github.com/MaterializeInc/materialize" target="_blank" rel="noopener">materialize</a>的 <a href="https://github.com/TimelyDataflow/timely-dataflow" target="_blank" rel="noopener">timely-dataflow</a>，虎哥用golang 也写过一个<a href="https://github.com/vectorengine/vectorsql/tree/master/src/processors" target="_blank" rel="noopener">原型</a>。</p><p>拼的是实现细节，正是这些模块的精良设计，才有了 ClickHous e整体的高性能。</p><h2 id="Pipeline问题"><a href="#Pipeline问题" class="headerlink" title="Pipeline问题"></a><b>Pipeline问题</b></h2><p>在传统数据库系统中，一个 Query 处理流程大体是:<br><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/processor-plan.png" align="center" style="zoom:50%;" /></p><p>其中在Plan阶段，往往会增加一个 Pipeline 组装(一个 transformer 代表一次数据处理)：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/processor-transformer.png" align="center" style="zoom:50%;" /><p>所有 transformer 被编排成一个流水线(pipeline)，然后交给 executor 串行式执行，每执行一个 transformer 数据集就会被加工并输出，一直到下游的 sinker。<br>可以看到，这种模型的优点是<b>简单</b>，缺点是<b>性能低</b>，无法发挥 CPU 的<b>并行</b>能力，通常叫火山模型(<em>volcano</em>-style)，对于 OLTP 低延迟来说足够，对于计算密集的 OLAP 来说是远远不够的，CPU 不到 100% 就是犯罪！</p><p>对于上面的例子，如果 transformer1 和 transformer2 没有交集，那么它们就可以并行处理：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/processor-transformer2.png" align="center" style="zoom:50%;" /><p>这样就涉及到一些比较灵魂的问题：</p><ol><li>如何实现 transformer 的灵活编排？</li><li>如何实现 transformer 间的数据同步？</li><li>如何实现 transformer 间的并行调度？</li></ol><h2 id="Processor-和-DAG-Scheduler"><a href="#Processor-和-DAG-Scheduler" class="headerlink" title="Processor 和 DAG Scheduler"></a><b>Processor 和 DAG Scheduler</b></h2><h3 id="1-Transformer-编排"><a href="#1-Transformer-编排" class="headerlink" title="1. Transformer 编排"></a><b>1. Transformer 编排</b></h3><p>ClickHouse 实现了一系列基础 transformer 模块，见 <a href="https://github.com/ClickHouse/ClickHouse/tree/master/src/Processors/Transforms" target="_blank" rel="noopener">src/Processors/Transforms</a>，比如:</p><ul><li>FilterTransform       – WHERE 条件过滤</li><li>SortingTransform   – ORDER BY 排序</li><li>LimitByTransform  – LIMIT 裁剪</li></ul><p>当我们执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM t1 WHERE id&#x3D;1 ORDER BY time DESC LIMIT 10</span><br></pre></td></tr></table></figure><p>对于 ClickHouse 的 QueryPipeline 来说，它会按照以下方式进行编排组装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">QueryPipeline::addSimpleTransform(Source)</span><br><span class="line">QueryPipeline::addSimpleTransform(FilterTransform)</span><br><span class="line">QueryPipeline::addSimpleTransform(SortingTransform)</span><br><span class="line">QueryPipeline::addSimpleTransform(LimitByTransform)</span><br><span class="line">QueryPipeline::addSimpleTransform(Sinker)</span><br></pre></td></tr></table></figure><p>这样就实现了 Transformer 的编排，但是执行时数据如何进行同步呢？</p><h3 id="2-Transformer-数据同步"><a href="#2-Transformer-数据同步" class="headerlink" title="2. Transformer 数据同步"></a><b>2. Transformer 数据同步</b></h3><p>当 QueryPipeline 进行 transformer 编排时，我们还需要进行更加底层的 DAG 连通构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">connect(Source.OutPort, FilterTransform.InPort)</span><br><span class="line">connect(FilterTransform.OutPort, SortingTransform.InPort)</span><br><span class="line">connect(SortingTransform.OutPort, LimitByTransform.InPort)</span><br><span class="line">connect(LimitByTransform.OutPort, Sinker.InPort)</span><br></pre></td></tr></table></figure><p>这样就实现了数据的流向关系，一个 transformer 的 OutPort 对接另外一个的 InPort，就像我们现实中的水管管道一样，接口有 3 通甚至多通。</p><h3 id="3-Transformer-执行调度"><a href="#3-Transformer-执行调度" class="headerlink" title="3. Transformer 执行调度"></a><b>3. Transformer 执行调度</b></h3><p>现在管道组装起来了，那么管道内的水如何进行处理和给压流动呢？</p><p>ClickHouse 定义了一套 transform 状态，processor 根据这些状态来实现调度。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="class"><span class="keyword">class</span> <span class="title">Status</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    NeedData  <span class="comment">// 等待数据流进入</span></span><br><span class="line">    PortFull, <span class="comment">// 管道流出端阻塞</span></span><br><span class="line">    Finished, <span class="comment">// 完成状态，退出</span></span><br><span class="line">    Ready,    <span class="comment">// 切换到 work 函数，进行逻辑处理</span></span><br><span class="line">    Async,    <span class="comment">// 切换到 schedule 函数，进行异步处理</span></span><br><span class="line">    Wait,     <span class="comment">// 等待异步处理</span></span><br><span class="line">    ExpandPipeline,      <span class="comment">// Pipeline 需要裂变</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>当 source 生成数据后，它的状态会设置为 PortFull，意思是等着流入其他 transformer 的 InPort，processor 会开始调度 FilterTransformer(NeedData) 的 Prepare，进行 PullData，然后它的状态设置为 Ready，等待 processor 调度 Work 方法进行数据Filter处理，大家就这样靠状态让 processor 去感知，来调度和做状态迁移，直到 Finished 状态。</p><p>这里值得一提的是 ExpandPipeline 状态，它会根据 transformer 的实现，可以把一个 transformer 裂变出更多个 transformer 并行执行，达到一个爆炸效果。</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a><b>Example</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT number + 1 FROM t1;</span><br></pre></td></tr></table></figure><p>为了更加深入理解 ClickHouse 的 processor 和 scheduler 机制，我们来一个原生态的 example:</p><ol><li>一个 Source:{0,1,2,3,4}</li><li>AdderTransformer 对每个数字做加1操作</li><li>一个 Sinker，输出结果</li></ol><h3 id="1-Source"><a href="#1-Source" class="headerlink" title="1. Source"></a><b>1. Source</b></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySource</span> :</span> <span class="keyword">public</span> ISource</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">String</span> <span class="title">getName</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">"MySource"</span>; &#125;</span><br><span class="line"></span><br><span class="line">    MySource(UInt64 end_)</span><br><span class="line">        : ISource(Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)), <span class="built_in">end</span>(end_)</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    UInt64 <span class="built_in">end</span>;</span><br><span class="line">    <span class="keyword">bool</span> done = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Chunk <span class="title">generate</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (done)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> Chunk();</span><br><span class="line">        &#125;</span><br><span class="line">        MutableColumns columns;</span><br><span class="line">        columns.emplace_back(ColumnUInt64::create());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0U</span>; i &lt; <span class="built_in">end</span>; i++)</span><br><span class="line">            columns[<span class="number">0</span>]-&gt;insert(i);</span><br><span class="line"></span><br><span class="line">        done = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> Chunk(<span class="built_in">std</span>::<span class="built_in">move</span>(columns), <span class="built_in">end</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="2-MyAddTransform"><a href="#2-MyAddTransform" class="headerlink" title="2. MyAddTransform"></a><b>2. MyAddTransform</b></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAddTransformer</span> :</span> <span class="keyword">public</span> IProcessor</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">String</span> <span class="title">getName</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">"MyAddTransformer"</span>; &#125;</span><br><span class="line"></span><br><span class="line">    MyAddTransformer()</span><br><span class="line">        : IProcessor(</span><br><span class="line">            &#123;Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)&#125;,</span><br><span class="line">            &#123;Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)&#125;)</span><br><span class="line">        , input(inputs.front())</span><br><span class="line">        , output(outputs.front())</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Status <span class="title">prepare</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (output.isFinished())</span><br><span class="line">        &#123;</span><br><span class="line">            input.<span class="built_in">close</span>();</span><br><span class="line">            <span class="keyword">return</span> Status::Finished;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!output.canPush())</span><br><span class="line">        &#123;</span><br><span class="line">            input.setNotNeeded();</span><br><span class="line">            <span class="keyword">return</span> Status::PortFull;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (has_process_data)</span><br><span class="line">        &#123;</span><br><span class="line">            output.push(<span class="built_in">std</span>::<span class="built_in">move</span>(current_chunk));</span><br><span class="line">            has_process_data = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (input.isFinished())</span><br><span class="line">        &#123;</span><br><span class="line">            output.finish();</span><br><span class="line">            <span class="keyword">return</span> Status::Finished;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!input.hasData())</span><br><span class="line">        &#123;</span><br><span class="line">            input.setNeeded();</span><br><span class="line">            <span class="keyword">return</span> Status::NeedData;</span><br><span class="line">        &#125;</span><br><span class="line">        current_chunk = input.pull(<span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">return</span> Status::Ready;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">work</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> num_rows = current_chunk.getNumRows();</span><br><span class="line">        <span class="keyword">auto</span> result_columns = current_chunk.cloneEmptyColumns();</span><br><span class="line">        <span class="keyword">auto</span> columns = current_chunk.detachColumns();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0U</span>; i &lt; num_rows; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">auto</span> val = columns[<span class="number">0</span>]-&gt;getUInt(i);</span><br><span class="line">            result_columns[<span class="number">0</span>]-&gt;insert(val+<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        current_chunk.setColumns(<span class="built_in">std</span>::<span class="built_in">move</span>(result_columns), num_rows);</span><br><span class="line">        has_process_data = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">InputPort &amp; <span class="title">getInputPort</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> input; &#125;</span><br><span class="line">    <span class="function">OutputPort &amp; <span class="title">getOutputPort</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> output; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="keyword">bool</span> has_input = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">bool</span> has_process_data = <span class="literal">false</span>;</span><br><span class="line">    Chunk current_chunk;</span><br><span class="line">    InputPort &amp; input;</span><br><span class="line">    OutputPort &amp; output;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="3-MySink"><a href="#3-MySink" class="headerlink" title="3. MySink"></a><b>3. MySink</b></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySink</span> :</span> <span class="keyword">public</span> ISink</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">String</span> <span class="title">getName</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">"MySinker"</span>; &#125;</span><br><span class="line"></span><br><span class="line">    MySink() : ISink(Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)) &#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    WriteBufferFromFileDescriptor out&#123;STDOUT_FILENO&#125;;</span><br><span class="line">    FormatSettings settings;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">consume</span><span class="params">(Chunk chunk)</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">size_t</span> rows = chunk.getNumRows();</span><br><span class="line">        <span class="keyword">size_t</span> columns = chunk.getNumColumns();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> row_num = <span class="number">0</span>; row_num &lt; rows; ++row_num)</span><br><span class="line">        &#123;</span><br><span class="line">            writeString(<span class="string">"prefix-"</span>, out);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> column_num = <span class="number">0</span>; column_num &lt; columns; ++column_num)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (column_num != <span class="number">0</span>)</span><br><span class="line">                    writeChar(<span class="string">'\t'</span>, out);</span><br><span class="line">                getPort()</span><br><span class="line">                    .getHeader()</span><br><span class="line">                    .getByPosition(column_num)</span><br><span class="line">                    .type-&gt;serializeAsText(*chunk.getColumns()[column_num], row_num, out, settings);</span><br><span class="line">            &#125;</span><br><span class="line">            writeChar(<span class="string">'\n'</span>, out);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        out.next();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-DAG-Scheduler"><a href="#4-DAG-Scheduler" class="headerlink" title="4. DAG Scheduler"></a><b>4. DAG Scheduler</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">int main(int, char **)</span><br><span class="line">&#123;</span><br><span class="line">    auto source0 &#x3D; std::make_shared&lt;MySource&gt;(5);</span><br><span class="line">    auto add0 &#x3D; std::make_shared&lt;MyAddTransformer&gt;();</span><br><span class="line">    auto sinker0 &#x3D; std::make_shared&lt;MySink&gt;();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;&#x2F; Connect.</span><br><span class="line">    connect(source0-&gt;getPort(), add0-&gt;getInputPort());</span><br><span class="line">    connect(add0-&gt;getOutputPort(), sinker0-&gt;getPort());</span><br><span class="line"></span><br><span class="line">    std::vector&lt;ProcessorPtr&gt; processors &#x3D; &#123;source0, add0, sinker0&#125;;</span><br><span class="line">    PipelineExecutor executor(processors);</span><br><span class="line">    executor.execute(1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从开发者角度看还是比较复杂，状态迁移还需要开发者自己控制，不过 upstream 已经做了大量的基础工作，比如对 source的封装 ISource，对 sink 的封装 ISink，还有一个基础的 ISimpleTransform，让开发者在上层使用 processor 时更加容易，可以积木式搭建出自己想要的 pipeline。</p><p>ClickHouse 的 transformer 数据单元是 Chunk，transformer 对上游 OutPort 流过来的 Chunk 进行加工，然后输出给下游的 InPort，图连通式的流水线并行工作，让 CPU 尽量满负荷工作。</p><p>当一个 SQL 被解析成 AST 后，ClickHouse 根据 AST 构建 Query Plan，然后根据 QueryPlan 构建出 pipeline，最后由  processor 负责调度和执行。<br>目前，ClickHouse 新版本已经默认开启 QueryPipeline，同时这块代码也在不停的迭代。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;b&gt;最后更新: 2020-08-15&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;本文谈下 ClickHouse 核心科技：处理器 Processor 和有向无环调度器 DAG Scheduler。&lt;/p&gt;
&lt;p&gt;这些概念并不是 ClickHouse 首创，感兴趣的同学可以关注下 &lt;a hre
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="processor" scheme="https://bohutang.me/tags/processor/"/>
    
      <category term="pipeline" scheme="https://bohutang.me/tags/pipeline/"/>
    
      <category term="dag scheduler" scheme="https://bohutang.me/tags/dag-scheduler/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（3）MySQL Protocol和Write调用栈</title>
    <link href="https://bohutang.me/2020/06/08/clickhouse-and-friends-mysql-protocol-write-stack/"/>
    <id>https://bohutang.me/2020/06/08/clickhouse-and-friends-mysql-protocol-write-stack/</id>
    <published>2020-06-07T16:00:00.000Z</published>
    <updated>2020-08-11T13:37:05.365Z</updated>
    
    <content type="html"><![CDATA[<p>上篇的<a href="/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/">MySQL Protocol和Read调用</a>里介绍了 ClickHouse 一条查询语句的调用栈，本文继续介绍写的调用栈，开整。</p><h2 id="Write请求"><a href="#Write请求" class="headerlink" title="Write请求"></a><b>Write请求</b></h2><ol><li><p>建表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE test(a UInt8, b UInt8, c UInt8) ENGINE&#x3D;MergeTree() PARTITION BY (a, b) ORDER BY c;</span><br><span class="line">Query OK, 0 rows affected (0.03 sec)</span><br></pre></td></tr></table></figure></li><li><p>写入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO test VALUES(1,1,1), (2,2,2);</span><br></pre></td></tr></table></figure></li></ol><h2 id="调用栈分析"><a href="#调用栈分析" class="headerlink" title="调用栈分析"></a><b>调用栈分析</b></h2><h3 id="1-获取存储引擎-OutputStream"><a href="#1-获取存储引擎-OutputStream" class="headerlink" title="1. 获取存储引擎 OutputStream"></a>1. 获取存储引擎 OutputStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DB::StorageMergeTree::write(std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, DB::Context const&amp;) StorageMergeTree.cpp:174</span><br><span class="line">DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr&lt;DB::IStorage&gt; const&amp;, DB::Context const&amp;, std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, bool) PushingToViewsBlockOutputStream.cpp:110</span><br><span class="line">DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:229</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:364</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:696</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h3 id="2-从-SQL-组装-InputStream"><a href="#2-从-SQL-组装-InputStream" class="headerlink" title="2. 从 SQL 组装 InputStream"></a>2. 从 SQL 组装 InputStream</h3><p><code>(1,1,1), (2,2,2)</code> 如何组装成 inputstream 结构呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DB::InputStreamFromASTInsertQuery::InputStreamFromASTInsertQuery(std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, DB::ReadBuffer*, </span><br><span class="line">DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:300</span><br><span class="line">DB::executeQueryImpl(char const*, char const*, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) executeQuery.cpp:386</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:150</span><br></pre></td></tr></table></figure><p>然后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res.in &#x3D; std::make_shared&lt;InputStreamFromASTInsertQuery&gt;(query_ptr, nullptr, query_sample_block, context, nullptr);</span><br><span class="line">res.in &#x3D; std::make_shared&lt;NullAndDoCopyBlockInputStream&gt;(res.in, out_streams.at(0));</span><br></pre></td></tr></table></figure><p>通过 NullAndDoCopyBlockInputStream的 copyData 方法构造出 Block：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DB::ValuesBlockInputFormat::readRow(std::__1::vector&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt;, std::__1::allocator&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt; &gt; &gt;&amp;, unsigned long) ValuesBlockInputFormat.cpp:93</span><br><span class="line">DB::ValuesBlockInputFormat::generate() ValuesBlockInputFormat.cpp:55</span><br><span class="line">DB::ISource::work() ISource.cpp:48</span><br><span class="line">DB::InputStreamFromInputFormat::readImpl() InputStreamFromInputFormat.h:48</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:57</span><br><span class="line">DB::InputStreamFromASTInsertQuery::readImpl() InputStreamFromASTInsertQuery.h:31</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:57</span><br><span class="line">void DB::copyDataImpl&lt;DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::$_0&amp;, void (&amp;)(DB::Block const&amp;)&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::$_0&amp;, void (&amp;)(DB::Block const&amp;)) copyData.cpp:26</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*) copyData.cpp:62</span><br><span class="line">DB::NullAndDoCopyBlockInputStream::readImpl() NullAndDoCopyBlockInputStream.h:47</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:57</span><br><span class="line">void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:26</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:73</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:785</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:150</span><br></pre></td></tr></table></figure><h3 id="3-组装-OutputStream"><a href="#3-组装-OutputStream" class="headerlink" title="3. 组装 OutputStream"></a>3. 组装 OutputStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:107</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:364</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:696</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><p>组装顺序:</p><ol><li>NullAndDoCopyBlockInputStream</li><li>CountingBlockOutputStream</li><li>AddingDefaultBlockOutputStream</li><li>SquashingBlockOutputStream</li><li>PushingToViewsBlockOutputStream</li><li>MergeTreeBlockOutputStream</li></ol><h3 id="4-写入OutputStream"><a href="#4-写入OutputStream" class="headerlink" title="4. 写入OutputStream"></a>4. 写入OutputStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DB::MergeTreeBlockOutputStream::write(DB::Block const&amp;) MergeTreeBlockOutputStream.cpp:17</span><br><span class="line">DB::PushingToViewsBlockOutputStream::write(DB::Block const&amp;) PushingToViewsBlockOutputStream.cpp:145</span><br><span class="line">DB::SquashingBlockOutputStream::finalize() SquashingBlockOutputStream.cpp:30</span><br><span class="line">DB::SquashingBlockOutputStream::writeSuffix() SquashingBlockOutputStream.cpp:50</span><br><span class="line">DB::AddingDefaultBlockOutputStream::writeSuffix() AddingDefaultBlockOutputStream.cpp:25</span><br><span class="line">DB::CountingBlockOutputStream::writeSuffix() CountingBlockOutputStream.h:37</span><br><span class="line">DB::copyDataImpl&lt;DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::&lt;lambda()&gt;&amp;, void (&amp;)(const DB::Block&amp;)&gt;(DB::IBlockInputStream &amp;, DB::IBlockOutputStream &amp;, &lt;lambda()&gt; &amp;, void (&amp;)(const DB::Block &amp;)) copyData.cpp:52</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*) copyData.cpp:138</span><br><span class="line">DB::NullAndDoCopyBlockInputStream::readImpl() NullAndDoCopyBlockInputStream.h:57</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:60</span><br><span class="line">void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:29</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:154</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:748</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><p>通过 copyData 方法，让数据在 OutputStream 间层层透传，一直到 MergeTreeBlockOutputStream。</p><h3 id="5-返回-Client"><a href="#5-返回-Client" class="headerlink" title="5. 返回 Client"></a>5. 返回 Client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DB::MySQLOutputFormat::finalize() MySQLOutputFormat.cpp:62</span><br><span class="line">DB::IOutputFormat::doWriteSuffix() IOutputFormat.h:78</span><br><span class="line">DB::OutputStreamToOutputFormat::writeSuffix() OutputStreamToOutputFormat.cpp:18</span><br><span class="line">DB::MaterializingBlockOutputStream::writeSuffix() MaterializingBlockOutputStream.h:22</span><br><span class="line">void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:52</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:154</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:748</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO test VALUES(1,1,1), (2,2,2);</span><br></pre></td></tr></table></figure><p>首先内核解析 SQL 语句生成 AST，根据 AST 获取 Interpreter：InterpreterInsertQuery。<br>其次 Interpreter 依次添加相应的 OutputStream。<br>然后从 InputStream 读取数据，写入到 OutputStream，stream 会层层渗透，一直写到底层的存储引擎。<br>最后写入到 Socket Output，返回结果。</p><p>ClickHouse 的 OutputStream 编排还是比较复杂，缺少类似 Pipeline 的调度和编排，但是由于模式比较固化，目前看还算清晰。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上篇的&lt;a href=&quot;/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/&quot;&gt;MySQL Protocol和Read调用&lt;/a&gt;里介绍了 ClickHouse 一条查询语句的调用栈，本文继续介绍写的调用
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mysql" scheme="https://bohutang.me/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（2）MySQL Protocol和Read调用栈</title>
    <link href="https://bohutang.me/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/"/>
    <id>https://bohutang.me/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/</id>
    <published>2020-06-06T16:00:00.000Z</published>
    <updated>2020-08-11T13:43:41.082Z</updated>
    
    <content type="html"><![CDATA[<p>作为一个 OLAP 的 DBMS 来说，有2个端非常重要：</p><ul><li><p>用户如何方便的链进来，这是入口端</p><ul><li>ClickHouse 除了自己的 client 外，还提供了 MySQL/PG/GRPC/HTTP 等接入方式</li></ul></li><li><p>数据如何方便的挂上去，这是数据源端</p><ul><li>ClickHouse 除了自己的引擎外，还可以挂载 MySQL/Kafka 等外部数据源</li></ul></li></ul><p>这样内外互通，多条朋友多条路，以实现“数据”级的编排能力。</p><p>今天谈的是入口端的 MySQL 协议，也是本系列 ClickHouse 的第一个好朋友，用户可通过 MySQL 客户端或相关 Driver 直接链接到 ClickHouse，进行数据读写等操作。</p><p>本文通过 MySQL的 Query 请求，借用调用栈来了解下 ClickHouse 的数据读取全过程。</p><h2 id="如何实现？"><a href="#如何实现？" class="headerlink" title=" 如何实现？"></a><b> 如何实现？</b></h2><p>入口文件在:<br><a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Server/MySQLHandler.cpp" target="_blank" rel="noopener">MySQLHandler.cpp</a></p><h3 id="握手协议"><a href="#握手协议" class="headerlink" title="握手协议"></a><b>握手协议</b></h3><ol><li>MySQLClient 发送 Greeting 数据报文到 MySQLHandler</li><li>MySQLHandler 回复一个 Greeting-Response 报文</li><li>MySQLClient 发送认证报文</li><li>MySQLHandler 对认证报文进行鉴权，并返回鉴权结果</li></ol><p>MySQL Protocol 实现在: <a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/MySQLProtocol.h" target="_blank" rel="noopener">Core/MySQLProtocol.h</a></p><h3 id="Query请求"><a href="#Query请求" class="headerlink" title="Query请求"></a><b>Query请求</b></h3><p>当认证通过后，就可以进行正常的数据交互了。</p><ol><li><p>当 MySQLClient 发送请求:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SELECT * FROM system.numbers LIMIT 5;</span><br></pre></td></tr></table></figure></li><li><p>MySQLHandler 的调用栈：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-&gt;MySQLHandler::comQuery -&gt; executeQuery -&gt; pipeline-&gt;execute -&gt; MySQLOutputFormat::consume</span><br></pre></td></tr></table></figure></li><li><p>MySQLClient 接收到结果</p></li></ol><p>在步骤2里，executeQuery(executeQuery.cpp)非常重要。<br>它是所有前端 Server 和 ClickHouse 内核的接入口，第一个参数是 SQL 文本(‘select 1’)，第二个参数是结果集要发送到哪里去(socket net)。</p><h2 id="调用栈分析"><a href="#调用栈分析" class="headerlink" title="调用栈分析"></a><b>调用栈分析</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM system.numbers LIMIT 5</span><br></pre></td></tr></table></figure><h3 id="1-获取数据源"><a href="#1-获取数据源" class="headerlink" title="1. 获取数据源"></a>1. 获取数据源</h3><p>StorageSystemNumbers 数据源：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DB::StorageSystemNumbers::read(std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::__1::shared_ptr&lt;DB::StorageInMemoryMetadata const&gt; const&amp;, DB::SelectQueryInfo const&amp;, DB::Context const&amp;, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) StorageSystemNumbers.cpp:135</span><br><span class="line">DB::ReadFromStorageStep::ReadFromStorageStep(std::__1::shared_ptr&lt;DB::RWLockImpl::LockHolderImpl&gt;, std::__1::shared_ptr&lt;DB::StorageInMemoryMetadata const&gt;&amp;, DB::SelectQueryOptions, </span><br><span class="line">DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) memory:3028</span><br><span class="line">DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) InterpreterSelectQuery.cpp:1361</span><br><span class="line">DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, std::__1::optional&lt;DB::Pipe&gt;) InterpreterSelectQuery.cpp:791</span><br><span class="line">DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&amp;) InterpreterSelectQuery.cpp:472</span><br><span class="line">DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&amp;) InterpreterSelectWithUnionQuery.cpp:183</span><br><span class="line">DB::InterpreterSelectWithUnionQuery::execute() InterpreterSelectWithUnionQuery.cpp:198</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:385</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, </span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><p>这里最主要的是 ReadFromStorageStep 函数，从不同 storage 里获取数据源 pipe:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pipes pipes &#x3D; storage-&gt;read(required_columns, metadata_snapshot, query_info, *context, processing_stage, max_block_size, max_streams);</span><br></pre></td></tr></table></figure><h3 id="2-Pipeline构造"><a href="#2-Pipeline构造" class="headerlink" title="2. Pipeline构造"></a>2. Pipeline构造</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DB::LimitTransform::LimitTransform(DB::Block const&amp;, unsigned long, unsigned long, unsigned long, bool, bool, std::__1::vector&lt;DB::SortColumnDescription, std::__1::allocator&lt;DB::SortColumnDescription&gt; &gt;) LimitTransform.cpp:21</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:2214</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:2299</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:3570</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:4400</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) LimitStep.cpp:33</span><br><span class="line">DB::ITransformingStep::updatePipeline(std::__1::vector&lt;std::__1::unique_ptr&lt;DB::QueryPipeline, std::__1::default_delete&lt;DB::QueryPipeline&gt; &gt;, std::__1::allocator&lt;std::__1::unique_ptr&lt;DB::QueryPipeline, std::__1::default_delete&lt;DB::QueryPipeline&gt; &gt; &gt; &gt;) ITransformingStep.cpp:21</span><br><span class="line">DB::QueryPlan::buildQueryPipeline() QueryPlan.cpp:154</span><br><span class="line">DB::InterpreterSelectWithUnionQuery::execute() InterpreterSelectWithUnionQuery.cpp:200</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:385</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:722</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h3 id="3-Pipeline执行"><a href="#3-Pipeline执行" class="headerlink" title="3. Pipeline执行"></a>3. Pipeline执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">DB::LimitTransform::prepare(std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;, std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;) LimitTransform.cpp:67</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:291</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::initializeExecution(unsigned long) PipelineExecutor.cpp:747</span><br><span class="line">DB::PipelineExecutor::executeImpl(unsigned long) PipelineExecutor.cpp:764</span><br><span class="line">DB::PipelineExecutor::execute(unsigned long) PipelineExecutor.cpp:479</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:833</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h3 id="4-Output执行发送"><a href="#4-Output执行发送" class="headerlink" title="4. Output执行发送"></a>4. Output执行发送</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DB::MySQLOutputFormat::consume(DB::Chunk) MySQLOutputFormat.cpp:53</span><br><span class="line">DB::IOutputFormat::work() IOutputFormat.cpp:62</span><br><span class="line">DB::executeJob(DB::IProcessor *) PipelineExecutor.cpp:155</span><br><span class="line">operator() PipelineExecutor.cpp:172</span><br><span class="line">DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic&lt;bool&gt;*) PipelineExecutor.cpp:630</span><br><span class="line">DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) PipelineExecutor.cpp:546</span><br><span class="line">DB::PipelineExecutor::executeImpl(unsigned long) PipelineExecutor.cpp:812</span><br><span class="line">DB::PipelineExecutor::execute(unsigned long) PipelineExecutor.cpp:479</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:800</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>ClickHouse 的模块化比较清晰，像乐高积木一样可以组合拼装，当我们执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM system.numbers LIMIT 5</span><br></pre></td></tr></table></figure><p>首先内核解析 SQL 语句生成 AST，然后根据 AST 获取数据源 Source，pipeline.Add(Source)。<br>其次根据 AST 信息生成 QueryPlan，根据 QueryPlan 再生成相应的 Transform，pipeline.Add(LimitTransform)。<br>然后添加 Output Sink 作为数据发送对象，pipeline.Add(OutputSink)。<br>执行 pipeline, 各个 Transformer 开始工作。</p><p>ClickHouse 的 Transformer 调度系统叫做 Processor，也是决定性能的重要模块，详情见 <a href="/2020/06/11/clickhouse-and-friends-processor/">Pipeline 处理器和调度器</a>。<br>ClickHouse 是一辆手动挡的豪华跑车，免费拥有，海啸们！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作为一个 OLAP 的 DBMS 来说，有2个端非常重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用户如何方便的链进来，这是入口端&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClickHouse 除了自己的 client 外，还提供了 MySQL/PG/GRPC/HTTP 等接入方式&lt;/li&gt;

      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mysql" scheme="https://bohutang.me/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（1）编译、开发、测试</title>
    <link href="https://bohutang.me/2020/06/05/clickhouse-and-friends-development/"/>
    <id>https://bohutang.me/2020/06/05/clickhouse-and-friends-development/</id>
    <published>2020-06-04T16:00:00.000Z</published>
    <updated>2020-08-10T07:17:16.084Z</updated>
    
    <content type="html"><![CDATA[<p>一次偶然的机会，和ClickHouse团队做了一次线下沟通，Alexey提到ClickHouse的设计哲学:</p><ol><li>The product must solve actual problem</li><li>And do it better than others</li></ol><p>用工程思维解决商业问题的典范啊！</p><p>对用户来说，他们关心的不是什么天花乱坠、上天入地的高科技，只是需要一个能很好解决自己问题的方案，这在开源社区是非常难得的，靠实力“野蛮式”生长。</p><p>于是，我对这个散发着伏特加味道的利器充满了好奇，并参与到ClickHouse的社区中一探究竟，第一感觉是开放、友好、战斗力强(AK47 vs CK16, ClickHouse 2016年开源)。</p><p>本文先从编译和测试入手，再到如何为社区贡献Patch，希望对那些想参与CK社区的同学有所帮助。</p><h2 id="如何本地编译和测试ClickHouse？"><a href="#如何本地编译和测试ClickHouse？" class="headerlink" title=" 如何本地编译和测试ClickHouse？"></a><b> 如何本地编译和测试ClickHouse？</b></h2><h3 id="源码获取"><a href="#源码获取" class="headerlink" title="源码获取"></a><b>源码获取</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;ClickHouse&#x2F;ClickHouse</span><br></pre></td></tr></table></figure><h3 id="编译准备"><a href="#编译准备" class="headerlink" title="编译准备"></a><b>编译准备</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install build-essential</span><br><span class="line">sudo apt-get install software-properties-common</span><br><span class="line">sudo apt-add-repository ppa:ubuntu-toolchain-r&#x2F;test</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install gcc-9 g++-9 git python ninja-build</span><br><span class="line">sudo snap install cmake</span><br></pre></td></tr></table></figure><h3 id="开始编译"><a href="#开始编译" class="headerlink" title="开始编译"></a><b>开始编译</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd ClickHouse</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">export CC&#x3D;gcc-9</span><br><span class="line">export CXX&#x3D;g++-9</span><br><span class="line">cmake ..</span><br><span class="line">ninja</span><br></pre></td></tr></table></figure><h3 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a><b>测试方法</b></h3><p>ClickHouse的测试在官方<a href="https://github.com/ClickHouse/ClickHouse/blob/master/docs/en/development/tests.md" target="_blank" rel="noopener">development/tests</a>文档里有详细的介绍，这里列举3个常用的测试模式：</p><h4 id="1-Functional-Tests"><a href="#1-Functional-Tests" class="headerlink" title="1. Functional Tests"></a>1. Functional Tests</h4><p>  功能测试，主要用于ClickHouse内部功能测试，方式：输入一个sql文件，输出一个result，类似MySQL里的mtr，<a href="https://github.com/ClickHouse/ClickHouse/tree/master/tests/queries" target="_blank" rel="noopener">测试集合</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd tests</span><br><span class="line">.&#x2F;clickhouse-test -c &quot;..&#x2F;build&#x2F;programs&#x2F;clickhouse-client&quot; 00001_select_1</span><br></pre></td></tr></table></figure><h4 id="2-Integration-Tests"><a href="#2-Integration-Tests" class="headerlink" title="2. Integration Tests"></a>2. Integration Tests</h4><p>  集成测试，主要用于涉及第三方服务的测试，比如MySQL/Postgres/MongoDB等，以容器化方式编排调度(pytest)运行，<a href="https://github.com/ClickHouse/ClickHouse/tree/master/tests/integration" target="_blank" rel="noopener">测试集合</a></p><p>  由于涉及模块较多，集成测试环境的搭建有一定的难度，建议使用官方的docker镜像。比如要跑test_mysql_protocol下的集成测试集：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd tests&#x2F;integration</span><br><span class="line">docker pull yandex&#x2F;clickhouse-integration-tests-runner</span><br><span class="line">.&#x2F;runner --binary &#x2F;your&#x2F;ClickHouse&#x2F;build&#x2F;programs&#x2F;clickhouse  --bridge-binary &#x2F;your&#x2F;ClickHouse&#x2F;build&#x2F;programs&#x2F;clickhouse-odbc-bridge --configs-dir &#x2F;your&#x2F;ClickHouse&#x2F;programs&#x2F;server&#x2F; &#39;test_mysql_protocol&#x2F;test.py::test_java_client -ss -vv&#39;</span><br></pre></td></tr></table></figure><h4 id="3-Unit-Tests"><a href="#3-Unit-Tests" class="headerlink" title="3. Unit Tests"></a>3. Unit Tests</h4><p>  单元测试，主要用于代码模块的测试，测试集在各个模块的tests目录，比如: <a href="https://github.com/ClickHouse/ClickHouse/tree/master/src/Core/tests" target="_blank" rel="noopener">Core/tests</a></p><p>  如果大家想了解某个模块是如何工作的，强烈建议去翻翻该模块的tests目录，比如想了解processor的工作机制，跟踪调试 <a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Processors/tests/processors_test.cpp" target="_blank" rel="noopener">Processors/tests/</a> 即可。</p><h2 id="如何给ClickHouse社区提Patch？"><a href="#如何给ClickHouse社区提Patch？" class="headerlink" title=" 如何给ClickHouse社区提Patch？"></a><b> 如何给ClickHouse社区提Patch？</b></h2><h4 id="1-fork"><a href="#1-fork" class="headerlink" title="1. fork"></a>1. fork</h4><p>  首先在自己的github上fork一份ClickHouse代码，比如 <a href="https://github.com/BohuTANG/ClickHouse" target="_blank" rel="noopener">https://github.com/BohuTANG/ClickHouse</a></p><h4 id="2-clone到本地"><a href="#2-clone到本地" class="headerlink" title="2. clone到本地"></a>2. clone到本地</h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;BohuTANG&#x2F;ClickHouse</span><br><span class="line">git checkout -B mysql_replica(branch名字)</span><br></pre></td></tr></table></figure><h4 id="3-创建新的分支"><a href="#3-创建新的分支" class="headerlink" title="3. 创建新的分支"></a>3. 创建新的分支</h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -B mysql_replica(branch名字)</span><br></pre></td></tr></table></figure><h4 id="4-功能开发"><a href="#4-功能开发" class="headerlink" title="4. 功能开发"></a>4. 功能开发</h4><p>  开发者可以提交一个Draft Pull Request到官方，github会显示这个Pull Request处于Draft状态，官方是无法Merge的</p><h4 id="5-can-be-testd标签"><a href="#5-can-be-testd标签" class="headerlink" title="5. can be testd标签"></a>5. can be testd标签</h4><p>  等待Upstream打[can be tested]标签，一旦被标记CI狂魔们就强势开跑，跑一轮大概需要几十个小时。<br>  协助开发者发现一些代码Style、编译以及测试等错误，这样开发者就可以在自己的分支不停的迭代、修正。<br>   <img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/github-ck-ci.png"></p><p> 如果只是修改typo，这个标签Upstream通常不会添加。</p><h4 id="6-开发完毕"><a href="#6-开发完毕" class="headerlink" title="6. 开发完毕"></a>6. 开发完毕</h4><p> 开发完成，测试OK，把Draft提升为正式Pull Request，等待Upstraem Review。</p><h4 id="7-Merge到Master"><a href="#7-Merge到Master" class="headerlink" title="7. Merge到Master"></a>7. Merge到Master</h4><p> 如果Upstream通过，你的代码会被Merge到Master，恭喜你成为ClickHouse贡献者</p><h4 id="8-注意事项"><a href="#8-注意事项" class="headerlink" title="8. 注意事项"></a>8. 注意事项</h4><p> ClickHouse Upstream迭代非常快，一定要多关注master分支进度，尽量保持自己的分支代码与master同步。否则Upstream Docker更新，自己的test可能就过不了。</p><p> 建议把<a href="https://github.com/ClickHouse/ClickHouse/tree/master/docs/en/development" target="_blank" rel="noopener">doc/development</a>读一遍。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一次偶然的机会，和ClickHouse团队做了一次线下沟通，Alexey提到ClickHouse的设计哲学:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The product must solve actual problem&lt;/li&gt;
&lt;li&gt;And do it better than 
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
  </entry>
  
</feed>

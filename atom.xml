<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>虎哥的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://bohutang.me/"/>
  <updated>2022-05-08T10:08:01.229Z</updated>
  <id>https://bohutang.me/</id>
  
  <author>
    <name>BohuTANG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用 ECDSA 替换 RSA 生成 SSH Key</title>
    <link href="https://bohutang.me/2022/05/08/ecdsa-rsa/"/>
    <id>https://bohutang.me/2022/05/08/ecdsa-rsa/</id>
    <published>2022-05-07T16:00:00.000Z</published>
    <updated>2022-05-08T10:08:01.229Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h2><p>相信很多同学还在使用 RSA 算法用于生成 SSH 公钥，可能还会纠结选择多少位才足够安全，一般建议是 4096 bits:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot;</span><br></pre></td></tr></table></figure><p>这样我们的公钥(public key) 就会非常长：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat test_rsa_4096.pub</span><br><span class="line"></span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCXlH8OxqMFhv2+En10yV2ZorDzRFXQm9pPuWQ8G5iu+cUpyhwDoKnd+l6PCZTrCgcVJgLSsVAVbZ3CK6Qnoj3TDQl4yaj90UasmivWM2INc2hObr5P2y2AqWnnZBXmxpoUGZPz&#x2F;9323JalC+m&#x2F;EwXNcdrC5JzgD083BC0ykfB801vcAzrZwsnbKfCUsGfUNP9mco3+hFwTqgfJxEvmI3X6hbGIGY1d2QbGMLrs3JYVsfRzJDjFaYOSwXZR6pM5uUCKENt9hOmVUZfuZqvlzLZX95yc53a6qNgOJhzaFZYz3wD2gY0dNp1boGnAtXsLqEnqtm9skp05iMuT01B9WrKEOZG5rsRZDh3bYXJ8ZP0lO&#x2F;RbStuBczd8ZgObb32NfUyHG2JObDpm9mjsvWZqJxJbT5l&#x2F;6vMXu8hQ6ikDrf6R33PRcRdbUIrAOpDUrfBxjkUonxjqqEbHhpcAlMWNJ4qcjtjvSnLOhH9GBn5KCnFJ7VIbyXc+Gj9AAp9xuV&#x2F;9jv1R7CathkS2QrC5s9pFY3I24mFevpkioEeJYPAYUTuFBenWg5MdFK99FYO44wjmFa&#x2F;RxwEQtYFXV+RybTJTC0eDpjK1u3w7LVm2JjEVoSfOJIKt9yZQn5Fm0kmueBz5aQ4CzZNoZBMKr7TT0dX9cJoANzd19uM4uCV6HRVJmQyz4Q&#x3D;&#x3D; your_email@example.com</span><br></pre></td></tr></table></figure><h2 id="Ed25519"><a href="#Ed25519" class="headerlink" title="Ed25519"></a>Ed25519</h2><p>其实，有一些更先进的算法，比 RSA 更安全，公钥更简短，随着区块链的普及，它们正慢慢被更多的人接受，比如 Ed25519，虽然它只有 256 bit，但安全性比 RSA 3072 还要高。</p><p>Ed25519 SSH Key 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t ed25519 -C &quot;your_email@example.com&quot;</span><br></pre></td></tr></table></figure><p>公钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat test_ed25519.pub </span><br><span class="line">ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGcMXqCXtcjny9gXV1NDmwArHy0AgJs+R7N6XpOutviw your_email@example.com</span><br></pre></td></tr></table></figure><p>Github 已经默认推荐大家使用 Ed25519: <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent" target="_blank" rel="noopener">https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent</a></p><p>Ed25519 是一个椭圆曲线，非常优美，安全性经过数学严格证明：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/ed25519.png" style="zoom:45%;">         By Deirdre Connolly in [State of the Curve] (2016)<p>这里有一份目前使用 Ed25519 的列表： <a href="https://ianix.com/pub/ed25519-deployment.html" target="_blank" rel="noopener">https://ianix.com/pub/ed25519-deployment.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;RSA&quot;&gt;&lt;a href=&quot;#RSA&quot; class=&quot;headerlink&quot; title=&quot;RSA&quot;&gt;&lt;/a&gt;RSA&lt;/h2&gt;&lt;p&gt;相信很多同学还在使用 RSA 算法用于生成 SSH 公钥，可能还会纠结选择多少位才足够安全，一般建议是 4096 bits:&lt;/p&gt;
      
    
    </summary>
    
    
    
      <category term="ecdsa" scheme="https://bohutang.me/tags/ecdsa/"/>
    
      <category term="rsa" scheme="https://bohutang.me/tags/rsa/"/>
    
      <category term="ed25519" scheme="https://bohutang.me/tags/ed25519/"/>
    
  </entry>
  
  <entry>
    <title>Rust, Databend and the Cloud Warehouse（5）从 Git 到 Fuse Engine 存储引擎</title>
    <link href="https://bohutang.me/2022/05/07/databend-cloud-warehouse-fuse-engine/"/>
    <id>https://bohutang.me/2022/05/07/databend-cloud-warehouse-fuse-engine/</id>
    <published>2022-05-06T16:00:00.000Z</published>
    <updated>2022-05-08T03:12:58.918Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Databend 是一个使用 Rust 研发、开源的、完全面向云架构的新式数仓，致力于提供极速的弹性扩展能力，打造按需、按量的 Data Cloud 产品体验。<br>开源地址：<a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">https://github.com/datafuselabs/databend</a></p></blockquote><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a><b>前言</b></h2><p>这篇来介绍下 Databend 底座： Fuse Engine，一个动力澎湃的列式存储引擎，Databend Fuse Engine 在设计之初社区给它的定位是：<b>动力要澎湃，架构要简单，可靠性要高。</b><br>在正式介绍之前，我们先看一组“挑战数据”，Databend Fuse Engine + AWS S3，一个事务在 ~1.5 小时写入了 22.89 TB 原始数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; INSERT INTO ontime_new SELECT * FROM ontime_new;</span><br><span class="line">Query OK, 0 rows affected (1 hour 34 min 36.82 sec)</span><br><span class="line">Read 31619274180 rows, 22.89 TB in 5675.207 sec., 5.57 million rows&#x2F;sec., 4.03 GB&#x2F;sec.</span><br></pre></td></tr></table></figure><p>同时，在功能上要满足：</p><ul><li>分布式事务：支持多个计算节点同时读、写同一份数据（存算分离架构首先要解决的问题）</li><li>快照隔离：不同版本数据之间互不影响，方便做 Table Zero-Copy</li><li>回溯能力：可切换到任意一个版本，方便做 Time Travel</li><li>数据合并：合并后生成新版本数据</li><li>简单、健壮：关系通过文件来描述，基于这些文件即可恢复出整个数据系统</li></ul><p>从这些需求出发，你会发现 Fuse Engine 跟 Git “形似”（Git-inspired），在介绍 Fuse Engine 设计之前，我们先来看看 Git 底层是如何工作的。</p><h2 id="Git-工作机制"><a href="#Git-工作机制" class="headerlink" title="Git 工作机制"></a><b>Git 工作机制</b></h2><p>Git 解决了分布式环境下的数据版本管理（data version control）问题，它有隔离（branch）、提交（commit）、回溯（checkout），以及合并（merge）功能，基于 Git 语义完全可以打造出一个分布式存储引擎。市面上也出现一些基于 Git-like 思想而构建的产品，比如 <a href="https://projectnessie.org/" target="_blank" rel="noopener">Nessie - Transactional Catalog for Data Lakes</a> 和 <a href="https://lakefs.io/" target="_blank" rel="noopener">lakeFS</a> 。</p><p>为了更好的探索 Git 底层工作机制，我们选择从数据库角度出发，使用 Git 语义来完成一系列“数据”操作。</p><ol><li>首先， 准备一个数据文件 <code>cloud.txt</code>，内容为:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2022&#x2F;05&#x2F;06, Databend, Cloud</span><br></pre></td></tr></table></figure><ol start="2"><li>把 <code>cloud.txt</code> 数据写到 Git 系统：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;Add olap.txt&quot;</span><br></pre></td></tr></table></figure><ol start="3"><li>Git 为我们生成一个快照，Commit ID 为 <code>7d972c7ba9213c2a2b15422d4f31a8cbc9815f71</code>：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git log </span><br><span class="line">commit 7d972c7ba9213c2a2b15422d4f31a8cbc9815f71 (HEAD)</span><br><span class="line">Author: BohuTANG &lt;overred.shuttler@gmail.com&gt;</span><br><span class="line">Date:   Fri May 6 16:44:21 2022 +0800</span><br><span class="line"></span><br><span class="line">    Add cloud.txt</span><br></pre></td></tr></table></figure><ol start="4"><li>再准备一个新文件 <code>warehouse.txt</code></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2022&#x2F;05&#x2F;07, Databend, Warehouse</span><br></pre></td></tr></table></figure><ol start="5"><li>把 <code>warehouse.txt</code> 数据写到 Git 系统</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m &quot;Add warehouse.txt&quot;</span><br></pre></td></tr></table></figure><ol start="6"><li>Git 为我们生成一个新的快照，Commit ID 为  <code>15af34e4d16082034e1faeaddd0332b3836f1424</code></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">commit 15af34e4d16082034e1faeaddd0332b3836f1424 (HEAD)</span><br><span class="line">Author: BohuTANG &lt;overred.shuttler@gmail.com&gt;</span><br><span class="line">Date:   Fri May 6 17:41:43 2022 +0800</span><br><span class="line"></span><br><span class="line">    Add warehouse.txt</span><br><span class="line"></span><br><span class="line">commit 7d972c7ba9213c2a2b15422d4f31a8cbc9815f71</span><br><span class="line">Author: BohuTANG &lt;overred.shuttler@gmail.com&gt;</span><br><span class="line">Date:   Fri May 6 16:44:21 2022 +0800</span><br><span class="line"></span><br><span class="line">    Add cloud.txt</span><br></pre></td></tr></table></figure><p>到此为止，Git 已经为我们维护了 2 个版本的数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ID 15af34e4d16082034e1faeaddd0332b3836f1424，版本2</span><br><span class="line">ID 7d972c7ba9213c2a2b15422d4f31a8cbc9815f71，版本1</span><br></pre></td></tr></table></figure><p>我们可以根据 Commit ID 进行版本间的任意切换，也就是实现了 Time Travel 和 Table Zero-Copy 功能，那么 Git 底层是如何做到的呢？ 方式也比较简单，它通过引入 3 类对象文件来进行关系描述：</p><ul><li>Commit 文件，用于描述 tree 对象信息</li><li>Tree 文件，用于描述 blob 对象信息</li><li>Blob 文件，用于描述文件信息</li></ul><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/fuse-engien-git.png" align="center" style="zoom:45%;" /><h3 id="HEAD-文件"><a href="#HEAD-文件" class="headerlink" title="HEAD 文件"></a><b>HEAD 文件</b></h3><p>首先，我们需要知道一个 HEAD  指针：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat .git&#x2F;HEAD</span><br><span class="line">15af34e4d16082034e1faeaddd0332b3836f1424</span><br></pre></td></tr></table></figure><h3 id="Commit-文件"><a href="#Commit-文件" class="headerlink" title="Commit 文件"></a><b>Commit 文件</b></h3><p>Commit 文件会记录跟 commit 相关的一些元数据信息，比如当前 tree 以及 parent，还有提交人等，文件路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.git&#x2F;objects&#x2F;15&#x2F;af34e4d16082034e1faeaddd0332b3836f1424</span><br></pre></td></tr></table></figure><p>文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git cat-file -p 15af34e4d16082034e1faeaddd0332b3836f1424</span><br><span class="line"></span><br><span class="line">tree 576c63e580846fa6df2337c1f074c8d840e0b70a</span><br><span class="line">parent 7d972c7ba9213c2a2b15422d4f31a8cbc9815f71</span><br><span class="line">author BohuTANG &lt;overred.shuttler@gmail.com&gt; 1651830103 +0800</span><br><span class="line">committer BohuTANG &lt;overred.shuttler@gmail.com&gt; 1651830103 +0800</span><br><span class="line"></span><br><span class="line">Add warehouse.txt</span><br></pre></td></tr></table></figure><h3 id="Tree-文件"><a href="#Tree-文件" class="headerlink" title="Tree 文件"></a><b>Tree 文件</b></h3><p>Tree 文件记录当前版本下所有的数据文件，文件路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.git&#x2F;objects&#x2F;57&#x2F;6c63e580846fa6df2337c1f074c8d840e0b70a</span><br></pre></td></tr></table></figure><p>文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git cat-file -p 576c63e580846fa6df2337c1f074c8d840e0b70a</span><br><span class="line"></span><br><span class="line">100644 blob 688de5069f9e873c7e7bd15aa67c6c33e0594ddecloud.txt</span><br><span class="line">100644 blob bdea812b9602ed3c6662a2231b3f1e7b52dc1ccbwarehouse.txt</span><br></pre></td></tr></table></figure><h3 id="Blob-文件"><a href="#Blob-文件" class="headerlink" title="Blob 文件"></a><b>Blob 文件</b></h3><p>Blob 文件是原始数据文件，同样可以通过 <code>git cat-file</code> 命令来查看文件内容（如果使用 Git 来管理代码，Blob 就是我们的代码文件）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git cat-file -p 688de5069f9e873c7e7bd15aa67c6c33e0594dde</span><br><span class="line">2022&#x2F;05&#x2F;06, Databend, Cloud</span><br><span class="line"></span><br><span class="line">git cat-file -p bdea812b9602ed3c6662a2231b3f1e7b52dc1ccb</span><br><span class="line">2022&#x2F;05&#x2F;07, Databend, Warehouse</span><br></pre></td></tr></table></figure><h2 id="Fuse-Engine"><a href="#Fuse-Engine" class="headerlink" title="Fuse Engine"></a><b>Fuse Engine</b></h2><p>Databend Fuse Engine 在设计上，跟 Git 非常类似，它引入 3 个描述文件：</p><ul><li>Snapshot 文件，用于描述 Segment 对象信息</li><li>Segment 文件，用于描述 Block 对象信息</li><li>Block 文件，用于描述 Parquet 文件信息</li></ul><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/fuse-engine-fuse.png" align="center" style="zoom:45%;" /><p>我们继续在 Fuse Engine 里进行一把刚才在 Git 进行的操作。</p><ol><li>首先<a href="https://databend.rs/doc/reference/sql/ddl/table/ddl-create-table" target="_blank" rel="noopener">创建一个表</a>:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE git(file VARCHAR, content VARCHAR);</span><br></pre></td></tr></table></figure><ol start="2"><li>把 <code>cloud.txt</code> 数据写到 Fuse Engine</li></ol>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO git VALUES(&#39;cloud.txt&#39;, &#39;2022&#x2F;05&#x2F;06, Databend, Cloud&#39;);</span><br></pre></td></tr></table></figure><ol start="3"><li><p>Fuse 为我们生成一个新的 Snapshot ID <code>6450690b09c449939a83268c49c12bb2</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CALL system$fuse_snapshot(&#39;default&#39;, &#39;git&#39;);</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">         snapshot_id: 6450690b09c449939a83268c49c12bb2</span><br><span class="line">   snapshot_location: 53&#x2F;133&#x2F;_ss&#x2F;6450690b09c449939a83268c49c12bb2_v1.json</span><br><span class="line">      format_version: 1</span><br><span class="line">previous_snapshot_id: NULL</span><br><span class="line">       segment_count: 1</span><br><span class="line">         block_count: 1</span><br><span class="line">           row_count: 1</span><br><span class="line">  bytes_uncompressed: 68</span><br><span class="line">    bytes_compressed: 351</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>把 <code>warehouse.txt</code> 数据写到 Fuse Engine</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO git VALUES(&#39;warehouse.txt&#39;, &#39;2022&#x2F;05&#x2F;07, Databend, Warehouse&#39;);</span><br></pre></td></tr></table></figure></li><li><p>Fuse Engine 为我们生成一个新的 Snapshot ID <code>efe2687fd1fc48f8b414b5df2cec1e19</code>，并指向前一个 Snapshot ID <code>6450690b09c449939a83268c49c12bb2</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">CALL system$fuse_snapshot(&#39;default&#39;, &#39;git&#39;);</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">         snapshot_id: efe2687fd1fc48f8b414b5df2cec1e19</span><br><span class="line">   snapshot_location: 53&#x2F;133&#x2F;_ss&#x2F;efe2687fd1fc48f8b414b5df2cec1e19_v1.json</span><br><span class="line">      format_version: 1</span><br><span class="line">previous_snapshot_id: 6450690b09c449939a83268c49c12bb2</span><br><span class="line">       segment_count: 2</span><br><span class="line">         block_count: 2</span><br><span class="line">           row_count: 2</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">         snapshot_id: 6450690b09c449939a83268c49c12bb2</span><br><span class="line">   snapshot_location: 53&#x2F;133&#x2F;_ss&#x2F;6450690b09c449939a83268c49c12bb2_v1.json</span><br><span class="line">      format_version: 1</span><br><span class="line">previous_snapshot_id: NULL</span><br><span class="line">       segment_count: 1</span><br><span class="line">         block_count: 1</span><br><span class="line">           row_count: 1</span><br></pre></td></tr></table></figure><p>目前为止，Fuse Engine 为我们生成了 2 个版本的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ID efe2687fd1fc48f8b414b5df2cec1e19，版本2</span><br><span class="line">ID 6450690b09c449939a83268c49c12bb2，版本1</span><br></pre></td></tr></table></figure><p>是不是跟 Git 非常类似？</p></li></ol><h3 id="HEAD"><a href="#HEAD" class="headerlink" title="HEAD"></a><b>HEAD</b></h3><p>跟 Git 一样，Fuse Engine 也需要一个 HEAD 作为入口，查看 Fuse Engine 的 HEAD：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SHOW CREATE TABLE git\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">       Table: git</span><br><span class="line">Create Table: CREATE TABLE &#96;git&#96; (</span><br><span class="line">  &#96;file&#96; VARCHAR,</span><br><span class="line">  &#96;content&#96; VARCHAR</span><br><span class="line">) ENGINE&#x3D;FUSE SNAPSHOT_LOCATION&#x3D;&#39;53&#x2F;133&#x2F;_ss&#x2F;efe2687fd1fc48f8b414b5df2cec1e19_v1.json&#39;</span><br></pre></td></tr></table></figure><p><code>SNAPSHOT_LOCATION</code> 就是 HEAD，默认指向最新的快照 <code>efe2687fd1fc48f8b414b5df2cec1e19</code>，那我们如何切到 ID 为 <code>6450690b09c449939a83268c49c12bb2</code> 的快照数据呢？ 很简单，先查看当前表的 Snapshot 信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">CALL system$fuse_snapshot(&#39;default&#39;, &#39;git&#39;)\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">         snapshot_id: efe2687fd1fc48f8b414b5df2cec1e19</span><br><span class="line">   snapshot_location: 53&#x2F;133&#x2F;_ss&#x2F;efe2687fd1fc48f8b414b5df2cec1e19_v1.json</span><br><span class="line">      format_version: 1</span><br><span class="line">previous_snapshot_id: 6450690b09c449939a83268c49c12bb2</span><br><span class="line">       segment_count: 2</span><br><span class="line">         block_count: 2</span><br><span class="line">           row_count: 2</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">         snapshot_id: 6450690b09c449939a83268c49c12bb2</span><br><span class="line">   snapshot_location: 53&#x2F;133&#x2F;_ss&#x2F;6450690b09c449939a83268c49c12bb2_v1.json</span><br><span class="line">      format_version: 1</span><br><span class="line">previous_snapshot_id: NULL</span><br><span class="line">       segment_count: 1</span><br><span class="line">         block_count: 1</span><br><span class="line">           row_count: 1</span><br></pre></td></tr></table></figure><p>然后创建一个新表（git_v1）并把 <code>SNAPSHOT_LOCATION</code> 指向相应的 Snapshot 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE git_v1(&#96;file&#96; VARCHAR, &#96;content&#96; VARCHAR) SNAPSHOT_LOCATION&#x3D;&#39;53&#x2F;133&#x2F;_ss&#x2F;6450690b09c449939a83268c49c12bb2_v1.json&#39;;</span><br><span class="line"></span><br><span class="line">SELECT * FROM git_v1;</span><br><span class="line">+-----------+-----------------------------+</span><br><span class="line">| file      | content                     |</span><br><span class="line">+-----------+-----------------------------+</span><br><span class="line">| cloud.txt | 2022&#x2F;05&#x2F;06, Databend, Cloud |</span><br><span class="line">+-----------+-----------------------------+</span><br></pre></td></tr></table></figure><h3 id="Snapshot-文件"><a href="#Snapshot-文件" class="headerlink" title="Snapshot 文件"></a><b>Snapshot 文件</b></h3><p>用于存储 Segment 信息，文件路径 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">53&#x2F;133&#x2F;_ss&#x2F;efe2687fd1fc48f8b414b5df2cec1e19_v1.json</span><br></pre></td></tr></table></figure><p>文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;format_version&quot;:1,</span><br><span class="line">   &quot;snapshot_id&quot;:&quot;efe2687f-d1fc-48f8-b414-b5df2cec1e19&quot;,</span><br><span class="line">   &quot;prev_snapshot_id&quot;:[</span><br><span class="line">      &quot;6450690b-09c4-4993-9a83-268c49c12bb2&quot;,</span><br><span class="line">      1</span><br><span class="line">   ],</span><br><span class="line">   </span><br><span class="line">   &quot;segments&quot;:[</span><br><span class="line">      [</span><br><span class="line">         &quot;53&#x2F;133&#x2F;_sg&#x2F;df56e911eb26446b9f8fac5acc65a580_v1.json&quot;</span><br><span class="line">      ],</span><br><span class="line">      [</span><br><span class="line">         &quot;53&#x2F;133&#x2F;_sg&#x2F;d0bff902b98846469480b23c2a8f93d7_v1.json&quot;</span><br><span class="line">      ]</span><br><span class="line">   ]</span><br><span class="line">   ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Segment-文件"><a href="#Segment-文件" class="headerlink" title="Segment 文件"></a><b>Segment 文件</b></h3><p>用于存储 Block 相关信息，文件路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">53&#x2F;133&#x2F;_sg&#x2F;df56e911eb26446b9f8fac5acc65a580_v1.json</span><br></pre></td></tr></table></figure><p>文件内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   &quot;format_version&quot;:1,</span><br><span class="line">   &quot;blocks&quot;:[</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;row_count&quot;:1,</span><br><span class="line">         &quot;block_size&quot;:76,</span><br><span class="line">         &quot;file_size&quot;:360,</span><br><span class="line">         &quot;location&quot;:[</span><br><span class="line">            &quot;53&#x2F;133&#x2F;_b&#x2F;ba4a60013e27479e856f739aefeadfaf_v0.parquet&quot;,</span><br><span class="line">            0</span><br><span class="line">         ],</span><br><span class="line">         &quot;compression&quot;:&quot;Lz4Raw&quot;</span><br><span class="line">      &#125;</span><br><span class="line">   ]</span><br><span class="line">   ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Block-文件"><a href="#Block-文件" class="headerlink" title="Block 文件"></a><b>Block 文件</b></h3><p>Fuse Engine 底层数据使用 Parquet 格式，每个文件内部有多个 Block 组成。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a><b>小结</b></h2><p>Databend Fuse Engine 在早期设计（2021 年 10 月）时候，需求很明确，但方案选型还是经历过一段小曲折。当时，Databend 社区调研了市面上大量的 Table Format 方案（比如 Iceberg 等），当时面临的挑战是基于现有方案还是自己造一套？最终选择研发一套简洁的、适合自己的 Storage Engine，但数据存储格式选择 Parquet 标准。<br>在 Fuse Engine 里，我们把 Parquet Footer 单独存放，以减少不必要的 Seek 操作，另外增加了一套更加灵活的索引机制，比如 Aggregation，Join 等都可以有自己的索引来进行加速。</p><p>欢迎体验 Fuse Engine，挂上对象存储，让你体验不一样的大数据分析 <a href="https://databend.rs/doc/deploy" target="_blank" rel="noopener">https://databend.rs/doc/deploy</a><br>Databend 开源地址：<a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">https://github.com/datafuselabs/databend</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Databend 是一个使用 Rust 研发、开源的、完全面向云架构的新式数仓，致力于提供极速的弹性扩展能力，打造按需、按量的 Data Cloud 产品体验。&lt;br&gt;开源地址：&lt;a href=&quot;https://github.com/datafus
      
    
    </summary>
    
    
    
      <category term="cloud warehouse" scheme="https://bohutang.me/tags/cloud-warehouse/"/>
    
      <category term="databend" scheme="https://bohutang.me/tags/databend/"/>
    
      <category term="git" scheme="https://bohutang.me/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>Rust, Databend and the Cloud Warehouse（4）Databend 社区如何做测试</title>
    <link href="https://bohutang.me/2021/09/14/databend-cloud-warehouse-how-to-test/"/>
    <id>https://bohutang.me/2021/09/14/databend-cloud-warehouse-how-to-test/</id>
    <published>2021-09-14T07:41:00.000Z</published>
    <updated>2021-09-15T02:37:36.199Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Databend 是一个使用 Rust 研发、开源的、完全面向云架构的新式数仓，致力于提供极速的弹性扩展能力，打造按需、按量的 Data Cloud 产品体验。<br>开源地址：<a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">https://github.com/datafuselabs/databend</a></p></blockquote><p>Databend 从第一天就是开源的，测试系统也是基于开源生态所构建，使用了大量的 github CI（免费），已经支撑了我们半年来的快速迭代。</p><p>对于一个开源数据库项目，做到可测试性是加速迭代的不二法宝。一个 Pull Request (通常说的 Patch) 从提交到合并到主干分支，作为一个 Review 人员会比较关注以下几个问题:</p><ol><li>是否会导致功能不正常？</li><li>是否会影响分布式执行？</li><li>是否有跨平台编译问题？</li><li>是否会导致性能下降？</li></ol><p>本篇就从一个 Pull Request 测试周期说起，看看它从创建再到合并入主干分支，Databend 经过了哪些测试，针对上面四个问题做到心中有数，让每个 Pull Request 都有质量保障。</p><h3 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a><strong>单元测试</strong></h3><p>单元测试是最小的测试单元。</p><p>我们每写一个函数都要做到可独立测试，如果这个函数有其他状态依赖，那状态也要是可以 Mock 的。</p><p>在 Databend 中，单元测试都放在一个独立的文件中，比如，x_test.rs:</p><figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#[test]</span></span><br><span class="line"><span class="function"><span class="keyword">fn</span> <span class="title">test_y</span></span>() -&gt; <span class="built_in">Result</span>&lt;()&gt; &#123;</span><br><span class="line">   ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>目前 Databend 有 500+ 的单元测试，并对一些状态做了全局 Mock，让开发者更加容易的编写测试用例，以从代码层面确保函数执行都符合预期，尽早的发现和解决问题。</p><p>Databend 的单元测试会在 Ubuntu 和 MacOS 两个系统上运行 （Databend 研发主要使用 Mac 和 Ubuntu 两个主力系统）。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/test-unit.png" style="zoom:50%;"/><h3 id="功能测试"><a href="#功能测试" class="headerlink" title="功能测试"></a><strong>功能测试</strong></h3><p>当单元测试通过后，并不一定保证功能是正确的，因为功能通常来说是由多个函数逻辑性的贯穿而成。</p><p>功能测试又分为 Stateless 和 Stateful 两种模型，其中 Stateless 测试模型不需要加载数据集， Stateful 测试模型则需要加载预值的数据集，接下来我们着重看下 Stateless 测试模型。</p><p>Databend 参考了 ClickHouse 的做法，使用表函数 <code>numbers_mt</code> 来便捷的做 Stateless 测试。</p><p>比如这个稍微“复杂”的SQL：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT number%3 as c1, number%2 as c2 FROM numbers_mt(10000) WHERE number &gt; 2 GROUP BY number%3, number%2 ORDER BY c1, c2;</span><br></pre></td></tr></table></figure><p>它先根据条件过滤数据，然后再进行 GROUP BY 分组，最后做一个排序，这个 SQL 执行时，会涉及非常多的函数，所以我们必须有一套便捷的机制来保证多个函数组成的功能也是正确的。</p><p>Databend 是如何做的呢？</p><p>我们会先定义一个需要测试的 SQL 集，x.sql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">number</span>%<span class="number">3</span> <span class="keyword">as</span> c1, <span class="built_in">number</span>%<span class="number">2</span> <span class="keyword">as</span> c2 <span class="keyword">FROM</span> numbers_mt(<span class="number">10000</span>) <span class="keyword">WHERE</span> <span class="built_in">number</span> &gt; <span class="number">2</span> <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="built_in">number</span>%<span class="number">3</span>, <span class="built_in">number</span>%<span class="number">2</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> c1, c2;</span><br></pre></td></tr></table></figure><p>然后再定义一个预期的结果集，x.result：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0  0</span><br><span class="line">0  1</span><br><span class="line">1  0</span><br><span class="line">1  1</span><br><span class="line">2  0</span><br><span class="line">2  1</span><br></pre></td></tr></table></figure><p>每次做功能测试的时候，Databend 会调用这个 x.sql 文件，然后把得到的结果集和 x.result 文件进行对比，如果有出入则报错并给出提示信息。</p><p>由于 Databend 具备分布式的 MPP 能力，所以功能测试会在单机（Standalone）和 集群（Cluster）两种模式下进行回归测试，以确保 Patch 对功能没有影响。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/test-stateless.png" style="zoom:50%;"/><h3 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a><strong>性能测试</strong></h3><p>当单元测试和功能测试都通过后，我们还会关注一个重要的指标：这个 Patch 是否导致性能下降？或者是一个性能优化的 Patch 提升了多少性能？</p><p>针对这个问题，Databend 使用数字来做量化，我们只需在 Pull Request 里回复: <code>/run-perf master</code> CI 会自动编译当前分支然后跑相应的性能测试，再跟 master 做对比并生成一份性能对比报告：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/test-perf-report.png" style="zoom:50%;"/><p>这样，Review 人员就可以根据这个报告清晰的知道当前 Patch 对性能的影响，以确保每个 Patch 在性能上都是可控的。</p><h3 id="编译测试"><a href="#编译测试" class="headerlink" title="编译测试"></a><strong>编译测试</strong></h3><p>Databend 目标是打造一个跨平台的 Cloud Warehouse，所以要求每个 Patch 在以下几个平台都可以正常编译和工作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- &#123;os: ubuntu-latest, toolchain: stable, target: x86_64-unknown-linux-gnu, cross: false&#125;</span><br><span class="line">- &#123;os: ubuntu-latest, toolchain: stable, target: aarch64-unknown-linux-gnu, cross: true&#125;</span><br><span class="line">- &#123;os: ubuntu-latest, toolchain: stable, target: arm-unknown-linux-gnueabi, cross: true&#125;</span><br><span class="line">- &#123;os: ubuntu-latest, toolchain: stable, target: armv7-unknown-linux-gnueabihf, cross: true&#125;</span><br><span class="line">- &#123;os: macos-latest, toolchain: stable, target: x86_64-apple-darwin, cross: false&#125;</span><br></pre></td></tr></table></figure><p>当这个 CI 跑完后，我们可以明确的知道当前 Patch 对跨版本编译是没有影响的。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a><strong>小结</strong></h3><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/test-all-passed.png" style="zoom:50%;"/><p>以上所有的 CI 测试都通过后，我们的 Pull Request 才算合格，具备合并到主干分支的条件。</p><p>如果没有这些自动化测试 CI 做保障，每个问题都会消耗 Review 人员大量的精力去做验证，这种模式肯定不可持久，严重影响产品的迭代速度，拖慢社区的节奏。</p><p>Databend 从第一天开始就在努力打造一个可测试的系统，为此我们研发了 test-infra 以及社区协作的 fusebot 机器人，以加速 Databend 产品迭代，尽快提供一个可试用的 Alpha 版本。</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h3><ol><li><a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">Databend: A Modern Real-Time Data Processing &amp; Analytics DBMS with Cloud-Native Architecture</a></li><li><a href="https://github.com/datafuselabs/databend/tree/master/.github/workflows" target="_blank" rel="noopener">Databend Github Workflows</a></li><li><a href="https://github.com/datafuselabs/test-infra" target="_blank" rel="noopener">Databend Test Infra</a></li><li><a href="https://github.com/datafuselabs/fusebots" target="_blank" rel="noopener">Databend FuseBots</a></li><li><a href="https://clickhouse.tech/docs/en/development/tests/" target="_blank" rel="noopener">ClickHouse Testing</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Databend 是一个使用 Rust 研发、开源的、完全面向云架构的新式数仓，致力于提供极速的弹性扩展能力，打造按需、按量的 Data Cloud 产品体验。&lt;br&gt;开源地址：&lt;a href=&quot;https://github.com/datafus
      
    
    </summary>
    
    
    
      <category term="cloud warehouse" scheme="https://bohutang.me/tags/cloud-warehouse/"/>
    
      <category term="databend" scheme="https://bohutang.me/tags/databend/"/>
    
  </entry>
  
  <entry>
    <title>Rust, Databend and the Cloud Warehouse（3）Datafuse 更名为 Databend</title>
    <link href="https://bohutang.me/2021/09/14/datafuse-cloud-warehouse-rename-databend/"/>
    <id>https://bohutang.me/2021/09/14/datafuse-cloud-warehouse-rename-databend/</id>
    <published>2021-09-13T16:00:00.000Z</published>
    <updated>2021-09-15T02:37:32.167Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Databend 是一个使用 Rust 研发、开源的、完全面向云架构的新式数仓，致力于提供极速的弹性扩展能力，打造按需、按量的 Data Cloud 产品体验。<br>开源地址：<a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">https://github.com/datafuselabs/databend</a></p></blockquote><p>此篇不算一个技术文章，但是比较重要，因为我们项目改名了，做个说明。</p><h3 id="项目更名原因"><a href="#项目更名原因" class="headerlink" title="项目更名原因"></a><strong>项目更名原因</strong></h3><p>Datafuse 和开源项目 DataFusion 名称在英语语境里比较接近，为了大家未来的发展，我们决定把项目更名为：Databend 。</p><p>由于文化差异导致的一些误解，大家聊开后也就化解了，感谢 Andy 的理解和支持，相信 DataFusion 和  Databend 都有着美好的前景。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/databend-andy-comment.png" style="zoom:50%;"/><h3 id="Databend-寓意"><a href="#Databend-寓意" class="headerlink" title="Databend 寓意"></a><strong>Databend 寓意</strong></h3><p>相对论是 20 世纪最伟大的发现之一，由于物质的存在，时间和空间会发生弯曲（space-time bend），它让人类重新审视时间与空间。我们期望 Databend 的出现，让更多人重新审视数据，发掘出更大的价值。</p><h3 id="关于-「Datafuse-Labs」"><a href="#关于-「Datafuse-Labs」" class="headerlink" title="关于 「Datafuse Labs」"></a><strong>关于 「Datafuse Labs」</strong></h3><p>「Datafuse Labs」成立于 2021 年 3 月，是开源项目 Databend 的背后团队，团队在云原生数据库领域有着丰富的工程经验，同时也是数据库开源社区活跃贡献者，目前在中国、美国、新加坡均设有研发中心，致力于成为业界领先的 Data Cloud 基础软件供应商。</p><h3 id="加入我们"><a href="#加入我们" class="headerlink" title="加入我们"></a><strong>加入我们</strong></h3><ul><li><p>数据库工程师：负责 Databend 内核设计与研发，有丰富的分布式系统经验，熟悉 Rust 或 C++ ，对数据库有着浓厚的兴趣，可 Worldwide Remote 办公。</p></li><li><p>云平台工程师：负责 Databend Cloud 产品设计与研发，有 Kubernetes 开发经验者优先，可 Worldwide Remote 办公。<br>如果对我们感兴趣，简历请投递：<a href="mailto:hr@datafuselabs.com">hr@datafuselabs.com</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Databend 是一个使用 Rust 研发、开源的、完全面向云架构的新式数仓，致力于提供极速的弹性扩展能力，打造按需、按量的 Data Cloud 产品体验。&lt;br&gt;开源地址：&lt;a href=&quot;https://github.com/datafus
      
    
    </summary>
    
    
    
      <category term="cloud warehouse" scheme="https://bohutang.me/tags/cloud-warehouse/"/>
    
      <category term="databend" scheme="https://bohutang.me/tags/databend/"/>
    
  </entry>
  
  <entry>
    <title>Rust, Databend and the Cloud Warehouse（2）Databend 架构概览</title>
    <link href="https://bohutang.me/2021/08/21/datafuse-cloud-warehouse-design/"/>
    <id>https://bohutang.me/2021/08/21/datafuse-cloud-warehouse-design/</id>
    <published>2021-08-20T16:00:00.000Z</published>
    <updated>2021-09-14T07:23:10.519Z</updated>
    
    <content type="html"><![CDATA[<p>Databend 是一个开源的、完全面向云架构的新式数仓，它提供快速的弹性扩展能力，并结合云的弹性、简单性和低成本，使 Data Cloud 构建变得更加容易。Databend 把数据存储在像 AWS S3 ，Azure Blob 这些云上的存储系统，可以使不同的计算节点挂载同一份数据，从而做到较高的弹性，实现对资源的精细化控制。</p><p>Databend 在设计上专注以下能力：</p><ol><li><strong>弹性</strong> 在 Databend 中，存储和计算资源可以按需、按量弹性扩展。</li><li><strong>安全</strong> Databend 中数据文件和网络传输都是端到端加密，并在 SQL 级别提供基于角色的权限控制。</li><li><strong>易用</strong> Databend 兼容 ANSI SQL，并可以使用 MySQL 和 ClickHouse 客户端接入，几乎无学习成本。</li><li><strong>成本</strong> Databend 处理查询非常高效，用户只需要为使用的资源付费。</li></ol><img src="https://datafuse-1253727613.cos.ap-hongkong.myqcloud.com/arch/datafuse-arch-20210817.svg" align="center" /><p>上图是 Databend 的整体架构图，整个系统主要由三大部分组成：<code>Meta service Layer</code>、<code>Compute Layer</code> 和 <code>Storage Layer</code>。</p><h2 id="Meta-Service-Layer"><a href="#Meta-Service-Layer" class="headerlink" title="Meta Service Layer"></a><strong>Meta Service Layer</strong></h2><p>Meta Service 是一个多租户、高可用的分布式 key-value 存储服务，具备事务能力，主要用于存储：</p><ul><li><code>Metadata</code> : 表的元信息、索引信息、集群信息、事务信息等。</li><li><code>Administration</code>：用户系统、用户权限等信息。</li><li><code>Security</code> ：用户登录认证、数据加密等。</li></ul><h2 id="Compute-Layer"><a href="#Compute-Layer" class="headerlink" title="Compute Layer"></a><strong>Compute Layer</strong></h2><p>计算层由多个集群（cluster）组成，不同集群可以承担不同的工作负载，每个集群又有多个计算节点（node）组成，你可以轻松的添加、删除节点或集群，做到资源的按需、按量管理。</p><p>计算节点是计算层的最小构成单元，其中每个计算节点包含以下几个组件：</p><ul><li><strong>执行计划 （Planner）</strong></li></ul><p>根据用户输入的 SQL 生成执行计划，它只是个逻辑表达，并不能真正的执行，而是用于指导整个计算流水线（Pipeline）的编排与生成。<br>比如语句 <code>SELECT number + 1 FROM numbers_mt(10) WHERE number &gt; 8 LIMIT 2</code><br>执行计划：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">datafuse :) EXPLAIN SELECT number + 1 FROM numbers_mt(10) WHERE number &gt; 8 LIMIT 2</span><br><span class="line">┌─explain─────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Limit: 2                                                                                                                │</span><br><span class="line">│   Projection: (number + 1):UInt64                                                                                       │</span><br><span class="line">│     Expression: (number + 1):UInt64 (Before Projection)                                                                 │</span><br><span class="line">│       Filter: (number &gt; 8)                                                                                              │</span><br><span class="line">│         ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] │</span><br><span class="line">└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure><p>这个执行计划自下而上分别是 ：</p><p><code>ReadDataSource</code>：表示从哪些文件里读取数据<br><code>Filter</code>: 表示要做 (number &gt; 8) 表达式过滤<br><code>Expression</code>: 表示要做 (number + 1) 表达式运算<br><code>Projection</code>: 表示查询列是哪些<br><code>Limit</code>: 表示取前 2 条数据</p><ul><li><strong>优化器 （Optimizer）</strong></li></ul><p>对执行计划做一些基于规则的优化（A Rule Based Optimizer）, 比如做一些谓词下推或是去掉一些不必要的列等，以使整个执行计划更优。</p><ul><li><strong>处理器 （Processors）</strong></li></ul><p>处理器（Processor）是执行计算逻辑的核心组件。根据执行计划，处理器们被编排成一个流水线（Pipeline），用于执行计算任务。</p><p>整个 Pipeline 是一个有向无环图，每个点是一个处理器，每条边由处理器的 InPort 和 OutPort 相连构成，数据到达不同的处理器进行计算后，通过边流向下一个处理器，多个处理器可以并行计算，在集群模式下还可以跨节点分布式执行，这是 Databend 高性能的一个重要设计。</p><p>例如，我们可以通过 EXPLAIN PIPELINE 来查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">datafuse :) EXPLAIN PIPELINE SELECT number + 1 FROM numbers_mt(10000) WHERE number &gt; 8 LIMIT 2</span><br><span class="line">┌─explain───────────────────────────────────────────────────────────────┐</span><br><span class="line">│ LimitTransform × 1 processor                                          │</span><br><span class="line">│   Merge (ProjectionTransform × 16 processors) to (LimitTransform × 1) │</span><br><span class="line">│     ProjectionTransform × 16 processors                               │</span><br><span class="line">│       ExpressionTransform × 16 processors                             │</span><br><span class="line">│         FilterTransform × 16 processors                               │</span><br><span class="line">│           SourceTransform × 16 processors                             │</span><br><span class="line">└───────────────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure><p>同样，理解这个 Pipeline 我们自下而上来看：<br><code>SourceTransform</code>：读取数据文件，16 个物理 CPU 并行处理<br><code>FilterTransform</code>：对数据进行 (number &gt; 8) 表达式过滤，16 个物理 CPU 并行处理<br><code>ExpressionTransform</code>：对数据进行 (number + 1) 表达式执行，16 个物理 CPU 并行处理<br><code>ProjectionTransform</code>：对数据处理生成最终列<br><code>LimitTransform</code>：对数据进行 Limit 2 处理，Pipeline 进行折叠，由一个物理 CPU 来执行</p><p>Databend 通过 Pipeline 并行模型，并结合向量计算最大限度的去压榨 CPU 资源，以加速计算。</p><ul><li><strong>缓存 (Cache)</strong></li></ul><p>计算节点使用本地 SSD 缓存数据和索引，以提高数据亲和性来加速计算。<br>缓存的预热方式有：</p><ul><li>LOAD_ON_DEMAND - 按需加载索引或数据块（默认）。</li><li>LOAD_INDEX - 只加载索引。</li><li>LOAD_ALL - 加载全部的数据和索引，对于较小的表可以采取这种模式。</li></ul><h2 id="Storage-Layer"><a href="#Storage-Layer" class="headerlink" title="Storage Layer"></a><strong>Storage Layer</strong></h2><p>Databend 使用 Parquet 列式存储格式来储存数据，为了加快查找（Partition Pruning），Datafuse 为每个 Parquet 提供了自己的索引（根据 Primary Key 生成）：</p><ul><li>min_max.idx Parquet 文件 minimum 和 maximum 值</li><li>sparse.idx 以 N 条记录为颗粒度的稀疏索引</li></ul><p>通过这些索引， 我们可以减少数据的交互，并使计算量大大减少。<br>假设有两个Parquet 文件：<code>f1</code>, <code>f2</code>。<br><code>f1</code> 的 <code>min_max.idx: [3, 5]</code> ；<code>f2</code> 的 <code>min_max.idx: [4, 6]</code> 。如果查询条件为：<code>where x &lt; 4</code> ， 我们只需要 <code>f1</code> 文件就可以，再根据 <code>sparse.idx</code> 索引定位到 <code>f1</code> 文件中的某个数据页。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>Databend 是一个完全面向云架构而设计的新式数仓，它把传统数据库进行解耦，再根据需求组装出一个 Cloud Warehouse，以追求高弹性、低成本为宗旨。</p><p>Databend 目前的聚合函数已经<a href="https://datafuse.rs/overview/performance/" target="_blank" rel="noopener">非常高效</a>，基于这些高效的聚合函数（尤其是 Group By），我们实现了模型函数 <a href="https://datafuse.rs/sqlstatement/aggregate-functions/aggregate-windowfunnel/" target="_blank" rel="noopener">windowFunnel</a>，用于漏洞模型的高效计算。</p><p>Databend 正处于高速迭代期，欢迎关注我们: <a href="https://github.com/datafuselabs/databend/" target="_blank" rel="noopener">https://github.com/datafuselabs/databend/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Databend 是一个开源的、完全面向云架构的新式数仓，它提供快速的弹性扩展能力，并结合云的弹性、简单性和低成本，使 Data Cloud 构建变得更加容易。Databend 把数据存储在像 AWS S3 ，Azure Blob 这些云上的存储系统，可以使不同的计算节点挂
      
    
    </summary>
    
    
    
      <category term="cloud warehouse" scheme="https://bohutang.me/tags/cloud-warehouse/"/>
    
      <category term="databend" scheme="https://bohutang.me/tags/databend/"/>
    
  </entry>
  
  <entry>
    <title>Rust, Datafuse and the Cloud Warehouse（1）云时代数仓架构设计</title>
    <link href="https://bohutang.me/2021/08/08/datafuse-cloud-warehouse-arch/"/>
    <id>https://bohutang.me/2021/08/08/datafuse-cloud-warehouse-arch/</id>
    <published>2021-08-07T16:00:00.000Z</published>
    <updated>2022-05-06T00:55:10.481Z</updated>
    
    <content type="html"><![CDATA[<p>传统数仓架构不适合云？</p><p>Cloud Warehouse 解决了什么问题？</p><p>Cloud Warehouse 架构应该是什么样？</p><p>带着问题，通过实战，向 Cloud Warehouse 出发。</p><h2 id="Sharding-Warehouse"><a href="#Sharding-Warehouse" class="headerlink" title="Sharding Warehouse"></a><b>Sharding Warehouse</b></h2><p>首先，来看看传统式 Sharding Warehouse 架构，以及它在云上的局限性。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/sharding-warehouse.png" align="center" style="zoom:35%;" /><p>每个 shard 数据区间是固定的，很容易发生数据热点(data skew)问题，一般解决办法：</p><p> ① 提升该 shard 硬件配置，如果热点很难预估，整个集群配置都需要提升，资源上粒度控制粗暴。</p><p> ② 扩容，扩容过程(增加 shard-4)涉及数据迁移，如果数据量大，shard-4 可服务等待时间也会加长。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/sharding-warehouse-scale.png" align="center" style="zoom:35%;" /><p>如果只是把 Sharding Warehouse 简单的搬到云上，资源控制粒度还是很粗糙，很难做到精细化控制，从而无法实现比较精确的按需、按量计费。</p><p>也就说，虽然我们可以随意扩展，但是成本依然高昂。</p><h2 id="Cloud-Warehouse"><a href="#Cloud-Warehouse" class="headerlink" title="Cloud Warehouse"></a><b>Cloud Warehouse</b></h2><p>如果一个 Cloud  Warehouse 满足:</p><ol><li>按需的弹性扩展</li><li>按量的精细化资源控制</li></ol><p>那么它的架构应该是什么样子呢？</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/cloud-warehouse-v1.png" align="center" style="zoom:35%;" /><p>首先它是一个存储和计算分离的架构，其次是计算节点尽量无状态，这样我们可以根据需要添加/删除计算节点，算力随时增加和减少，是一个很平滑的过程，不涉及数据的迁移。<br>node-4 基本是 severless 的，可认为是一个进程，运行完毕自动消亡，在调度上可以做到更加精细化。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/cloud-warehouse-v1-scale.png" align="center" style="zoom:35%;" /><p>大家看到这个架构后或许有一个疑问：<br>Cloud Warehouse 架构比传统架构更简单啊 :)<br>Shared Storage 可以是 AWS S3，还可以是 Azure Blob Storage，都让云来做了，compute 使用类似 Presto 的计算引擎不就是完美的 Cloud Warehouse 了吗？</p><p>这里有一个现实问题挡住了通往理想的大道：</p><p>Shared Storage 通常不是为低延迟、高吞吐而设计，偶尔性的抖动也很难控制，如果靠计算引擎蛮力硬刚，这看起来并不是一个好的产品。</p><h2 id="如何设计？"><a href="#如何设计？" class="headerlink" title="如何设计？"></a><b>如何设计？</b></h2><p>首先我们看下 Cloud Warehouse 里的数据有几种状态:</p><ol><li>Persistent data：通常指用户的数据，重度依赖 Shared Storage</li><li>Intermediate data：一般指计算的临时中间结果，比如排序、JOIN等产生的临时数据</li><li>Metadata：object catalogs, table schema, user 等元数据</li></ol><p>既然 Shared Storage 已经假设是不可靠的，那我们尽量减少从 Shared Storage 读取数据好了，增加 Cache 来解决。</p><p>新的问题又来了，这个 Cache 到底 Cache 什么数据呢，是原始的块数据还是索引？是一个全局 Cache 还是计算节点内的 Cache？</p><h3 id="Snowflake-架构"><a href="#Snowflake-架构" class="headerlink" title="Snowflake 架构"></a>Snowflake 架构</h3><p>我们先看看 Snowflake 老大哥的设计:</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/cloud-warehouse-distributed-cache.png" align="center" style="zoom:35%;" /><p>Snowflake 在计算和存储之间加了一个共享的 Ephemeral Storage，主要用于 Intermediate data 存储，同时肩负着 Persistent data cache，好处是缓存可以充分利用，缺点是这个 Distributed Emphemeral Storage 做到 Elastic 同样面临一些挑战，比如多租户情况下资源隔离等问题。</p><h3 id="Datafuse-架构"><a href="#Datafuse-架构" class="headerlink" title="Datafuse 架构"></a>Datafuse 架构</h3><p>Cloud Warehouse 强调状态分离，我们可以把 Persistent data 预先生成足够多的索引放到 Metadata Service，每个计算节点进行订阅，根据需要更新本地的 Cache，这个架构跟 FireBolt 比较相似。</p><p>这是目前比较简单可行的方式，增加计算节点，只要加热 Cache 即可，同样会面临一些挑战，比如海量的索引信息快速同步问题。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/blog-datafuse/cloud-warehouse-cache-index.png" align="center" style="zoom:35%;" /><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p><a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">Databend</a> 是一个开源的 Cloud Warehouse，重在计算和状态分离，专注云上的弹性扩展，让大家轻松打造出自己的 Data Cloud。</p><p>很高兴，又开了新系列来讲 Databend，一个把 Rust 和 Cloud 进行连接的 Warehouse 项目，充满乐趣和挑战。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a><b>References</b></h2><ol><li><a href="https://github.com/datafuselabs/databend" target="_blank" rel="noopener">Databend: A Modern Real-Time Data Processing &amp; Analytics DBMS with Cloud-Native Architecture</a></li><li><a href="https://www.usenix.org/system/files/nsdi20-paper-vuppalapati.pdf" target="_blank" rel="noopener">Building An Elastic Query Engine on Disaggregated</a></li><li><a href="http://www.vertica.com/wp-content/uploads/2018/05/Vertica_EON_SIGMOD_Paper.pdf" target="_blank" rel="noopener">Eon Mode: Bringing the Vertica Columnar Database to the Cloud</a></li><li><a href="https://www.firebolt.io/resources/firebolt-cloud-data-warehouse-whitepaper" target="_blank" rel="noopener">The Firebolt Cloud Data Warehouse Whitepaper</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;传统数仓架构不适合云？&lt;/p&gt;
&lt;p&gt;Cloud Warehouse 解决了什么问题？&lt;/p&gt;
&lt;p&gt;Cloud Warehouse 架构应该是什么样？&lt;/p&gt;
&lt;p&gt;带着问题，通过实战，向 Cloud Warehouse 出发。&lt;/p&gt;
&lt;h2 id=&quot;Sharding-
      
    
    </summary>
    
    
    
      <category term="snowflake" scheme="https://bohutang.me/tags/snowflake/"/>
    
      <category term="firebolt" scheme="https://bohutang.me/tags/firebolt/"/>
    
      <category term="databend" scheme="https://bohutang.me/tags/databend/"/>
    
      <category term="Cloud Warehouse" scheme="https://bohutang.me/tags/Cloud-Warehouse/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（15）Group By 为什么这么快</title>
    <link href="https://bohutang.me/2021/01/21/clickhouse-and-friends-groupby/"/>
    <id>https://bohutang.me/2021/01/21/clickhouse-and-friends-groupby/</id>
    <published>2021-01-20T16:00:00.000Z</published>
    <updated>2021-01-26T13:03:35.810Z</updated>
    
    <content type="html"><![CDATA[<p>在揭秘 ClickHouse Group By 之前，先聊聊数据库的性能对比测试问题。<br>在虎哥看来，一个“讲武德”的性能对比测试应该提供什么信息呢？</p><p>首先要尊重客观事实，在什么场景下，x 比 y 快？<br>其次是为什么 x 会比 y 快？ </p><p>如果以上两条都做到了，还有一点也比较重要： x 的优势可以支撑多久？ 是架构等带来的长期优势，还是一袋烟的优化所得，是否能持续跟上自己的灵魂。<br>如果只是贴几个妖艳的数字，算不上是 benchmark，而是 benchmarket。</p><p>好了，回到 Group By 正题。<br>相信很多同学已经体验到 ClickHouse Group By 的出色性能，本篇就来分析下快的原因。<br>首先安慰一下，ClickHouse 的 Group By 并没有使用高大上的黑科技，只是摸索了一条相对较优的方案。</p><h2 id="一条-SQL"><a href="#一条-SQL" class="headerlink" title="一条 SQL"></a><b>一条 SQL</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT sum(number) FROM numbers(10) GROUP BY number % 3</span><br></pre></td></tr></table></figure><p>我们就以这条简单的 SQL 作为线索，看看 ClickHouse 怎么实现 Group By 聚合。</p><h2 id="1-生成-AST"><a href="#1-生成-AST" class="headerlink" title="1. 生成 AST"></a><b>1. 生成 AST</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN AST</span><br><span class="line">SELECT sum(number)</span><br><span class="line">FROM numbers(10)</span><br><span class="line">GROUP BY number % 3</span><br><span class="line"></span><br><span class="line">┌─explain─────────────────────────────────────┐</span><br><span class="line">│ SelectWithUnionQuery (children 1)           │</span><br><span class="line">│  ExpressionList (children 1)                │</span><br><span class="line">│   SelectQuery (children 3)                  │</span><br><span class="line">│    ExpressionList (children 1)              │</span><br><span class="line">│     Function sum (children 1)               │  &#x2F;&#x2F; sum 聚合</span><br><span class="line">│      ExpressionList (children 1)            │</span><br><span class="line">│       Identifier number                     │</span><br><span class="line">│    TablesInSelectQuery (children 1)         │</span><br><span class="line">│     TablesInSelectQueryElement (children 1) │</span><br><span class="line">│      TableExpression (children 1)           │</span><br><span class="line">│       Function numbers (children 1)         │</span><br><span class="line">│        ExpressionList (children 1)          │</span><br><span class="line">│         Literal UInt64_10                   │</span><br><span class="line">│    ExpressionList (children 1)              │</span><br><span class="line">│     Function modulo (children 1)            │  &#x2F;&#x2F; number % 3 函数</span><br><span class="line">│      ExpressionList (children 2)            │</span><br><span class="line">│       Identifier number                     │</span><br><span class="line">│       Literal UInt64_3                      │</span><br><span class="line">└─────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure><h2 id="2-生成-Query-Plan"><a href="#2-生成-Query-Plan" class="headerlink" title="2. 生成 Query Plan"></a><b>2. 生成 Query Plan</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN</span><br><span class="line">SELECT sum(number)</span><br><span class="line">FROM numbers(10)</span><br><span class="line">GROUP BY number % 3</span><br><span class="line"></span><br><span class="line">┌─explain───────────────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Expression ((Projection + Before ORDER BY))                                   │ </span><br><span class="line">│   Aggregating                                                                 │ &#x2F;&#x2F; sum 聚合</span><br><span class="line">│     Expression (Before GROUP BY)                                              │ &#x2F;&#x2F; number % 3</span><br><span class="line">│       SettingQuotaAndLimits (Set limits and quota after reading from storage) │</span><br><span class="line">│         ReadFromStorage (SystemNumbers)                                       │</span><br><span class="line">└───────────────────────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></table></figure><p>代码主要在 <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Interpreters/InterpreterSelectQuery.cpp#L1063" target="_blank" rel="noopener">InterpreterSelectQuery::executeImpl@Interpreters/InterpreterSelectQuery.cpp</a></p><h2 id="3-生成-Pipeline"><a href="#3-生成-Pipeline" class="headerlink" title="3. 生成 Pipeline"></a><b>3. 生成 Pipeline</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN PIPELINE</span><br><span class="line">SELECT sum(number)</span><br><span class="line">FROM numbers(10)</span><br><span class="line">GROUP BY number % 3</span><br><span class="line"></span><br><span class="line">┌─explain───────────────────────┐</span><br><span class="line">│ (Expression)                  │</span><br><span class="line">│ ExpressionTransform           │</span><br><span class="line">│   (Aggregating)               │</span><br><span class="line">│   AggregatingTransform        │  &#x2F;&#x2F; sum 计算</span><br><span class="line">│     (Expression)              │</span><br><span class="line">│     ExpressionTransform       │  &#x2F;&#x2F; number % 3 计算</span><br><span class="line">│       (SettingQuotaAndLimits) │</span><br><span class="line">│         (ReadFromStorage)     │</span><br><span class="line">└───────────────────────────────┘</span><br></pre></td></tr></table></figure><h2 id="4-执行-Pipeline"><a href="#4-执行-Pipeline" class="headerlink" title=" 4. 执行 Pipeline "></a><b> 4. 执行 Pipeline </b></h2><p>Pipeline 是从底部往上逐一执行。</p><h3 id="4-1-ReadFromStorage"><a href="#4-1-ReadFromStorage" class="headerlink" title="4.1 ReadFromStorage"></a>4.1 ReadFromStorage</h3><p>首先从 ReadFromStorage 执行，生成一个 block1， 数据如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">┌─number─┐</span><br><span class="line">│      0 │</span><br><span class="line">│      1 │</span><br><span class="line">│      2 │</span><br><span class="line">│      3 │</span><br><span class="line">│      4 │</span><br><span class="line">│      5 │</span><br><span class="line">│      6 │</span><br><span class="line">│      7 │</span><br><span class="line">│      8 │</span><br><span class="line">│      9 │</span><br><span class="line">└────────┘</span><br><span class="line">number类型为 UInt64</span><br></pre></td></tr></table></figure><h3 id="4-2-ExpressionTransform"><a href="#4-2-ExpressionTransform" class="headerlink" title="4.2 ExpressionTransform"></a>4.2 ExpressionTransform</h3><p>ExpressionTransform 包含了 2 个 action:</p><ol><li>名字为 number，type 为 INPUT</li><li>名字为 modulo(number, 3)， type 为 FUNCTION</li></ol><p>经过 ExpressionTransform 运行处理后生成一个新的 block2， 数据如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">┌─number─┬─modulo(number, 3)─┐</span><br><span class="line">│      0 │                 0 │</span><br><span class="line">│      1 │                 1 │</span><br><span class="line">│      2 │                 2 │</span><br><span class="line">│      3 │                 0 │</span><br><span class="line">│      4 │                 1 │</span><br><span class="line">│      5 │                 2 │</span><br><span class="line">│      6 │                 0 │</span><br><span class="line">│      7 │                 1 │</span><br><span class="line">│      8 │                 2 │</span><br><span class="line">│      9 │                 0 │</span><br><span class="line">└────────┴───────────────────┘</span><br><span class="line">number 类型为 UInt64</span><br><span class="line">modulo(number, 3) 类型为 UInt8</span><br></pre></td></tr></table></figure><p>代码主要在 <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Interpreters/ExpressionActions.cpp#L416" target="_blank" rel="noopener">ExpressionActions::execute@Interpreters/ExpressionActions.cpp</a></p><h3 id="4-3-AggregatingTransform"><a href="#4-3-AggregatingTransform" class="headerlink" title="4.3 AggregatingTransform"></a>4.3 AggregatingTransform</h3><p>AggregatingTransform 是 Group By 高性能的核心所在。<br>本示例中的 modulo(number, 3) 类型为 UInt8，在做优化上，ClickHouse 会选择使用数组代替 hashtable作为分组，区分逻辑见 <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Interpreters/Aggregator.cpp#L526" target="_blank" rel="noopener">Interpreters/Aggregator.cpp</a></p><p>在计算 sum 的时候，首先会生成一个数组 [1024]，然后做了一个编译展开(代码 <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/AggregateFunctions/IAggregateFunction.h#L412-L487" target="_blank" rel="noopener">addBatchLookupTable8@AggregateFunctions/IAggregateFunction.h</a>):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">static constexpr size_t UNROLL_COUNT &#x3D; 4;</span><br><span class="line">std::unique_ptr&lt;Data[]&gt; places&#123;new Data[256 * UNROLL_COUNT]&#125;;</span><br><span class="line">bool has_data[256 * UNROLL_COUNT]&#123;&#125;; &#x2F;&#x2F;&#x2F; Separate flags array to avoid heavy initialization.</span><br><span class="line"></span><br><span class="line">size_t i &#x3D; 0;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;&#x2F; Aggregate data into different lookup tables.</span><br><span class="line">size_t batch_size_unrolled &#x3D; batch_size &#x2F; UNROLL_COUNT * UNROLL_COUNT;</span><br><span class="line">for (; i &lt; batch_size_unrolled; i +&#x3D; UNROLL_COUNT)</span><br><span class="line">&#123;</span><br><span class="line">    for (size_t j &#x3D; 0; j &lt; UNROLL_COUNT; ++j)</span><br><span class="line">    &#123;</span><br><span class="line">        size_t idx &#x3D; j * 256 + key[i + j];</span><br><span class="line">        if (unlikely(!has_data[idx]))</span><br><span class="line">        &#123;</span><br><span class="line">            new (&amp;places[idx]) Data;</span><br><span class="line">            has_data[idx] &#x3D; true;</span><br><span class="line">        &#125;</span><br><span class="line">        func.add(reinterpret_cast&lt;char *&gt;(&amp;places[idx]), columns, i + j, nullptr);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>sum(number) … GROUP BY number % 3 计算方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array[0] &#x3D; 0 + 3 + 6 + 9 &#x3D; 18</span><br><span class="line">array[1] &#x3D; 1 + 4 + 7 &#x3D; 12</span><br><span class="line">array[2] &#x3D; 2 + 5 + 8 &#x3D; 15</span><br></pre></td></tr></table></figure><p>这里只是针对 UInt8 做的一个优化分支，那么对于其他类型怎么优化处理呢？<br>ClickHouse 针对不同的类型分别提供了不同的 hashtable，声势比较浩大（代码见 <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Interpreters/Aggregator.h#L68-L103" target="_blank" rel="noopener">Aggregator.h</a>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">using AggregatedDataWithUInt8Key &#x3D; FixedImplicitZeroHashMapWithCalculatedSize&lt;UInt8, AggregateDataPtr&gt;;</span><br><span class="line">using AggregatedDataWithUInt16Key &#x3D; FixedImplicitZeroHashMap&lt;UInt16, AggregateDataPtr&gt;;</span><br><span class="line">using AggregatedDataWithUInt32Key &#x3D; HashMap&lt;UInt32, AggregateDataPtr, HashCRC32&lt;UInt32&gt;&gt;;</span><br><span class="line">using AggregatedDataWithUInt64Key &#x3D; HashMap&lt;UInt64, AggregateDataPtr, HashCRC32&lt;UInt64&gt;&gt;;</span><br><span class="line">using AggregatedDataWithShortStringKey &#x3D; StringHashMap&lt;AggregateDataPtr&gt;;</span><br><span class="line">using AggregatedDataWithStringKey &#x3D; HashMapWithSavedHash&lt;StringRef, AggregateDataPtr&gt;;</span><br><span class="line">using AggregatedDataWithKeys128 &#x3D; HashMap&lt;UInt128, AggregateDataPtr, UInt128HashCRC32&gt;;</span><br><span class="line">using AggregatedDataWithKeys256 &#x3D; HashMap&lt;DummyUInt256, AggregateDataPtr, UInt256HashCRC32&gt;;</span><br><span class="line">using AggregatedDataWithUInt32KeyTwoLevel &#x3D; TwoLevelHashMap&lt;UInt32, AggregateDataPtr, HashCRC32&lt;UInt32&gt;&gt;;</span><br><span class="line">using AggregatedDataWithUInt64KeyTwoLevel &#x3D; TwoLevelHashMap&lt;UInt64, AggregateDataPtr, HashCRC32&lt;UInt64&gt;&gt;;</span><br><span class="line">using AggregatedDataWithShortStringKeyTwoLevel &#x3D; TwoLevelStringHashMap&lt;AggregateDataPtr&gt;;</span><br><span class="line">using AggregatedDataWithStringKeyTwoLevel &#x3D; TwoLevelHashMapWithSavedHash&lt;StringRef, AggregateDataPtr&gt;;</span><br><span class="line">using AggregatedDataWithKeys128TwoLevel &#x3D; TwoLevelHashMap&lt;UInt128, AggregateDataPtr, UInt128HashCRC32&gt;;</span><br><span class="line">using AggregatedDataWithKeys256TwoLevel &#x3D; TwoLevelHashMap&lt;DummyUInt256, AggregateDataPtr, UInt256HashCRC32&gt;;</span><br><span class="line">using AggregatedDataWithUInt64KeyHash64 &#x3D; HashMap&lt;UInt64, AggregateDataPtr, DefaultHash&lt;UInt64&gt;&gt;;</span><br><span class="line">using AggregatedDataWithStringKeyHash64 &#x3D; HashMapWithSavedHash&lt;StringRef, AggregateDataPtr, StringRefHash64&gt;;</span><br><span class="line">using AggregatedDataWithKeys128Hash64 &#x3D; HashMap&lt;UInt128, AggregateDataPtr, UInt128Hash&gt;;</span><br><span class="line">using AggregatedDataWithKeys256Hash64 &#x3D; HashMap&lt;DummyUInt256, AggregateDataPtr, UInt256Hash&gt;;</span><br></pre></td></tr></table></figure><p>如果我们改成 GROUP BY number*100000 后，它会选择 AggregatedDataWithUInt64Key 的 hashtable 作为分组。</p><p>而且 ClickHouse 提供了一种 Two Level 方式，用语应对有大量分组 key 的情况，Level1 先分大组，Level2 小组可以并行计算。<br>针对 String 类型，根据不同的长度，hashtable 也做了很多优化，代码见 <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Common/HashTable/StringHashMap.h#L78-L82" target="_blank" rel="noopener">HashTable/StringHashMap.h</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>ClickHouse 会根据 Group By 的最终类型，选择一个最优的 hashtable 或数组，作为分组基础数据结构，使内存和计算尽量最优。</p><p>这个”最优解“是怎么找到的？从 test 代码可以看出，是不停的尝试、测试验证出来的，浓厚的 bottom-up 哲学范。</p><p>hashtable 测试代码：<a href="https://github.com/ClickHouse/ClickHouse/tree/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Interpreters/tests" target="_blank" rel="noopener">Interpreters/tests</a></p><p>lookuptable 测试代码： <a href="https://github.com/ClickHouse/ClickHouse/blob/27ddf78ba572b893cb5351541f566d1080d8a9c6/src/Common/tests/average.cpp" target="_blank" rel="noopener">tests/average.cpp</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在揭秘 ClickHouse Group By 之前，先聊聊数据库的性能对比测试问题。&lt;br&gt;在虎哥看来，一个“讲武德”的性能对比测试应该提供什么信息呢？&lt;/p&gt;
&lt;p&gt;首先要尊重客观事实，在什么场景下，x 比 y 快？&lt;br&gt;其次是为什么 x 会比 y 快？ &lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="aggregator" scheme="https://bohutang.me/tags/aggregator/"/>
    
      <category term="hashtable" scheme="https://bohutang.me/tags/hashtable/"/>
    
      <category term="groupby" scheme="https://bohutang.me/tags/groupby/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（14）存储计算分离方案与实现</title>
    <link href="https://bohutang.me/2020/09/18/clickhouse-and-friends-compute-storage/"/>
    <id>https://bohutang.me/2020/09/18/clickhouse-and-friends-compute-storage/</id>
    <published>2020-09-17T16:00:00.000Z</published>
    <updated>2020-09-28T00:41:39.600Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-replicatedmergetree.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-28</b></p><p>如果多个 ClickHouse server 可以挂载同一份数据(分布式存储等)，并且每个 server 都可写，这样会有什么好处呢？</p><p>首先，我们可以把副本机制交给分布式存储来保障，上层架构变得简单朴素；</p><p>其次，clickhouse-server 可以在任意机器上增加、减少，使存储和计算能力得到充分发挥。</p><p>本文就来探讨一下 ClickHouse 的存储计算分离方案，实现上并不复杂。</p><h2 id="1-问题"><a href="#1-问题" class="headerlink" title="1. 问题"></a><b>1. 问题</b></h2><p>ClickHouse 运行时数据由两部分组成：<b>内存元数据</b>和<b>磁盘数据</b>。</p><p>我们先看写流程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w1. 开始写入数据</span><br><span class="line">w2. 生成内存part信息，并维护part metadata列表</span><br><span class="line">w3. 把part数据写到磁盘</span><br></pre></td></tr></table></figure><p>再来看读流程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r1. 从part metadata定位需要读取的part</span><br><span class="line">r2. 从磁盘读取part数据</span><br><span class="line">r3. 返回给上层数据</span><br></pre></td></tr></table></figure><p>这样，如果 server1 写了一条数据，只会更新自己内存的 part metadata，其他 server 是感知不到的，这样也就无法查询到刚写入的数据。</p><p>存储计算分离，首先要解决的就是内存状态数据的同步问题。</p><p>在 ClickHouse 里，我们需要解决的是内存中 part metadata 同步问题。</p><h2 id="2-内存数据同步"><a href="#2-内存数据同步" class="headerlink" title="2. 内存数据同步"></a><b>2. 内存数据同步</b></h2><p>在上篇 <a href="/2020/09/13/clickhouse-and-friends-replicated-merge-tree/">&lt;ReplicatedMergeTree表引擎及同步机制&gt;</a> 中，我们知道副本间的数据同步机制：<br>首先同步元数据，再通过元数据获取相应part数据。</p><p>这里，我们借用 ReplicatedMergeTree 同步通道，然后再做减法，同步完元数据后跳过 part 数据的同步，因为磁盘数据只需一个 server 做更新(需要 fsync 语义)即可。</p><p>核心代码：<br>MergeTreeData::renameTempPartAndReplace</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if (!share_storage)</span><br><span class="line">    part-&gt;renameTo(part_name, true);</span><br></pre></td></tr></table></figure><h2 id="3-演示demo"><a href="#3-演示demo" class="headerlink" title="3. 演示demo"></a><b>3. 演示demo</b></h2><figure class="video_container">  <iframe src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/video/clickhouse-storage-compute.mp4" frameborder="0" allowfullscreen="true"> </iframe></figure><p>script：</p><ol><li>首先起 2 个 clickhouse-server，它们都挂载同一份数据 <code>&lt;path&gt;/home/bohu/work/cluster/d1/datas/&lt;/path&gt;</code></li><li>通过 clickhouse-server1(port 9101) 写入一条记录:(111, 3333)</li><li>通过 clickhouse-server2(port 9102) 进行查询正常</li><li>通过 clickhouse-server2(port 9102) truncate 表</li><li>通过 clickhouse-server1(port 9101) 查询正常</li></ol><h2 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4. 代码实现"></a><b>4. 代码实现</b></h2><p><a href="https://github.com/BohuTANG/ClickHouse/commit/f67d98ef408fda1a359e4fb17848619ef1f6e59b" target="_blank" rel="noopener">原型</a><br>需要注意的是，这里只实现了写入数据同步，而且是非常 tricky 的方式。</p><p>由于 DDL 没有实现，所以在 zookeeper 上的注册方式也比较 tricky，demo 里的 replicas 都是手工注册的。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><b>5. 总结</b></h2><p>本文提供一个思路，算是抛砖引玉，同时也期待更加系统的工程实现。</p><p>ClickHouse 暂时还不支持 Distributed Query 功能，如果这个能力支持，ClickHouse 存储计算分离就是一个威力无比的小氢弹。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-replicatedmergetree.png&quot; align=&quot;center&quot; style=&quot;z
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="replicatedmergetree" scheme="https://bohutang.me/tags/replicatedmergetree/"/>
    
      <category term="storage" scheme="https://bohutang.me/tags/storage/"/>
    
      <category term="compute" scheme="https://bohutang.me/tags/compute/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（13）ReplicatedMergeTree表引擎及同步机制</title>
    <link href="https://bohutang.me/2020/09/13/clickhouse-and-friends-replicated-merge-tree/"/>
    <id>https://bohutang.me/2020/09/13/clickhouse-and-friends-replicated-merge-tree/</id>
    <published>2020-09-12T16:00:00.000Z</published>
    <updated>2020-09-18T13:26:56.566Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-replicatedmergetree.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-13</b></p><p>在 MySQL 里，为了保证高可用以及数据安全性会采取主从模式，数据通过 binlog 来进行同步。</p><p>在 ClickHouse 里，我们可以使用 ReplicatedMergeTree 引擎，数据同步通过 zookeeper 完成。</p><p>本文先从搭建一个多 replica 集群开始，然后一窥底层的机制，简单吃两口。</p><h2 id="1-集群搭建"><a href="#1-集群搭建" class="headerlink" title="1. 集群搭建"></a><b>1. 集群搭建</b></h2><p>搭建一个 2 replica 测试集群，由于条件有限，这里在同一台物理机上起 clickhouse-server(2个 replica) + zookeeper(1个)，为了避免端口冲突，两个 replica 端口会有所不同。</p><h3 id="1-1-zookeeper"><a href="#1-1-zookeeper" class="headerlink" title="1.1 zookeeper"></a><b>1.1 zookeeper</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run  -p 2181:2181 --name some-zookeeper --restart always -d zookeeper</span><br></pre></td></tr></table></figure><h3 id="1-2-replica集群"><a href="#1-2-replica集群" class="headerlink" title="1.2 replica集群"></a><b>1.2 replica集群</b></h3><p>replica-1 config.xml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;zookeeper&gt;</span><br><span class="line">   &lt;node index&#x3D;&quot;1&quot;&gt;</span><br><span class="line">      &lt;host&gt;172.17.0.2&lt;&#x2F;host&gt;</span><br><span class="line">      &lt;port&gt;2181&lt;&#x2F;port&gt;</span><br><span class="line">   &lt;&#x2F;node&gt;</span><br><span class="line">&lt;&#x2F;zookeeper&gt;</span><br><span class="line"></span><br><span class="line">&lt;remote_servers&gt;</span><br><span class="line">   &lt;mycluster_1&gt;</span><br><span class="line">      &lt;shard_1&gt;</span><br><span class="line">         &lt;internal_replication&gt;true&lt;&#x2F;internal_replication&gt;</span><br><span class="line">         &lt;replica&gt;</span><br><span class="line">            &lt;host&gt;s1&lt;&#x2F;host&gt;</span><br><span class="line">            &lt;port&gt;9000&lt;&#x2F;port&gt;</span><br><span class="line">         &lt;&#x2F;replica&gt;</span><br><span class="line">         &lt;replica&gt;</span><br><span class="line">            &lt;host&gt;s2&lt;&#x2F;host&gt;</span><br><span class="line">            &lt;port&gt;9001&lt;&#x2F;port&gt;</span><br><span class="line">         &lt;&#x2F;replica&gt;</span><br><span class="line">      &lt;&#x2F;shard_1&gt;</span><br><span class="line">   &lt;&#x2F;mycluster_1&gt;</span><br><span class="line">&lt;&#x2F;remote_servers&gt;</span><br><span class="line"></span><br><span class="line">&lt;macros&gt;</span><br><span class="line">   &lt;cluster&gt;mycluster_1&lt;&#x2F;cluster&gt;</span><br><span class="line">   &lt;shard&gt;1&lt;&#x2F;shard&gt;</span><br><span class="line">   &lt;replica&gt;s1&lt;&#x2F;replica&gt;</span><br><span class="line">&lt;&#x2F;macros&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;tcp_port&gt;9101&lt;&#x2F;tcp_port&gt;</span><br><span class="line">&lt;interserver_http_port&gt;9009&lt;&#x2F;interserver_http_port&gt;</span><br><span class="line">&lt;path&gt;&#x2F;cluster&#x2F;d1&#x2F;datas&#x2F;&lt;&#x2F;path&gt;</span><br></pre></td></tr></table></figure><p>replica-2 config.xml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;zookeeper&gt;</span><br><span class="line">   &lt;node index&#x3D;&quot;1&quot;&gt;</span><br><span class="line">      &lt;host&gt;172.17.0.2&lt;&#x2F;host&gt;</span><br><span class="line">      &lt;port&gt;2181&lt;&#x2F;port&gt;</span><br><span class="line">   &lt;&#x2F;node&gt;</span><br><span class="line">&lt;&#x2F;zookeeper&gt;</span><br><span class="line"></span><br><span class="line">&lt;remote_servers&gt;</span><br><span class="line">   &lt;mycluster_1&gt;</span><br><span class="line">      &lt;shard_1&gt;</span><br><span class="line">         &lt;internal_replication&gt;true&lt;&#x2F;internal_replication&gt;</span><br><span class="line">         &lt;replica&gt;</span><br><span class="line">            &lt;host&gt;s1&lt;&#x2F;host&gt;</span><br><span class="line">            &lt;port&gt;9000&lt;&#x2F;port&gt;</span><br><span class="line">         &lt;&#x2F;replica&gt;</span><br><span class="line">         &lt;replica&gt;</span><br><span class="line">            &lt;host&gt;s2&lt;&#x2F;host&gt;</span><br><span class="line">            &lt;port&gt;9001&lt;&#x2F;port&gt;</span><br><span class="line">         &lt;&#x2F;replica&gt;</span><br><span class="line">      &lt;&#x2F;shard_1&gt;</span><br><span class="line">   &lt;&#x2F;mycluster_1&gt;</span><br><span class="line">&lt;&#x2F;remote_servers&gt;</span><br><span class="line"></span><br><span class="line">&lt;macros&gt;</span><br><span class="line">   &lt;cluster&gt;mycluster_1&lt;&#x2F;cluster&gt;</span><br><span class="line">   &lt;shard&gt;1&lt;&#x2F;shard&gt;</span><br><span class="line">   &lt;replica&gt;s2&lt;&#x2F;replica&gt;</span><br><span class="line">&lt;&#x2F;macros&gt;</span><br><span class="line"></span><br><span class="line">&lt;tcp_port&gt;9102&lt;&#x2F;tcp_port&gt;</span><br><span class="line">&lt;interserver_http_port&gt;9010&lt;&#x2F;interserver_http_port&gt;</span><br><span class="line">&lt;path&gt;&#x2F;cluster&#x2F;d2&#x2F;datas&#x2F;&lt;&#x2F;path&gt;</span><br></pre></td></tr></table></figure><h3 id="1-3-创建测试表"><a href="#1-3-创建测试表" class="headerlink" title="1.3 创建测试表"></a><b>1.3 创建测试表</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE default.rtest1 ON CLUSTER &#39;mycluster_1&#39;</span><br><span class="line">(</span><br><span class="line">    &#96;id&#96; Int64,</span><br><span class="line">    &#96;p&#96; Int16</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; ReplicatedMergeTree(&#39;&#x2F;clickhouse&#x2F;tables&#x2F;replicated&#x2F;test&#39;, &#39;&#123;replica&#125;&#39;)</span><br><span class="line">PARTITION BY p</span><br><span class="line">ORDER BY id</span><br></pre></td></tr></table></figure><h3 id="1-4-查看-zookeeper"><a href="#1-4-查看-zookeeper" class="headerlink" title=" 1.4 查看 zookeeper "></a><b> 1.4 查看 zookeeper </b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it some-zookeeper bash</span><br><span class="line">.&#x2F;bin&#x2F;zkCli.sh</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 17] ls &#x2F;clickhouse&#x2F;tables&#x2F;replicated&#x2F;test&#x2F;replicas</span><br><span class="line">[s1, s2]</span><br></pre></td></tr></table></figure><p>两个 replica 都已经注册到 zookeeper。</p><h2 id="2-同步原理"><a href="#2-同步原理" class="headerlink" title=" 2. 同步原理 "></a><b> 2. 同步原理 </b></h2><p>如果在 replica-1 上执行了一条写入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">replica-1&gt; INSERT INTO rtest VALUES(33,33);</span><br></pre></td></tr></table></figure><p>数据是如何同步到 replica-2 的呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">s1.  replica-1&gt; StorageReplicatedMergeTree::write --&gt; ReplicatedMergeTreeBlockOutputStream::write(const Block &amp; block)</span><br><span class="line">s2.  replica-1&gt; storage.writer.writeTempPart，写入本地磁盘</span><br><span class="line">s3.  replica-1&gt; ReplicatedMergeTreeBlockOutputStream::commitPart</span><br><span class="line">s4.  replica-1&gt; StorageReplicatedMergeTree::getCommitPartOp，提交LogEntry到zookeeper，信息包括:</span><br><span class="line">    ReplicatedMergeTreeLogEntry &#123;</span><br><span class="line">     type: GET_PART,</span><br><span class="line">     source_replica: replica-1,</span><br><span class="line">     new_part_name: part-&gt;name,</span><br><span class="line">     new_part_type: part-&gt;getType</span><br><span class="line">    &#125;</span><br><span class="line">s5.  replica-1&gt; zkutil::makeCreateRequest(zookeeper_path + &quot;&#x2F;log&#x2F;log-0000000022&quot;)，更新log_pointer到zookeeper</span><br><span class="line"></span><br><span class="line">s6.  replica-2&gt; StorageReplicatedMergeTree::queueUpdatingTask()，定时pull任务</span><br><span class="line">s7.  replica-2&gt; ReplicatedMergeTreeQueue::pullLogsToQueue ，拉取</span><br><span class="line">s8.  replica-2&gt; zookeeper-&gt;get(replica_path + &quot;&#x2F;log_pointer&quot;) ，向zookeeper获取当前replica已经同步的位点</span><br><span class="line">s9.  replica-2&gt; zookeeper-&gt;getChildrenWatch(zookeeper_path + &quot;&#x2F;log&quot;) ，向zookeeper获取所有的LogEntry信息</span><br><span class="line">s10. replica-2&gt; 根据同步位点log_pointer从所有LogEntry中筛选需要同步的LogEntry，写到queue</span><br><span class="line">s11. replica-2&gt; StorageReplicatedMergeTree::queueTask，消费queue任务</span><br><span class="line">s12. replica-2&gt; StorageReplicatedMergeTree::executeLogEntry(LogEntry &amp; entry)，根据LogEntry type执行消费</span><br><span class="line">s13. replica-2&gt; StorageReplicatedMergeTree::executeFetch(LogEntry &amp; entry) </span><br><span class="line">s14. replica-2&gt; StorageReplicatedMergeTree::fetchPart，从replica-1的interserver_http_port下载part目录数据</span><br><span class="line">s15. replica-2&gt; MergeTreeData::renameTempPartAndReplace，把文件写入本地并更新内存meta信息</span><br><span class="line">s16. replica-2&gt; 数据同步完成</span><br></pre></td></tr></table></figure><p>也可以进入 zookeeper docker 内部直接查看某个 LogEntry:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 85] get &#x2F;clickhouse&#x2F;tables&#x2F;replicated&#x2F;test&#x2F;log&#x2F;log-0000000022</span><br><span class="line">format version: 4</span><br><span class="line">create_time: 2020-09-13 16:39:05</span><br><span class="line">source replica: s1</span><br><span class="line">block_id: 33_2673203974107464807_7670041793554220344</span><br><span class="line">get</span><br><span class="line">33_2_2_0</span><br></pre></td></tr></table></figure><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a><b>3. 总结</b></h2><p>本文以写入为例，从底层分析了 ClickHouse ReplicatedMergeTree 的工作原理，逻辑并不复杂。</p><p>不同 replica 的数据同步需要 zookeeper(目前社区有人在做etcd的集成 <a href="https://github.com/ClickHouse/ClickHouse/pull/10376" target="_blank" rel="noopener">pr#10376</a>)做元数据协调，是一个订阅/消费模型，涉及具体数据目录还需要去相应的 replica 通过 interserver_http_port 端口进行下载。</p><p>replica 的同步都是以文件目录为单位，这样就带来一个好处：我们<b>可以轻松实现 ClickHouse 的存储计算分离</b>，多个 clickhouse-server 可以同时挂载同一份数据进行计算，而且这些 server 每个节点都是可写，虎哥已经实现了一个可以 work 的原型，详情请参考下篇 <a href="/2020/09/18/clickhouse-and-friends-compute-storage/">&lt;存储计算分离方案与实现&gt;</a>。</p><h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4. 参考"></a><b>4. 参考</b></h2><p>[1]  <a href="https://github.com/ClickHouse/ClickHouse/blob/f37814b36754bf11b52bd9c77d0e15f4d1825033/src/Storages/StorageReplicatedMergeTree.cpp" target="_blank" rel="noopener">StorageReplicatedMergeTree.cpp</a><br>[2] <a href="https://github.com/ClickHouse/ClickHouse/blob/f37814b36754bf11b52bd9c77d0e15f4d1825033/src/Storages/MergeTree/ReplicatedMergeTreeBlockOutputStream.cpp" target="_blank" rel="noopener">ReplicatedMergeTreeBlockOutputStream.cpp</a><br>[3] <a href="https://github.com/ClickHouse/ClickHouse/blob/f37814b36754bf11b52bd9c77d0e15f4d1825033/src/Storages/MergeTree/ReplicatedMergeTreeLogEntry.cpp" target="_blank" rel="noopener">ReplicatedMergeTreeLogEntry.cpp</a><br>[4] <a href="https://github.com/ClickHouse/ClickHouse/blob/f37814b36754bf11b52bd9c77d0e15f4d1825033/src/Storages/MergeTree/ReplicatedMergeTreeQueue.cpp" target="_blank" rel="noopener">ReplicatedMergeTreeQueue.cpp</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-replicatedmergetree.png&quot; align=&quot;center&quot; style=&quot;z
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="replicatedmergetree" scheme="https://bohutang.me/tags/replicatedmergetree/"/>
    
      <category term="zookeeper" scheme="https://bohutang.me/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（12）神奇的物化视图(Materialized View)与原理</title>
    <link href="https://bohutang.me/2020/08/31/clickhouse-and-friends-materialized-view/"/>
    <id>https://bohutang.me/2020/08/31/clickhouse-and-friends-materialized-view/</id>
    <published>2020-08-30T16:00:00.000Z</published>
    <updated>2020-09-01T14:44:59.219Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materializeview.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-08-31</b></p><p>在 ClickHouse 里，物化视图(Materialized View)可以说是一个神奇且强大的东西，用途别具一格。</p><p>本文从底层机制进行分析，看看 ClickHouse 的 Materalized View 是怎么工作的，以方便更好的使用它。</p><h2 id="什么是物化视图"><a href="#什么是物化视图" class="headerlink" title="什么是物化视图"></a><b>什么是物化视图</b></h2><p>对大部分人来说，物化视图这个概念会比较抽象，物化？视图？。。。</p><p>为了更好的理解它，我们先看一个场景。</p><p>假设你是 *hub 一个“幸福”的小程序员，某天产品经理有个需求：实时统计每小时视频下载量。</p><p>用户下载明细表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download LIMIT 10;</span><br><span class="line">+---------------------+--------+--------+</span><br><span class="line">| when                | userid | bytes  |</span><br><span class="line">+---------------------+--------+--------+</span><br><span class="line">| 2020-08-31 18:22:06 |     19 | 530314 |</span><br><span class="line">| 2020-08-31 18:22:06 |     19 | 872957 |</span><br><span class="line">| 2020-08-31 18:22:06 |     19 | 107047 |</span><br><span class="line">| 2020-08-31 18:22:07 |     19 | 214876 |</span><br><span class="line">| 2020-08-31 18:22:07 |     19 | 820943 |</span><br><span class="line">| 2020-08-31 18:22:07 |     19 | 693959 |</span><br><span class="line">| 2020-08-31 18:22:08 |     19 | 882151 |</span><br><span class="line">| 2020-08-31 18:22:08 |     19 | 644223 |</span><br><span class="line">| 2020-08-31 18:22:08 |     19 | 199800 |</span><br><span class="line">| 2020-08-31 18:22:09 |     19 | 511439 |</span><br><span class="line"></span><br><span class="line">... ....</span><br></pre></td></tr></table></figure><p>计算每小时下载量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT toStartOfHour(when) AS hour, userid, count() as downloads, sum(bytes) AS bytes FROM download GROUP BY userid, hour ORDER BY userid, hour;</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| hour                | userid | downloads | bytes      |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| 2020-08-31 18:00:00 |     19 |      6822 | 3378623036 |</span><br><span class="line">| 2020-08-31 19:00:00 |     19 |     10800 | 5424173178 |</span><br><span class="line">| 2020-08-31 20:00:00 |     19 |     10800 | 5418656068 |</span><br><span class="line">| 2020-08-31 21:00:00 |     19 |     10800 | 5404309443 |</span><br><span class="line">| 2020-08-31 22:00:00 |     19 |     10800 | 5354077456 |</span><br><span class="line">| 2020-08-31 23:00:00 |     19 |     10800 | 5390852563 |</span><br><span class="line">| 2020-09-01 00:00:00 |     19 |     10800 | 5369839540 |</span><br><span class="line">| 2020-09-01 01:00:00 |     19 |     10800 | 5384161012 |</span><br><span class="line">| 2020-09-01 02:00:00 |     19 |     10800 | 5404581759 |</span><br><span class="line">| 2020-09-01 03:00:00 |     19 |      6778 | 3399557322 |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">10 rows in set (0.13 sec)</span><br></pre></td></tr></table></figure><p>很容易嘛，不过有个问题：<br>每次都要以 <code>download</code> 表为基础数据进行计算，*hub 数据量太大，无法忍受。</p><p>想到一个办法：如果对 <code>download</code> 进行预聚合，把结果保存到一个新表 <code>download_hour_mv</code>，并随着 <code>download</code> 增量实时更新，每次去查询<code>download_hour_mv</code> 不就可以了。</p><p>这个新表可以看做是一个物化视图，它在 ClickHouse 是一个普通表。</p><h2 id="创建物化视图"><a href="#创建物化视图" class="headerlink" title="创建物化视图"></a><b>创建物化视图</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; CREATE MATERIALIZED VIEW download_hour_mv</span><br><span class="line">ENGINE &#x3D; SummingMergeTree</span><br><span class="line">PARTITION BY toYYYYMM(hour) ORDER BY (userid, hour)</span><br><span class="line">AS SELECT</span><br><span class="line">  toStartOfHour(when) AS hour,</span><br><span class="line">  userid,</span><br><span class="line">  count() as downloads,</span><br><span class="line">  sum(bytes) AS bytes</span><br><span class="line">FROM download WHERE when &gt;&#x3D; toDateTime(&#39;2020-09-01 04:00:00&#39;)</span><br><span class="line">GROUP BY userid, hour</span><br></pre></td></tr></table></figure><p>这个语句主要做了：</p><ul><li>创建一个引擎为 <code>SummingMergeTree</code> 的物化视图 <code>download_hour_mv</code></li><li>物化视图的数据来源于 <code>download</code> 表，并根据 <code>select</code> 语句中的表达式进行相应“物化”操作</li><li>选取一个未来时间(当前时间是 <code>2020-08-31 18:00:00</code>)作为开始点 <code>WHERE when &gt;= toDateTime(&#39;2020-09-01 04:00:00&#39;)</code>，表示在<code>2020-09-01 04:00:00</code> 之后的数据才会被同步到 <code>download_hour_mv</code></li></ul><p>这样，目前 <code>download_hour_mv</code> 是一个空表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY userid, hour;</span><br><span class="line">Empty set (0.02 sec)</span><br></pre></td></tr></table></figure><p>注意：官方有 <a href="https://clickhouse.tech/docs/en/sql-reference/statements/create/view/#materialized" target="_blank" rel="noopener">POPULATE</a> 关键字，但是不建议使用，因为视图创建期间 <code>download</code> 如果有写入数据会丢失，这也是我们加一个 <code>WHERE</code> 作为数据同步点的原因。</p><p>那么，我们如何让源表数据可以一致性的同步到 <code>download_hour_mv</code> 呢？</p><h2 id="物化全量数据"><a href="#物化全量数据" class="headerlink" title="物化全量数据"></a><b>物化全量数据</b></h2><p>在<code>2020-09-01 04:00:00</code>之后，我们可以通过一个带 <code>WHERE</code> 快照的<code>INSERT INTO SELECT...</code> 对 <code>download</code> 历史数据进行物化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; INSERT INTO download_hour_mv</span><br><span class="line">SELECT</span><br><span class="line">  toStartOfHour(when) AS hour,</span><br><span class="line">  userid,</span><br><span class="line">  count() as downloads,</span><br><span class="line">  sum(bytes) AS bytes</span><br><span class="line">FROM download WHERE when &lt; toDateTime(&#39;2020-09-01 04:00:00&#39;)</span><br><span class="line">GROUP BY userid, hour</span><br></pre></td></tr></table></figure><p>查询物化视图：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY hour, userid, downloads DESC;</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| hour                | userid | downloads | bytes      |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| 2020-08-31 18:00:00 |     19 |      6822 | 3378623036 |</span><br><span class="line">| 2020-08-31 19:00:00 |     19 |     10800 | 5424173178 |</span><br><span class="line">| 2020-08-31 20:00:00 |     19 |     10800 | 5418656068 |</span><br><span class="line">| 2020-08-31 21:00:00 |     19 |     10800 | 5404309443 |</span><br><span class="line">| 2020-08-31 22:00:00 |     19 |     10800 | 5354077456 |</span><br><span class="line">| 2020-08-31 23:00:00 |     19 |     10800 | 5390852563 |</span><br><span class="line">| 2020-09-01 00:00:00 |     19 |     10800 | 5369839540 |</span><br><span class="line">| 2020-09-01 01:00:00 |     19 |     10800 | 5384161012 |</span><br><span class="line">| 2020-09-01 02:00:00 |     19 |     10800 | 5404581759 |</span><br><span class="line">| 2020-09-01 03:00:00 |     19 |      6778 | 3399557322 |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">10 rows in set (0.05 sec)</span><br></pre></td></tr></table></figure><p>可以看到数据已经“物化”到 <code>download_hour_mv</code>。</p><h2 id="物化增量数据"><a href="#物化增量数据" class="headerlink" title="物化增量数据"></a><b>物化增量数据</b></h2><p>写一些数据到 <code>download</code>表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; INSERT INTO download</span><br><span class="line">       SELECT</span><br><span class="line">         toDateTime(&#39;2020-09-01 04:00:00&#39;) + number*(1&#x2F;3) as when,</span><br><span class="line">         19,</span><br><span class="line">         rand() % 1000000</span><br><span class="line">       FROM system.numbers</span><br><span class="line">       LIMIT 10;</span><br></pre></td></tr></table></figure><p>查询物化视图 <code>download_hour_mv</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT * FROM download_hour_mv ORDER BY hour, userid, downloads;</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| hour                | userid | downloads | bytes      |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">| 2020-08-31 18:00:00 |     19 |      6822 | 3378623036 |</span><br><span class="line">| 2020-08-31 19:00:00 |     19 |     10800 | 5424173178 |</span><br><span class="line">| 2020-08-31 20:00:00 |     19 |     10800 | 5418656068 |</span><br><span class="line">| 2020-08-31 21:00:00 |     19 |     10800 | 5404309443 |</span><br><span class="line">| 2020-08-31 22:00:00 |     19 |     10800 | 5354077456 |</span><br><span class="line">| 2020-08-31 23:00:00 |     19 |     10800 | 5390852563 |</span><br><span class="line">| 2020-09-01 00:00:00 |     19 |     10800 | 5369839540 |</span><br><span class="line">| 2020-09-01 01:00:00 |     19 |     10800 | 5384161012 |</span><br><span class="line">| 2020-09-01 02:00:00 |     19 |     10800 | 5404581759 |</span><br><span class="line">| 2020-09-01 03:00:00 |     19 |      6778 | 3399557322 |</span><br><span class="line">| 2020-09-01 04:00:00 |     19 |        10 |    5732600 |</span><br><span class="line">+---------------------+--------+-----------+------------+</span><br><span class="line">11 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>可以看到最后一条数据就是我们增量的一个物化聚合，已经实时同步，这是如何做到的呢？</p><h2 id="物化视图原理"><a href="#物化视图原理" class="headerlink" title="物化视图原理"></a><b>物化视图原理</b></h2><p>ClickHouse 的物化视图原理并不复杂，在 <code>download</code> 表有新的数据写入时，如果检测到有物化视图跟它关联，会针对这批写入的数据进行物化操作。</p><p>比如上面新增数据是通过以下 SQL 生成的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">clickhouse&gt; SELECT</span><br><span class="line">    -&gt;          toDateTime(&#39;2020-09-01 04:00:00&#39;) + number*(1&#x2F;3) as when,</span><br><span class="line">    -&gt;          19,</span><br><span class="line">    -&gt;          rand() % 1000000</span><br><span class="line">    -&gt;        FROM system.numbers</span><br><span class="line">    -&gt;        LIMIT 10;</span><br><span class="line">+---------------------+------+-------------------------+</span><br><span class="line">| when                | 19   | modulo(rand(), 1000000) |</span><br><span class="line">+---------------------+------+-------------------------+</span><br><span class="line">| 2020-09-01 04:00:00 |   19 |                  870495 |</span><br><span class="line">| 2020-09-01 04:00:00 |   19 |                  322270 |</span><br><span class="line">| 2020-09-01 04:00:00 |   19 |                  983422 |</span><br><span class="line">| 2020-09-01 04:00:01 |   19 |                  759708 |</span><br><span class="line">| 2020-09-01 04:00:01 |   19 |                  975636 |</span><br><span class="line">| 2020-09-01 04:00:01 |   19 |                  365507 |</span><br><span class="line">| 2020-09-01 04:00:02 |   19 |                  865569 |</span><br><span class="line">| 2020-09-01 04:00:02 |   19 |                  975742 |</span><br><span class="line">| 2020-09-01 04:00:02 |   19 |                   85827 |</span><br><span class="line">| 2020-09-01 04:00:03 |   19 |                  992779 |</span><br><span class="line">+---------------------+------+-------------------------+</span><br><span class="line">10 rows in set (0.02 sec)</span><br></pre></td></tr></table></figure><p>物化视图执行的语句类似：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO download_hour_mv</span><br><span class="line">SELECT</span><br><span class="line">  toStartOfHour(when) AS hour,</span><br><span class="line">  userid,</span><br><span class="line">  count() as downloads,</span><br><span class="line">  sum(bytes) AS bytes</span><br><span class="line">FROM [新增的10条数据] WHERE when &gt;&#x3D; toDateTime(&#39;2020-09-01 04:00:00&#39;)</span><br><span class="line">GROUP BY userid, hour</span><br></pre></td></tr></table></figure><p>代码导航：</p><ol><li><p>添加视图 OutputStream， <a href="https://github.com/ClickHouse/ClickHouse/blob/cb4644ea6d04b3d5900868b4f8d686a03082379a/src/Interpreters/InterpreterInsertQuery.cpp#L313" target="_blank" rel="noopener">InterpreterInsertQuery.cpp</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (table-&gt;noPushingToViews() &amp;&amp; !no_destination)</span><br><span class="line">    out &#x3D; table-&gt;write(query_ptr, metadata_snapshot, context);</span><br><span class="line">else</span><br><span class="line">    out &#x3D; std::make_shared&lt;PushingToViewsBlockOutputStream&gt;(table, metadata_snapshot, context, query_ptr, no_destination);</span><br></pre></td></tr></table></figure></li><li><p>构造 Insert ， <a href="https://github.com/ClickHouse/ClickHouse/blob/cb4644ea6d04b3d5900868b4f8d686a03082379a/src/DataStreams/PushingToViewsBlockOutputStream.cpp#L85" target="_blank" rel="noopener">PushingToViewsBlockOutputStream.cpp</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ASTPtr insert_query_ptr(insert.release());</span><br><span class="line">InterpreterInsertQuery interpreter(insert_query_ptr, *insert_context);</span><br><span class="line">BlockIO io &#x3D; interpreter.execute();</span><br><span class="line">out &#x3D; io.out;</span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li>物化新增数据：<a href="https://github.com/ClickHouse/ClickHouse/blob/cb4644ea6d04b3d5900868b4f8d686a03082379a/src/DataStreams/PushingToViewsBlockOutputStream.cpp#L331" target="_blank" rel="noopener">PushingToViewsBlockOutputStream.cpp</a></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Context local_context &#x3D; *select_context;</span><br><span class="line">local_context.addViewSource(</span><br><span class="line">    StorageValues::create(</span><br><span class="line">        storage-&gt;getStorageID(), metadata_snapshot-&gt;getColumns(), block, storage-&gt;getVirtuals()));</span><br><span class="line">select.emplace(view.query, local_context, SelectQueryOptions());</span><br><span class="line">in &#x3D; std::make_shared&lt;MaterializingBlockInputStream&gt;(select-&gt;execute().getInputStream()</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>物化视图的用途较多。</p><p>比如可以解决表索引问题，我们可以用物化视图创建另外一种物理序，来满足某些条件下的查询问题。</p><p>还有就是通过物化视图的实时同步数据能力，我们可以做到更加灵活的表结构变更。</p><p>更强大的地方是它可以借助 MergeTree 家族引擎(SummingMergeTree、Aggregatingmergetree等)，得到一个实时的预聚合，满足快速查询。</p><p>原理是把增量的数据根据 <code>AS SELECT ...</code> 对其进行处理并写入到物化视图表，物化视图是一种普通表，可以直接读取和写入。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materializeview.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="materialized view" scheme="https://bohutang.me/tags/materialized-view/"/>
    
      <category term="物化视图" scheme="https://bohutang.me/tags/%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（11）MySQL实时复制之GTID模式</title>
    <link href="https://bohutang.me/2020/08/26/clickhouse-and-friends-mysql-gtid-replication/"/>
    <id>https://bohutang.me/2020/08/26/clickhouse-and-friends-mysql-gtid-replication/</id>
    <published>2020-08-25T16:00:00.000Z</published>
    <updated>2020-09-03T15:05:16.838Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-03</b></p><p> <a href="/2020/07/26/clickhouse-and-friends-mysql-replication/">MySQL实时复制原理篇</a></p><p>几天前 ClickHouse 官方发布了 <a href="https://github.com/ClickHouse/ClickHouse/releases/tag/v20.8.1.4447-testing" target="_blank" rel="noopener">v20.8.1.4447-testing</a>，这个版本已经包含了 MaterializeMySQL 引擎，实现了 ClickHouse 实时复制 MySQL 数据的能力，感兴趣的朋友可以通过官方安装包来做体验，安装方式参考: <a href="https://clickhouse.tech/#quick-start" target="_blank" rel="noopener">https://clickhouse.tech/#quick-start</a>，需要注意的是要选择 testing 分支。</p><h2 id="基于位点同步"><a href="#基于位点同步" class="headerlink" title="基于位点同步"></a><b>基于位点同步</b></h2><p>MaterializeMySQL 在 v20.8.1.4447-testing 版本是基于 binlog 位点模式进行同步的。</p><p>每次消费完一批 binlog event，就会记录 event 的位点信息到 .metadata 文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Version:1</span><br><span class="line">Binlog File:mysql-bin.000002</span><br><span class="line">Binlog Position:328</span><br><span class="line">Data Version:1</span><br></pre></td></tr></table></figure><p>这样当 ClickHouse 再次启动时，它会把 {‘mysql-bin.000002’, 328} 二元组通过协议告知 MySQL Server，MySQL 从这个位点开始发送数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1&gt; ClickHouse 发送 &#123;&#39;mysql-bin.000002&#39;, 328&#125; 位点信息给 MySQL</span><br><span class="line">s2&gt; MySQL 找到本地 mysql-bin.000002 文件并定位到 328 偏移位置，读取下一个 event 发送给 ClickHouse</span><br><span class="line">s3&gt; ClickHouse 接收 binlog event 并更新 .metadata位点</span><br></pre></td></tr></table></figure><p>看起来不错哦，但是有个问题：<br>如果 MySQL Server 是一个集群(比如１主２从)，通过 VIP 对外服务，MaterializeMySQL 的 host 指向的是这个 vip。<br>当集群主从发生切换后，{binlog-name, binlog-position} 二元组其实是不准确的，因为集群里主从 binlog 不一定是完全一致的(binlog 可以做 reset 操作)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1&gt; ClickHouse 发送 &#123;&#39;mysql-bin.000002&#39;, 328&#125; 给集群新主 MySQL</span><br><span class="line">s2&gt; 新主 MySQL 发现本地没有 mysql-bin.000002 文件，因为它做过 reset master 操作，binlog 文件是 mysql-bin.000001</span><br><span class="line">... oops ...</span><br></pre></td></tr></table></figure><p>为了解决这个问题，我们开发了 GTID 同步模式，废弃了不安全的位点同步模式，目前已被 upstream merged <a href="https://github.com/ClickHouse/ClickHouse/pull/13820" target="_blank" rel="noopener">#PR13820</a>，下一个 testing 版本即可体验。</p><p>着急的话可以自己编译或通过 <a href="https://clickhouse-builds.s3.yandex.net/0/2b8ad576cc3892d2d760f3f8b670adf17db0c2a0/clickhouse_build_check/report.html" target="_blank" rel="noopener">ClickHouse Build Check for master-20.9.1</a> 下载安装。</p><h2 id="基于GTID同步"><a href="#基于GTID同步" class="headerlink" title="基于GTID同步"></a><b>基于GTID同步</b></h2><p>GTID 是 MySQL 复制增强版，从 MySQL 5.6 版本开始支持，目前已经是 MySQL 主流复制模式。</p><p>它为每个 event 分配一个全局唯一ID和序号，我们可以不用关心 MySQL 集群主从拓扑结构，直接告知 MySQL 这个 GTID 即可，.metadata变为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Version:2</span><br><span class="line">Executed GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5</span><br><span class="line">Data Version:1</span><br></pre></td></tr></table></figure><p><code>f4aee41e-e36f-11ea-8b37-0242ac110002</code> 是生成 event的主机UUID，<code>1-5</code>是已经同步的event区间。</p><p>这样流程就变为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s1&gt; ClickHouse 发送 GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5 给 MySQL</span><br><span class="line">s2&gt; MySQL 根据 GTID:f4aee41e-e36f-11ea-8b37-0242ac110002:1-5 找到本地位点，读取下一个 event 发送给 ClickHouse</span><br><span class="line">s3&gt; ClickHouse 接收 binlog event 并更新 .metadata GTID信息</span><br></pre></td></tr></table></figure><h2 id="MySQL开启GTID"><a href="#MySQL开启GTID" class="headerlink" title=" MySQL开启GTID"></a><b> MySQL开启GTID</b></h2><p>那么，MySQL 侧怎么开启 GTID 呢？增加以下两个参数即可:</p><p><code>--gtid-mode=ON --enforce-gtid-consistency</code></p><p>比如启动一个启用 GTID 的 MySQL docker：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -e MYSQL_ROOT_PASSWORD&#x3D;123 mysql:5.7 mysqld --datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql --server-id&#x3D;1 --log-bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin.log --gtid-mode&#x3D;ON --enforce-gtid-consistency</span><br></pre></td></tr></table></figure><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a><b>注意事项</b></h2><p>启用 GTID 复制模式后，metadata Version 会变为 2，也就是老版本启动时会直接报错，database 需要重建。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>MaterializeMySQL 引擎还处于不停迭代中，对于它我们有一个初步的规划：</p><ul><li><p><b>稳定性保证</b><br>这块需要更多测试，更多试用反馈</p></li><li><p><b>索引优化</b><br>OLTP 索引一般不是为 OLAP 设计，目前索引转换还是依赖 MySQL 表结构，需要更加智能化</p></li><li><p><b>可观测性</b><br>在 ClickHouse 侧可以方便的查看当前同步信息，类似 MySQL <code>show slave status</code></p></li><li><p><b>数据一致性校验</b><br>需要提供方式可以校验 MySQL 和 ClickHouse 数据一致性</p></li></ul><p>MaterializeMySQL 已经是社区功能，仍然有不少的工作要做。期待更多的力量加入，我们的征途不止星辰大海。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="MySQL" scheme="https://bohutang.me/tags/MySQL/"/>
    
      <category term="replication" scheme="https://bohutang.me/tags/replication/"/>
    
      <category term="GTID" scheme="https://bohutang.me/tags/GTID/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（10）MergeTree Write-Ahead Log</title>
    <link href="https://bohutang.me/2020/08/18/clickhouse-and-friends-merge-tree-wal/"/>
    <id>https://bohutang.me/2020/08/18/clickhouse-and-friends-merge-tree-wal/</id>
    <published>2020-08-17T16:00:00.000Z</published>
    <updated>2020-09-19T12:35:02.035Z</updated>
    
    <content type="html"><![CDATA[<p><b>最后更新: 2020-09-18</b></p><p>数据库系统为了提高写入性能，会把数据先写到内存，等“攒”到一定程度后再回写到磁盘，比如 MySQL 的 buffer pool 机制。</p><p>因为数据先写到内存，为了数据的安全性，我们需要一个 Write-Ahead Log (WAL) 来保证内存数据的安全性。</p><p>今天我们来看看 ClickHouse 新增的 <a href="https://github.com/ClickHouse/ClickHouse/pull/8290" target="_blank" rel="noopener">MergeTreeWriteAheadLog</a> 模块，它到底解决了什么问题。</p><h2 id="高频写问题"><a href="#高频写问题" class="headerlink" title="高频写问题"></a><b>高频写问题</b></h2><p>对于 ClickHouse MergeTree 引擎，每次写入(即使１条数据)都会在磁盘生成一个分区目录(part)，等着 merge 线程合并。</p><p>如果有多个客户端，每个客户端写入的数据量较少、次数较频繁的情况下，就会引发 <code>DB::Exception: Too many parts</code> 错误。</p><p>这样就对客户端有一定的要求，比如需要做 batch 写入。</p><p>或者，写入到 Buffer 引擎，定时的刷回 MergeTree，缺点是在宕机时可能会丢失数据。</p><h2 id="MergeTree-WAL"><a href="#MergeTree-WAL" class="headerlink" title="MergeTree WAL"></a><b>MergeTree WAL</b></h2><h3 id="1-默认模式"><a href="#1-默认模式" class="headerlink" title="1. 默认模式"></a><b>1. 默认模式</b></h3><p>我们先看看在没有 WAL 情况下，MergeTree 是如何写入的：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/mergetree-part-raw.png" align="center" style="zoom:50%;" /><p>每次写入 MergeTree 都会直接在磁盘上创建分区目录，并生成分区数据，这种模式其实就是 WAL + 数据的融合。</p><p>很显然，这种模式不适合频繁写操作的情况，否则会生成非常多的分区目录和文件，引发 <code>Too many parts</code> 错误。</p><h3 id="2-WAL模式"><a href="#2-WAL模式" class="headerlink" title="2. WAL模式"></a><b>2. WAL模式</b></h3><p>设置SETTINGS: <code>min_rows_for_compact_part=2</code>，分别执行２条写 SQL，数据会先写到 wal.bin 文件：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/mergetree-part-wal.png" align="center" style="zoom:50%;" /><p>当满足  <code>min_rows_for_compact_part=2</code> 后，merger 线程触发合并操作，生成  <code>1_1_2_1</code> 分区，也就是完成了 wal.bin 里的 <code>1_1_1_0</code> 和 <code>1_2_2_0</code> 两个分区的合并操作。当我们执行第三条 SQL 写入:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into default.mt(a,b,c) values(1,3,3)</span><br></pre></td></tr></table></figure><p>数据块(分区)会继续追加到 wal.bin 尾部：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/mergetree-part-wal-merge.png" align="center" style="zoom:50%;" /><p>此时，3 条数据分布在两个地方：分区 <code>1_1_2_1</code>， wal.bin 里的 <code>1_3_3_0</code>。</p><p>这样就有一个问题：当我们执行查询的时候，数据是怎么合并的呢？</p><p>MergeTree 使用全局结构 <code>data_parts_indexes</code> 维护分区信息，当服务启动的时候，<code>MergeTreeData::loadDataParts</code>方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. data_parts_indexes.insert(1_1_2_1)</span><br><span class="line">2. 读取 wal.bin，通过 getActiveContainingPart 判断分区是否已经merge到磁盘：1_1_1_0 已经存在, 1_2_2_0 已经存在，data_parts_indexes.insert(1_3_3_0)</span><br><span class="line">3. data_parts_indexes:&#123;1_1_2_1,1_3_3_0&#125;</span><br></pre></td></tr></table></figure><p>这样，它总是能维护全局的分区信息。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>WAL 功能在 <a href="https://github.com/ClickHouse/ClickHouse/pull/8290" target="_blank" rel="noopener">PR＃8290</a> 实现，master 分支已经默认开启。</p><p>MergeTree 通过 WAL 来保护客户端的高频、少量写机制，减少服务端目录和文件数量，让客户端操作尽可能简单、高效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;b&gt;最后更新: 2020-09-18&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;数据库系统为了提高写入性能，会把数据先写到内存，等“攒”到一定程度后再回写到磁盘，比如 MySQL 的 buffer pool 机制。&lt;/p&gt;
&lt;p&gt;因为数据先写到内存，为了数据的安全性，我们需要一个 Write
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mergetree" scheme="https://bohutang.me/tags/mergetree/"/>
    
      <category term="WAL" scheme="https://bohutang.me/tags/WAL/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（9）MySQL实时复制与实现</title>
    <link href="https://bohutang.me/2020/07/26/clickhouse-and-friends-mysql-replication/"/>
    <id>https://bohutang.me/2020/07/26/clickhouse-and-friends-mysql-replication/</id>
    <published>2020-07-25T16:00:00.000Z</published>
    <updated>2020-09-07T23:52:58.642Z</updated>
    
    <content type="html"><![CDATA[<img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png" align="center" style="zoom:50%;" /><p><b>最后更新: 2020-09-08</b></p><p>很多人看到标题还以为自己走错了夜场，其实没有。</p><p>ClickHouse  可以挂载为 MySQL 的一个从库 ，先全量再增量的实时同步 MySQL 数据，这个功能可以说是今年最亮眼、最刚需的功能，基于它我们可以轻松的打造一套企业级解决方案，让 OLTP 和 OLAP 的融合从此不再头疼。</p><p>目前支持 MySQL 5.6/5.7/8.0 版本，兼容 Delete/Update 语句，及大部分常用的 DDL 操作。<br><a href="https://github.com/ClickHouse/ClickHouse/pull/10851" target="_blank" rel="noopener">代码</a>已经合并到 upstream master 分支，预计在20.8版本作为experimental 功能发布。</p><p>毕竟是两个异构生态的融合，仍然有不少的工作要做，同时也期待着社区用户的反馈，以加速迭代。</p><h2 id="代码获取"><a href="#代码获取" class="headerlink" title="代码获取"></a><b>代码获取</b></h2><p>获取 <a href="https://github.com/ClickHouse/ClickHouse" target="_blank" rel="noopener">clickhouse/master</a> 代码编译即可，方法见 <a href="/2020/06/05/clickhouse-and-friends-development/">ClickHouse和他的朋友们（1）编译、开发、测试</a>…</p><h2 id="MySQL-Master"><a href="#MySQL-Master" class="headerlink" title=" MySQL Master"></a><b> MySQL Master</b></h2><p>我们需要一个开启 binlog 的 MySQL 作为 master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -e MYSQL_ROOT_PASSWORD&#x3D;123 mysql:5.7 mysqld --datadir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql --server-id&#x3D;1 --log-bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin.log --gtid-mode&#x3D;ON --enforce-gtid-consistency</span><br></pre></td></tr></table></figure><p>创建数据库和表，并写入数据:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database ckdb;</span><br><span class="line">mysql&gt; use ckdb;</span><br><span class="line">mysql&gt; create table t1(a int not null primary key, b int);</span><br><span class="line">mysql&gt; insert into t1 values(1,1),(2,2);</span><br><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 1 |    1 |</span><br><span class="line">| 2 |    2 |</span><br><span class="line">+---+------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><h2 id="ClickHouse-Slave"><a href="#ClickHouse-Slave" class="headerlink" title="ClickHouse Slave "></a><b>ClickHouse Slave </b></h2><p>目前以 database 为单位进行复制，不同的 database 可以来自不同的 MySQL master，这样就可以实现多个 MySQL 源数据同步到一个 ClickHouse 做 OLAP 分析功能。</p><p>首先开启体验开关:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) SET allow_experimental_database_materialize_mysql&#x3D;1;</span><br></pre></td></tr></table></figure><p>创建一个复制通道：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) CREATE DATABASE ckdb ENGINE &#x3D; MaterializeMySQL(&#39;172.17.0.2:3306&#39;, &#39;ckdb&#39;, &#39;root&#39;, &#39;123&#39;);</span><br><span class="line">clickhouse :) use ckdb;</span><br><span class="line">clickhouse :) show tables;</span><br><span class="line">┌─name─┐</span><br><span class="line">│ t1   │</span><br><span class="line">└──────┘</span><br><span class="line">clickhouse :) select * from t1;</span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 1 │ 1 │</span><br><span class="line">└───┴───┘</span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 2 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">2 rows in set. Elapsed: 0.017 sec.</span><br></pre></td></tr></table></figure><p>看下 ClickHouse 的同步位点：<br>cat ckdatas/metadata/ckdb/.metadata</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Version:1</span><br><span class="line">Binlog File:mysql-bin.000001</span><br><span class="line">Binlog Position:913</span><br><span class="line">Data Version:0</span><br></pre></td></tr></table></figure><h2 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a><b>Delete</b></h2><p>首先在 MySQL Master 上执行一个删除操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from t1 where a&#x3D;1;</span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br></pre></td></tr></table></figure><p>然后在 ClickHouse Slave 侧查看记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select * from t1;</span><br><span class="line"></span><br><span class="line">SELECT *</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 2 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">1 rows in set. Elapsed: 0.032 sec.</span><br></pre></td></tr></table></figure><p>此时的 metadata 里 Data Version 已经递增到 2:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat ckdatas&#x2F;metadata&#x2F;ckdb&#x2F;.metadata </span><br><span class="line">Version:1</span><br><span class="line">Binlog File:mysql-bin.000001</span><br><span class="line">Binlog Position:1171</span><br><span class="line">Data Version:2</span><br></pre></td></tr></table></figure><h2 id="Update"><a href="#Update" class="headerlink" title="Update"></a><b>Update</b></h2><p>MySQL Master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 2 |    2 |</span><br><span class="line">+---+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; update t1 set b&#x3D;b+1;</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 2 |    3 |</span><br><span class="line">+---+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>ClickHouse Slave:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select * from t1;</span><br><span class="line"></span><br><span class="line">SELECT *</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 3 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">1 rows in set. Elapsed: 0.023 sec.</span><br></pre></td></tr></table></figure><h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a><b>性能测试</b></h2><h3 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a><b>测试环境</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MySQL          8C16G 云主机, 192.168.0.3，基础数据 10188183 条记录</span><br><span class="line">ClickHouse     8C16G 云主机, 192.168.0.4</span><br><span class="line">benchyou       8C8G  云主机,  192.168.0.5, 256并发写, https:&#x2F;&#x2F;github.com&#x2F;xelabs&#x2F;benchyou</span><br></pre></td></tr></table></figure><p>性能测试跟硬件环境有较大关系，这里使用的是云主机模式，数据供参考。</p><h3 id="全量性能"><a href="#全量性能" class="headerlink" title="全量性能"></a><b>全量性能</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">8c16G-vm :) create database sbtest engine&#x3D;MaterializeMySQL(&#39;192.168.0.3:3306&#39;, &#39;sbtest&#39;, &#39;test&#39;, &#39;123&#39;);</span><br><span class="line"></span><br><span class="line">8c16G-vm :) watch lv1;</span><br><span class="line"></span><br><span class="line">WATCH lv1</span><br><span class="line"></span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│       0 │ 2020-07-29 06:36:04 │        1 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 1113585 │ 2020-07-29 06:36:05 │        2 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 2227170 │ 2020-07-29 06:36:07 │        3 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 3340755 │ 2020-07-29 06:36:10 │        4 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 4454340 │ 2020-07-29 06:36:13 │        5 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 5567925 │ 2020-07-29 06:36:16 │        6 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 6681510 │ 2020-07-29 06:36:18 │        7 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 7795095 │ 2020-07-29 06:36:22 │        8 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 8908680 │ 2020-07-29 06:36:25 │        9 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌──count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 10022265 │ 2020-07-29 06:36:28 │       10 │</span><br><span class="line">└──────────┴─────────────────────┴──────────┘</span><br><span class="line">┌──count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│ 10188183 │ 2020-07-29 06:36:28 │       11 │</span><br><span class="line">└──────────┴─────────────────────┴──────────┘</span><br><span class="line">← Progress: 11.00 rows, 220.00 B (0.16 rows&#x2F;s., 3.17 B&#x2F;s.)</span><br></pre></td></tr></table></figure><p>在这个硬件环境下，全量同步性能大概是 <b>424507/s</b>，<b>42w</b> 事务每秒。<br>因为全量的数据之间没有依赖关系，可以进一步优化成并行，加速同步。<br>全量的性能直接决定 ClickHouse slave 坏掉后重建的速度，如果你的 MySQL 有<b> 10 亿</b>条数据，大概<b> 40 分钟</b>就可以重建完成。</p><h3 id="增量性能-实时同步"><a href="#增量性能-实时同步" class="headerlink" title="增量性能(实时同步)"></a><b>增量性能(实时同步)</b></h3><p>在当前配置下，ClickHouse slave 单线程回放消费能力大于 MySQL master 256 并发下生产能力，通过测试可以看到它们保持<b>实时同步</b>。</p><p>benchyou 压测数据，<b>2.1w</b> 事务/秒(MySQL 在当前环境下TPS上不去):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;benchyou --mysql-host&#x3D;192.168.0.3 --mysql-user&#x3D;test --mysql-password&#x3D;123 --oltp-tables-count&#x3D;1 --write-threads&#x3D;256 --read-threads&#x3D;0</span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[13s]        [r:0,w:256,u:0,d:0]  19962    19962   0    </span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[14s]        [r:0,w:256,u:0,d:0]  20415    20415   0 </span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[15s]        [r:0,w:256,u:0,d:0]  21131    21131   0</span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[16s]        [r:0,w:256,u:0,d:0]  21606    21606   0</span><br><span class="line"></span><br><span class="line">time            thds               tps     wtps    rtps</span><br><span class="line">[17s]        [r:0,w:256,u:0,d:0]  22505    22505   0</span><br></pre></td></tr></table></figure><p>ClickHouse 侧单线程回放能力，<b>2.1w</b> 事务/秒，实时同步：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  150732 │ 2020-07-30 05:17:15 │       17 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  155477 │ 2020-07-30 05:17:16 │       18 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  160222 │ 2020-07-30 05:17:16 │       19 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  164967 │ 2020-07-30 05:17:16 │       20 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  169712 │ 2020-07-30 05:17:16 │       21 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  174457 │ 2020-07-30 05:17:16 │       22 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  179202 │ 2020-07-30 05:17:17 │       23 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  183947 │ 2020-07-30 05:17:17 │       24 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  188692 │ 2020-07-30 05:17:17 │       25 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  193437 │ 2020-07-30 05:17:17 │       26 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br><span class="line">┌─count()─┬───────────────now()─┬─_version─┐</span><br><span class="line">│  198182 │ 2020-07-30 05:17:17 │       27 │</span><br><span class="line">└─────────┴─────────────────────┴──────────┘</span><br></pre></td></tr></table></figure><h2 id="实现机制"><a href="#实现机制" class="headerlink" title="实现机制"></a><b>实现机制</b></h2><p>在探讨机制之前，首先需要了解下 MySQL 的 binlog event ，主要有以下几种类型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. MYSQL_QUERY_EVENT　　　　-- DDL</span><br><span class="line">2. MYSQL_WRITE_ROWS_EVENT　-- insert数据</span><br><span class="line">3. MYSQL_UPDATE_ROWS_EVENT -- update数据</span><br><span class="line">4. MYSQL_DELETE_ROWS_EVENT -- delete数据</span><br></pre></td></tr></table></figure><p>当一个事务提交后，MySQL 会把执行的 SQL 处理成相应的 binlog event，并持久化到 binlog 文件。</p><p>binlog 是 MySQL 对外输出的重要途径，只要你实现 MySQL Replication Protocol，就可以流式的消费MySQL 生产的 binlog event，具体协议见 <a href="https://dev.mysql.com/doc/internals/en/replication-protocol.html" target="_blank" rel="noopener">Replication Protocol</a>。</p><p>由于历史原因，协议繁琐而诡异，这不是本文重点。</p><p>对于 ClickHouse 消费 MySQL binlog 来说，主要有以下３个难点：</p><ul><li>DDL 兼容</li><li>Delete/Update 支持</li><li>Query 过滤</li></ul><h3 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a><b>DDL</b></h3><p>DDL 兼容花费了大量的代码去实现。</p><p>首先，我们看看 MySQL 的表复制到 ClickHouse 后会变成什么样子。</p><p>MySQL master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show create table t1\G;</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">       Table: t1</span><br><span class="line">Create Table: CREATE TABLE &#96;t1&#96; (</span><br><span class="line">  &#96;a&#96; int(11) NOT NULL,</span><br><span class="line">  &#96;b&#96; int(11) DEFAULT NULL,</span><br><span class="line">  PRIMARY KEY (&#96;a&#96;)</span><br><span class="line">) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;latin1</span><br></pre></td></tr></table></figure><p> ClickHouse slave:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ATTACH TABLE t1</span><br><span class="line">(</span><br><span class="line">    &#96;a&#96; Int32,</span><br><span class="line">    &#96;b&#96; Nullable(Int32),</span><br><span class="line">    &#96;_sign&#96; Int8,</span><br><span class="line">    &#96;_version&#96; UInt64</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; ReplacingMergeTree(_version)</span><br><span class="line">PARTITION BY intDiv(a, 4294967)</span><br><span class="line">ORDER BY tuple(a)</span><br><span class="line">SETTINGS index_granularity &#x3D; 8192</span><br></pre></td></tr></table></figure><p>可以看到：</p><ul><li>默认增加了 2 个隐藏字段：_sign(-1删除, 1写入) 和 _version(数据版本) </li><li>引擎转换成了 ReplacingMergeTree，以 _version 作为 column version</li><li>原主键字段 a 作为排序和分区键</li></ul><p>这只是一个表的复制，其他还有非常多的DDL处理，比如增加列、索引等，感兴趣可以观摩 Parsers/MySQL 下代码。</p><h3 id="索引转换"><a href="#索引转换" class="headerlink" title="索引转换"></a><b>索引转换</b></h3><p>MySQL 对应的主键/索引如何对应到 MaterializeMySQL 表结构呢？</p><p>首先针对 MySQL 建表语句进行key扫描，<a href="https://github.com/ClickHouse/ClickHouse/blob/1076a42cf599c7eedd7d0cda2246f9afaf6434a4/src/Interpreters/MySQL/InterpretersMySQLDDLQuery.cpp#L157" target="_blank" rel="noopener">InterpretersMySQLDDLQuery::getKeys</a>：</p><ol><li>扫描 unique_key</li><li>扫描 primary_key</li><li>扫描 auto_increment</li></ol><p>其次根据 key 按照以下顺序生成 OrderBy tuple 表达式，<a href="https://github.com/ClickHouse/ClickHouse/blob/1076a42cf599c7eedd7d0cda2246f9afaf6434a4/src/Interpreters/MySQL/InterpretersMySQLDDLQuery.cpp#L288" target="_blank" rel="noopener">InterpretersMySQLDDLQuery::getOrderByPolicy</a>：</p><ol><li>primary_key[not increment]</li><li>key[not increment]</li><li>unique[not increment]</li><li>unique[increment]</li><li>key[increment]</li><li>primary_key[increment]</li></ol><p>ClickHouse 的物理序目前只有一种(多物理序有一定的规划)，是有 OrderBy 决定，所以 MaterializeMySQL 索引如果利用不佳，可以使用物化视图建立新的物理序解决。</p><h3 id="Update和Delete"><a href="#Update和Delete" class="headerlink" title="Update和Delete"></a><b>Update和Delete</b></h3><p>当我们在 MySQL master 执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; delete from t1 where a&#x3D;1;</span><br><span class="line">mysql&gt; update t1 set b&#x3D;b+1;</span><br></pre></td></tr></table></figure><p>ClickHouse t1数据（把  _sign 和 _version 一并查询）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select a,b,_sign, _version from t1;</span><br><span class="line"></span><br><span class="line">SELECT </span><br><span class="line">    a,</span><br><span class="line">    b,</span><br><span class="line">    _sign,</span><br><span class="line">    _version</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │     1 │        1 │</span><br><span class="line">│ 2 │ 2 │     1 │        1 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br></pre></td></tr></table></figure><p>根据返回结果，可以看到是由 3 个 part 组成。</p><p>part1 由 <code>mysql&gt; insert into t1 values(1,1),(2,2)</code> 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │     1 │        1 │</span><br><span class="line">│ 2 │ 2 │     1 │        1 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br></pre></td></tr></table></figure><p>part2 由 <code>mysql&gt; delete from t1 where a=1</code> 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">说明：</span><br><span class="line">_sign &#x3D; -1表明处于删除状态</span><br></pre></td></tr></table></figure><p>part3 由 <code>update t1 set b=b+1</code> 生成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br></pre></td></tr></table></figure><p>使用 final 查询：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select a,b,_sign,_version from t1 final;</span><br><span class="line"></span><br><span class="line">SELECT </span><br><span class="line">    a,</span><br><span class="line">    b,</span><br><span class="line">    _sign,</span><br><span class="line">    _version</span><br><span class="line">FROM t1</span><br><span class="line">FINAL</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line"></span><br><span class="line">2 rows in set. Elapsed: 0.016 sec.</span><br></pre></td></tr></table></figure><p>可以看到 ReplacingMergeTree 已经根据 _version 和 OrderBy 对记录进行去重。</p><h3 id="Query"><a href="#Query" class="headerlink" title="Query"></a><b>Query</b></h3><p>MySQL master:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from t1;</span><br><span class="line">+---+------+</span><br><span class="line">| a | b    |</span><br><span class="line">+---+------+</span><br><span class="line">| 2 |    3 |</span><br><span class="line">+---+------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>ClickHouse slave:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">clickhouse :) select * from t1;</span><br><span class="line"></span><br><span class="line">SELECT *</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┐</span><br><span class="line">│ 2 │ 3 │</span><br><span class="line">└───┴───┘</span><br><span class="line"></span><br><span class="line">clickhouse :) select *,_sign,_version from t1;</span><br><span class="line"></span><br><span class="line">SELECT </span><br><span class="line">    *,</span><br><span class="line">    _sign,</span><br><span class="line">    _version</span><br><span class="line">FROM t1</span><br><span class="line"></span><br><span class="line">┌─a─┬─b─┬─_sign─┬─_version─┐</span><br><span class="line">│ 1 │ 1 │    -1 │        2 │</span><br><span class="line">│ 2 │ 3 │     1 │        3 │</span><br><span class="line">└───┴───┴───────┴──────────┘</span><br><span class="line">说明：这里还有一条删除记录，_sign为-1</span><br></pre></td></tr></table></figure><p>MaterializeMySQL 被定义成一种存储引擎，所以在读取的时候，会根据 _sign 状态进行判断，如果是-1则是已经删除，进行过滤。</p><h2 id="并行回放"><a href="#并行回放" class="headerlink" title="并行回放"></a><b>并行回放</b></h2><p>为什么 MySQL 需要并行回放？</p><p>假设 MySQL master 有 1024 个并发同时写入、更新数据，瞬间产生大量的 binlog event ，MySQL slave 上只有一个线程一个 event 接着一个 event 式回放，于是 MySQL 实现了并行回放功能！</p><p>那么，MySQL slave 回放时能否完全(或接近)模拟出 master 当时的 1024 并发行为呢？</p><p>要想并行首先要解决的就是依赖问题：我们需要 master 标记出哪些 event 可以并行，哪些 event 有先后关系，因为它是第一现场。</p><p>MySQL 通过在 binlog 里增加:</p><ul><li>last_committed，相同则可以并行</li><li>sequece_number，较小先执行，描述先后依赖</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">last_committed&#x3D;3   sequece_number&#x3D;4   -- event1</span><br><span class="line">last_committed&#x3D;4   sequece_number&#x3D;5   -- event2</span><br><span class="line">last_committed&#x3D;4   sequece_number&#x3D;6   -- event3</span><br><span class="line">last_committed&#x3D;5   sequece_number&#x3D;7   -- event4</span><br></pre></td></tr></table></figure><p>event2 和 event3 则可以并行，event4 需要等待前面 event 完成才可以回放。<br>以上只是一个大体原理，目前 MySQL 有３种并行模式可以选择：</p><ol><li>基于 database 并行</li><li>基于 group commit 并行</li><li>基于主键不冲突的 write set 并行</li></ol><p>最大程度上让 MySQL slave加速回放，整套机制还是异常复杂的。</p><p>回到 ClickHouse slave 问题，我们采用的单线程回放，延迟已经不是主要问题，这是由它们的机制决定的：<br>MySQL slave 回放时，需要把 binlog event 转换成  SQL，然后模拟 master 的写入，这种逻辑复制是导致性能低下的最重要原因。<br>而 ClickHouse 在回放上，直接把 binlog event 转换成 底层 block 结构，然后直接写入底层的存储引擎，接近于物理复制，可以理解为把 binlog event 直接回放到 InnoDB 的 page。</p><h2 id="读取最新"><a href="#读取最新" class="headerlink" title="读取最新"></a><b>读取最新</b></h2><p>虽然 ClickHouse slave 回放非常快，接近于实时，如何在ClickHouse slave上总是读取到最新的数据呢？</p><p>其实非常简单，借助 MySQL binlog GTID 特性，每次读的时候，我们跟 ｍaster 做一次 executed_gtid 同步，然后等待这些 executed_gtid 回放完毕即可。</p><h2 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a><b>数据一致性</b></h2><p>对一致性要求较高的场景，我们怎么验证 MySQL master 的数据和 ClickHouse slave 的数据一致性呢？</p><p>这块初步想法是提供一个兼容 MySQL checksum 算法的函数，我们只需对比两边的 checksum 值即可。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>ClickHouse 实时复制同步 MySQL 数据是 upstream 2020 的一个 roadmap，在整体构架上比较有挑战一直无人接单，挑战主要来自两方面：</p><ul><li>对 MySQL 复制通道与协议非常熟悉</li><li>对 ClickHouse 整体机制非常熟悉</li></ul><p>这样，在两个本来有点遥远的山头中间架起了一座高速，这条 <a href="https://github.com/ClickHouse/ClickHouse/pull/10851" target="_blank" rel="noopener">10851号</a> 高速由 zhang1024(ClickHouse侧) 和 BohuTANG(MySQL复制) 两个修路工联合承建，目前已经合并到 upstream 分支。</p><p>关于同步 MySQL 的数据，目前大家的方案基本都是在中间安置一个 binlog 消费工具，这个工具对 event 进行解析，然后再转换成 ClickHouse 的 SQL 语句，写到 ClickHouse server，链路较长，性能损耗较大。</p><p> <a href="https://github.com/ClickHouse/ClickHouse/pull/10851" target="_blank" rel="noopener">10851号</a> 高速是在 ClickHouse 内部实现一套 binlog 消费方案，然后根据 event 解析成 ClickHouse 内部的 block 结构，再直接回写到底层存储引擎，几乎是最高效的一种实现方式，实现与 MySQL 实时同步的能力，让分析更接近现实。</p><p>基于 database 级的复制，实现了多源复制的功能，如果复制通道坏掉，我们只需在 ClickHouse 侧删掉 database 再重建一次即可，非常快速、方便，OLTP+OLAP 就是这么简单！</p><p>要想富，先修路！</p><p><a href="/3030/12/12/clickhouse-and-friends-mysql-replication-materializemysql/">【置顶】ClickHouse MaterializeMySQL实时同步MySQL汇总</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;img src=&quot;https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/clickhouse-map-2020-materialzemysql.png&quot; align=&quot;center&quot; style=&quot;zoom:
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="MySQL" scheme="https://bohutang.me/tags/MySQL/"/>
    
      <category term="replication" scheme="https://bohutang.me/tags/replication/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（８）纯手工打造的SQL解析器</title>
    <link href="https://bohutang.me/2020/07/25/clickhouse-and-friends-parser/"/>
    <id>https://bohutang.me/2020/07/25/clickhouse-and-friends-parser/</id>
    <published>2020-07-24T16:00:00.000Z</published>
    <updated>2020-08-10T07:17:16.088Z</updated>
    
    <content type="html"><![CDATA[<p>现实生活中的物品一旦被标记为“纯手工打造”，给人的第一感觉就是“上乘之品”，一个字“贵”，比如北京老布鞋。</p><p>但是在计算机世界里，如果有人告诉你 ClickHouse 的 SQL 解析器是纯手工打造的，是不是很惊讶！<br>这个问题引起了不少网友的关注，所以本篇聊聊 ClickHouse 的纯手工解析器，看看它们的底层工作机制及优缺点。</p><p>枯燥先从一个 SQL 开始:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN SELECT a,b FROM t1</span><br></pre></td></tr></table></figure><h2 id="token"><a href="#token" class="headerlink" title="token "></a><b>token </b></h2><p>首先对 SQL 里的字符逐个做判断，然后根据其关联性做 token 分割：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/parser.png" align="center" style="zoom:40%;" /><p>比如连续的 WordChar，那它就是 BareWord，解析函数在 <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/Lexer.cpp#L61" target="_blank" rel="noopener">Lexer::nextTokenImpl()</a>，解析调用栈：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DB::Lexer::nextTokenImpl() Lexer.cpp:63</span><br><span class="line">DB::Lexer::nextToken() Lexer.cpp:52</span><br><span class="line">DB::Tokens::operator[](unsigned long) TokenIterator.h:36</span><br><span class="line">DB::TokenIterator::get() TokenIterator.h:62</span><br><span class="line">DB::TokenIterator::operator-&gt;() TokenIterator.h:64</span><br><span class="line">DB::tryParseQuery(DB::IParser&amp;, char const*&amp;, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;&amp;, bool, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, bool, unsigned long, unsigned long) parseQuery.cpp:224</span><br><span class="line">DB::parseQueryAndMovePosition(DB::IParser&amp;, char const*&amp;, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, bool, unsigned long, unsigned long) parseQuery.cpp:314</span><br><span class="line">DB::parseQuery(DB::IParser&amp;, char const*, char const*, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, unsigned long, unsigned long) parseQuery.cpp:332</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:272</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:731</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:150</span><br></pre></td></tr></table></figure><h2 id="ast"><a href="#ast" class="headerlink" title="ast"></a><b>ast</b></h2><p> token 是最基础的元组，他们之间没有任何关联，只是一堆生冷的词组与符号，所以我们还需对其进行<b>语法解析</b>，让这些 token 之间建立一定的关系，达到一个可描述的活力。</p><p>ClickHouse 在解每一个 token 的时候，会根据当前的 token 进行状态空间进行预判(parse 返回 true 则进入子状态空间继续)，然后决定状态跳转，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN  -- TokenType::BareWord</span><br></pre></td></tr></table></figure><p>逻辑首先会进入Parsers/ParserQuery.cpp 的 <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserQuery.cpp#L26" target="_blank" rel="noopener">ParserQuery::parseImpl</a> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bool res &#x3D; query_with_output_p.parse(pos, node, expected)</span><br><span class="line">    || insert_p.parse(pos, node, expected)</span><br><span class="line">    || use_p.parse(pos, node, expected)</span><br><span class="line">    || set_role_p.parse(pos, node, expected)</span><br><span class="line">    || set_p.parse(pos, node, expected)</span><br><span class="line">    || system_p.parse(pos, node, expected)</span><br><span class="line">    || create_user_p.parse(pos, node, expected)</span><br><span class="line">    || create_role_p.parse(pos, node, expected)</span><br><span class="line">    || create_quota_p.parse(pos, node, expected)</span><br><span class="line">    || create_row_policy_p.parse(pos, node, expected)</span><br><span class="line">    || create_settings_profile_p.parse(pos, node, expected)</span><br><span class="line">    || drop_access_entity_p.parse(pos, node, expected)</span><br><span class="line">    || grant_p.parse(pos, node, expected);</span><br></pre></td></tr></table></figure><p>这里会对所有 query 类型进行 parse 方法的调用，直到有分支返回 true。</p><p>我们来看<b>第一层</b> query_with_output_p.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserQueryWithOutput.cpp#L31" target="_blank" rel="noopener">Parsers/ParserQueryWithOutput.cpp</a>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">bool parsed &#x3D;</span><br><span class="line">       explain_p.parse(pos, query, expected)</span><br><span class="line">    || select_p.parse(pos, query, expected)</span><br><span class="line">    || show_create_access_entity_p.parse(pos, query, expected)</span><br><span class="line">    || show_tables_p.parse(pos, query, expected)</span><br><span class="line">    || table_p.parse(pos, query, expected)</span><br><span class="line">    || describe_table_p.parse(pos, query, expected)</span><br><span class="line">    || show_processlist_p.parse(pos, query, expected)</span><br><span class="line">    || create_p.parse(pos, query, expected)</span><br><span class="line">    || alter_p.parse(pos, query, expected)</span><br><span class="line">    || rename_p.parse(pos, query, expected)</span><br><span class="line">    || drop_p.parse(pos, query, expected)</span><br><span class="line">    || check_p.parse(pos, query, expected)</span><br><span class="line">    || kill_query_p.parse(pos, query, expected)</span><br><span class="line">    || optimize_p.parse(pos, query, expected)</span><br><span class="line">    || watch_p.parse(pos, query, expected)</span><br><span class="line">    || show_access_p.parse(pos, query, expected)</span><br><span class="line">    || show_access_entities_p.parse(pos, query, expected)</span><br><span class="line">    || show_grants_p.parse(pos, query, expected)</span><br><span class="line">    || show_privileges_p.parse(pos, query, expected</span><br></pre></td></tr></table></figure><p>跳进<b>第二层</b> explain_p.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserExplainQuery.cpp#L10" target="_blank" rel="noopener">ParserExplainQuery::parseImpl</a>状态空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">bool ParserExplainQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ASTExplainQuery::ExplainKind kind;</span><br><span class="line">    bool old_syntax &#x3D; false;</span><br><span class="line"></span><br><span class="line">    ParserKeyword s_ast(&quot;AST&quot;);</span><br><span class="line">    ParserKeyword s_analyze(&quot;ANALYZE&quot;);</span><br><span class="line">    ParserKeyword s_explain(&quot;EXPLAIN&quot;);</span><br><span class="line">    ParserKeyword s_syntax(&quot;SYNTAX&quot;);</span><br><span class="line">    ParserKeyword s_pipeline(&quot;PIPELINE&quot;);</span><br><span class="line">    ParserKeyword s_plan(&quot;PLAN&quot;);</span><br><span class="line"></span><br><span class="line">    ... ...</span><br><span class="line">    else if (s_explain.ignore(pos, expected))</span><br><span class="line">    &#123;</span><br><span class="line">       ... ...</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    ... ...</span><br><span class="line">    </span><br><span class="line">    ParserSelectWithUnionQuery select_p;</span><br><span class="line">    ASTPtr query;</span><br><span class="line">    if (!select_p.parse(pos, query, expected))</span><br><span class="line">        return false;</span><br><span class="line">    ... ...</span><br></pre></td></tr></table></figure><p>s_explain.ignore 方法会进行一个 keyword 解析，解析出 ast node:</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN -- keyword</span><br></pre></td></tr></table></figure><p>跃进<b>第三层</b> select_p.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserSelectWithUnionQuery.cpp#L26" target="_blank" rel="noopener">ParserSelectWithUnionQuery::parseImpl</a>状态空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bool ParserSelectWithUnionQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    ASTPtr list_node;</span><br><span class="line"></span><br><span class="line">    ParserList parser(std::make_unique&lt;ParserUnionQueryElement&gt;(), std::make_unique&lt;ParserKeyword&gt;(&quot;UNION ALL&quot;), false);</span><br><span class="line">    if (!parser.parse(pos, list_node, expected))</span><br><span class="line">        return false;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>parser.parse 里又调用<b>第四层</b> <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ParserSelectQuery.cpp#L24" target="_blank" rel="noopener">ParserSelectQuery::parseImpl</a> 状态空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">bool ParserSelectQuery::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    auto select_query &#x3D; std::make_shared&lt;ASTSelectQuery&gt;();</span><br><span class="line">    node &#x3D; select_query;</span><br><span class="line"></span><br><span class="line">    ParserKeyword s_select(&quot;SELECT&quot;);</span><br><span class="line">    ParserKeyword s_distinct(&quot;DISTINCT&quot;);</span><br><span class="line">    ParserKeyword s_from(&quot;FROM&quot;);</span><br><span class="line">    ParserKeyword s_prewhere(&quot;PREWHERE&quot;);</span><br><span class="line">    ParserKeyword s_where(&quot;WHERE&quot;);</span><br><span class="line">    ParserKeyword s_group_by(&quot;GROUP BY&quot;);</span><br><span class="line">    ParserKeyword s_with(&quot;WITH&quot;);</span><br><span class="line">    ParserKeyword s_totals(&quot;TOTALS&quot;);</span><br><span class="line">    ParserKeyword s_having(&quot;HAVING&quot;);</span><br><span class="line">    ParserKeyword s_order_by(&quot;ORDER BY&quot;);</span><br><span class="line">    ParserKeyword s_limit(&quot;LIMIT&quot;);</span><br><span class="line">    ParserKeyword s_settings(&quot;SETTINGS&quot;);</span><br><span class="line">    ParserKeyword s_by(&quot;BY&quot;);</span><br><span class="line">    ParserKeyword s_rollup(&quot;ROLLUP&quot;);</span><br><span class="line">    ParserKeyword s_cube(&quot;CUBE&quot;);</span><br><span class="line">    ParserKeyword s_top(&quot;TOP&quot;);</span><br><span class="line">    ParserKeyword s_with_ties(&quot;WITH TIES&quot;);</span><br><span class="line">    ParserKeyword s_offset(&quot;OFFSET&quot;);</span><br><span class="line"></span><br><span class="line">    ParserNotEmptyExpressionList exp_list(false);</span><br><span class="line">    ParserNotEmptyExpressionList exp_list_for_with_clause(false);</span><br><span class="line">    ParserNotEmptyExpressionList exp_list_for_select_clause(true);  </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">            if (!exp_list_for_select_clause.parse(pos, select_expression_list, expected))</span><br><span class="line">            return false;</span><br></pre></td></tr></table></figure><p><b>第五层</b> exp_list_for_select_clause.parse <a href="https://github.com/ClickHouse/ClickHouse/blob/558f9c76306ffc4e6add8fd34c2071b64e914103/src/Parsers/ExpressionListParsers.cpp#L520" target="_blank" rel="noopener">ParserExpressionList::parseImpl</a>状态空间继续：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bool ParserExpressionList::parseImpl(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span><br><span class="line">&#123;</span><br><span class="line">    return ParserList(</span><br><span class="line">        std::make_unique&lt;ParserExpressionWithOptionalAlias&gt;(allow_alias_without_as_keyword),</span><br><span class="line">        std::make_unique&lt;ParserToken&gt;(TokenType::Comma))</span><br><span class="line">        .parse(pos, node, expected);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>… … 写不下去个鸟！</p><p>可以发现，ast parser 的时候，预先构造好状态空间，比如 select 的状态空间:</p><ol><li>expression list</li><li>from tables</li><li>where</li><li>group by</li><li>with …</li><li>order by</li><li>limit</li></ol><p>在一个状态空间內，还可以根据 parse 返回的 bool 判断是否继续进入子状态空间，一直递归解析出整个 ast。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>手工 parser 的好处是代码清晰简洁，每个细节可防可控，以及友好的错误处理，改动起来不会一发动全身。<br>缺点是手工成本太高，需要大量的测试来保证其正确性，还需要一些fuzz来保证可靠性。<br>好在ClickHouse 已经实现的比较全面，即使有新的需求，在现有基础上修修补补即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现实生活中的物品一旦被标记为“纯手工打造”，给人的第一感觉就是“上乘之品”，一个字“贵”，比如北京老布鞋。&lt;/p&gt;
&lt;p&gt;但是在计算机世界里，如果有人告诉你 ClickHouse 的 SQL 解析器是纯手工打造的，是不是很惊讶！&lt;br&gt;这个问题引起了不少网友的关注，所以本篇
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（6）MergeTree存储结构</title>
    <link href="https://bohutang.me/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/"/>
    <id>https://bohutang.me/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/</id>
    <published>2020-06-25T16:00:00.000Z</published>
    <updated>2021-03-10T03:14:15.910Z</updated>
    
    <content type="html"><![CDATA[<p><b>最后更新: 2020-09-28</b></p><p>上篇的 <a href="/2020/06/20/clickhouse-and-friends-merge-tree-algo/">存储引擎技术进化与MergeTree</a> 介绍了存储算法的演进。</p><p>存储引擎是一个数据库的底盘，一定要稳和动力澎湃。</p><p>接下来我们将一起来探索下 ClickHouse MergeTree 列式存储引擎，解构下这台“跑车”最重要的部件。</p><p>所有的存储引擎，无论精良与粗制滥造，最终都是要把数据回写到磁盘，来满足存储和索引目的。</p><p>磁盘文件的构造可以说是算法的物理体现，我们甚至可以通过这些存储结构反推出其算法实现。</p><p>所以，要想深入了解一个存储引擎，最好的入手点是它的磁盘存储结构，然后再反观它的读、写机制就会有一种水到渠成的感觉。</p><p>如果这个分析顺序搞反了，会有一种生硬的感觉，网上大部分教程都是这种“生硬”式教学，本文将直击灵魂从最底层谈起，彻底搞明白４个问题：</p><ol><li><p>MergeTree 有哪些文件？</p></li><li><p>MergeTree 数据如何分布？</p></li><li><p>MergeTree 索引如何组织？</p></li><li><p>MergeTree 如何利用索引加速？</p></li></ol><p>话不多说，上表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE default.mt</span><br><span class="line">(</span><br><span class="line">    &#96;a&#96; Int32,</span><br><span class="line">    &#96;b&#96; Int32,</span><br><span class="line">    &#96;c&#96; Int32,</span><br><span class="line">    INDEX &#96;idx_c&#96; (c) TYPE minmax GRANULARITY 1</span><br><span class="line">)</span><br><span class="line">ENGINE &#x3D; MergeTree</span><br><span class="line">PARTITION BY a </span><br><span class="line">ORDER BY b</span><br><span class="line">SETTINGS index_granularity&#x3D;3</span><br></pre></td></tr></table></figure><p>造点数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">insert into default.mt(a,b,c) values(1,1,1);</span><br><span class="line">insert into default.mt(a,b,c) values(5,2,2),(5,3,3);</span><br><span class="line">insert into default.mt(a,b,c) values(3,10,4),(3,9,5),(3,8,6),(3,7,7),(3,6,8),(3,5,9),(3,4,10);</span><br></pre></td></tr></table></figure><h2 id="磁盘文件"><a href="#磁盘文件" class="headerlink" title="磁盘文件"></a><b>磁盘文件</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls ckdatas&#x2F;data&#x2F;default&#x2F;mt&#x2F;</span><br><span class="line">1_4_4_0  3_6_6_0  5_5_5_0  detached  format_version.txt</span><br></pre></td></tr></table></figure><p>可以看到，生成了 3 个数据目录，每个目录在 ClickHouse 里称作一个分区(part)，目录名的前缀正是我们写入时字段 a 的值: 1,3,5，因为表分区是这样定位的：<code>PARTITION BY a</code>。</p><p>现在我们看看 a=3 分区：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls ckdatas&#x2F;data&#x2F;default&#x2F;mt&#x2F;3_6_6_0&#x2F;</span><br><span class="line">a.bin  a.mrk2  b.bin  b.mrk2  c.bin  checksums.txt  c.mrk2  columns.txt  count.txt  minmax_a.idx  partition.dat  primary.idx  skp_idx_idx_c.idx  skp_idx_idx_c.mrk2</span><br></pre></td></tr></table></figure><ul><li>*.bin 是列数据文件，按主键排序(ORDER BY)，这里是按照字段 b 进行排序</li><li>*.mrk2 mark 文件，目的是快速定位 bin 文件数据位置</li><li>minmax_a.idx 分区键 min-max 索引文件，目的是加速分区键 a 查找</li><li>primay.idx 主键索引文件，目的是加速主键 b 查找</li><li>skp_idx_idx_c.* 字段 c 索引文件，目的是加速 c 的查找</li></ul><p>在磁盘上，MergeTree 只有一种物理排序，就是 ORDER BY 的主键序，其他文件(比如 .mrk/.idx)是一种逻辑加速，围绕仅有的一份物理排序，要解决的问题是：</p><p><b>在以字段 b 物理排序上，如何实现字段 a、字段 c 的快速查找？</b></p><p>MergeTree 引擎概括起来很简单：<br>整个数据集通过分区字段被划分为多个物理分区，每个分区內又通过逻辑文件围绕仅有的一种物理排序进行加速查找。</p><h2 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a><b>存储结构</b></h2><h3 id="数据文件"><a href="#数据文件" class="headerlink" title="数据文件"></a><b>数据文件</b></h3><p>对于单个物理分区內的存储结构，首先要明确一点，MergeTree 的数据只有一份：*.bin。</p><p>a.bin 是字段 a 的数据，b.bin 是字段 b 的数据，c.bin 是字段 c 的数据，也就是大家熟悉的列存储。</p><p>各个 bin 文件以 b.bin排序对齐（b 是排序键），如图：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-without-granule.png" align="center" style="zoom:45%;" /><p>这样会有一个比较严重的问题：<br>如果 *.bin 文件较大，即使读取一行数据，也要加载整个 bin 文件，浪费了大量的 IO，没法忍。</p><h3 id="granule"><a href="#granule" class="headerlink" title="granule"></a><b>granule</b></h3><p>高、黑科技来了，ClickHouse MergeTree 把 bin 文件根据颗粒度(GRANULARITY)划分为多个颗粒(granule)，每个 granule 单独压缩存储。</p><p><code>SETTINGS index_granularity=3</code>  表示每 ３ 行数据为一个 granule，分区目前只有 ７ 条数据，所以被划分成 3 个 granule(三个色块)：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-granule.png" align="center" style="zoom:50%;" /><p>为方便读取某个 granule，使用 *.mrk 文件记录每个 granule 的 offset，每个 granule 的 header 里会记录一些元信息，用于读取解析:</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-marker.png" align="center" style="zoom:50%;" /><p>这样，我们就可以根据 ｍark 文件，直接定位到想要的 granule，然后对这个单独的 granule 进行读取、校验。</p><p>目前，我们还有缺少一种映射：每个 mark 与字段值之间的对应，哪些值区间落在 mark0，哪些落在 mark1 …？</p><p>有了这个映射，就可以实现最小化读取 granule 来加速查询：</p><ol><li>根据查询条件确定需要哪些 mark</li><li>根据 mark 读取相应的 granule</li></ol><h3 id="存储排序"><a href="#存储排序" class="headerlink" title="存储排序"></a><b>存储排序</b></h3><p>在了解 MergeTree 索引机制之前，需要明白以下两点：</p><ol><li><p>只有一份全量数据，存储在 *.bin 文件</p></li><li><p>*.bin 按照 ORDER BY 字段降序存储</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-bin-orderby-sort.png" align="center" style="zoom:45%;" /></li></ol><h3 id="稀疏索引"><a href="#稀疏索引" class="headerlink" title="稀疏索引"></a><b>稀疏索引</b></h3><p>因为数据只有一份且只有一种物理排序，MergeTree在索引设计上选择了简单、高效的稀疏索引模式。</p><p>什么是稀疏索引呢？就是从已经排序的全量数据里，间隔性的选取一些点，并记录这些点属于哪个 mark。</p><h4 id="1-primary-index"><a href="#1-primary-index" class="headerlink" title="1. primary index"></a><b>1. primary index</b></h4><p>主键索引，可通过<code>[PRIMARY KEY expr]</code>指定，默认是 ORDER BY 字段值。</p><p>注意 ClickHouse primary index 跟 MySQL primary key 不是一个概念。</p><p>在稀疏点的选择上，取每个 granule 最小值：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-primary-key.png" align="center" style="zoom:45%;" /><h4 id="2-skipping-index"><a href="#2-skipping-index" class="headerlink" title="2. skipping index"></a><b>2. skipping index</b></h4><p>普通索引。</p><p><code>INDEX</code>idx_c<code>(c) TYPE minmax GRANULARITY 1</code> 针对字段 c 创建一个 minmax 模式索引。</p><p><code>GRANULARITY</code> 是稀疏点选择上的 granule 颗粒度，<code>GRANULARITY 1</code> 表示每 1 个 granule 选取一个：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-skipping-index-g1.png" align="center" style="zoom:40%;" /><p>如果定义为<code>GRANULARITY 2</code> ，则 2 个 granule 选取一个：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-skipping-index-g2.png" align="center" style="zoom:40%;" /><h4 id="3-partition-minmax-index"><a href="#3-partition-minmax-index" class="headerlink" title="3. partition minmax index"></a><b>3. partition minmax index</b></h4><p>针对分区键，MergeTree 还会创建一个 min/max 索引，来加速分区选择。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-minmax-idx.png" align="center" style="zoom:40%;" /><h4 id="4-全景图"><a href="#4-全景图" class="headerlink" title="4. 全景图 "></a><b>4. 全景图 </b></h4><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/merge-tree-layout.png" align="center" style="zoom:95%;" /><h2 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a><b>查询优化</b></h2><p>现在熟悉了 MergeTree 的存储结构，我们通过几个查询来体验下。</p><h3 id="1-分区键查询"><a href="#1-分区键查询" class="headerlink" title="1. 分区键查询"></a><b>1. 分区键查询</b></h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from default.mt where a&#x3D;3</span><br></pre></td></tr></table></figure><p>查询会直接根据 <code>a=3</code> 定位到单个分区:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;Debug&gt; InterpreterSelectQuery: MergeTreeWhereOptimizer: condition &quot;a &#x3D; 3&quot; moved to PREWHERE</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Key condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: (column 0 in [3, 3])</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Selected 1 parts by a, 1 parts by key, 3 marks by primary key, 3 marks to read from 1 ranges</span><br><span class="line">┌─a─┬──b─┬──c─┐</span><br><span class="line">│ 3 │  4 │ 10 │</span><br><span class="line">│ 3 │  5 │  9 │</span><br><span class="line">│ 3 │  6 │  8 │</span><br><span class="line">│ 3 │  7 │  7 │</span><br><span class="line">│ 3 │  8 │  6 │</span><br><span class="line">│ 3 │  9 │  5 │</span><br><span class="line">│ 3 │ 10 │  4 │</span><br><span class="line">└───┴────┴────┘</span><br></pre></td></tr></table></figure><h3 id="2-主键索引查询"><a href="#2-主键索引查询" class="headerlink" title="2. 主键索引查询"></a><b>2. 主键索引查询</b></h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from default.mt where b&#x3D;5</span><br></pre></td></tr></table></figure><p>查询会先从 3 个分区读取 prmary.idx，然后定位到只有一个分区符合条件，找到要读取的 mark:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Key condition: (column 0 in [5, 5])</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Selected 3 parts by a, 1 parts by key, 1 marks by primary key, 1 marks to read from 1 ranges</span><br><span class="line">┌─a─┬─b─┬─c─┐</span><br><span class="line">│ 3 │ 5 │ 9 │</span><br><span class="line">└───┴───┴───┘</span><br></pre></td></tr></table></figure><h3 id="3-索引查询"><a href="#3-索引查询" class="headerlink" title="3. 索引查询"></a><b>3. 索引查询</b></h3><p>语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from default.mt where c&#x3D;5</span><br></pre></td></tr></table></figure><p>查询会先从 3 个分区读取 prmary.idx 和 skp_idx_idx_c.idx 进行 granule 过滤（没用的 drop 掉），然后定位到只有 3_x_x_x 分区的一个 granule 符合条件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;Debug&gt; InterpreterSelectQuery: MergeTreeWhereOptimizer: condition &quot;b &#x3D; 5&quot; moved to PREWHERE</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Key condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): MinMax index condition: unknown</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Index &#96;idx_c&#96; has dropped 1 &#x2F; 1 granules.</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Index &#96;idx_c&#96; has dropped 1 &#x2F; 1 granules.</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Index &#96;idx_c&#96; has dropped 2 &#x2F; 3 granules.</span><br><span class="line">&lt;Debug&gt; default.mt (SelectExecutor): Selected 3 parts by a, 1 parts by key, 5 marks by primary key, 1 marks to read from 1 ranges</span><br><span class="line">┌─a─┬─b─┬─c─┐</span><br><span class="line">│ 3 │ 9 │ 5 │</span><br><span class="line">└───┴───┴───┘</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>本文从磁盘存储结构入手，分析 ClickHouse MergeTree 的存储、索引设计。</p><p>只有了解了这些底层机制，我们才好对自己的 SQL 和表结构进行优化，使其执行更加高效。</p><p>ClickHouse MergeTree 设计简单、高效，它首要解决的问题是：在一种物理排序上，如何实现快速查找。</p><p>针对这个问题，ClickHouse使用稀疏索引来解决。</p><p>在官方 roadmap 上，列举了一个有意思的索引方向：Z-Order Indexing，目的是把多个维度编码到一维存储，当我们给出多维度条件的时候，可以快速定位到这个条件点集的空间位置，目前 ClickHouse 针对这个索引设计暂无进展。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;b&gt;最后更新: 2020-09-28&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;上篇的 &lt;a href=&quot;/2020/06/20/clickhouse-and-friends-merge-tree-algo/&quot;&gt;存储引擎技术进化与MergeTree&lt;/a&gt; 介绍了存储算法的演进。&lt;/p&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="lsm" scheme="https://bohutang.me/tags/lsm/"/>
    
      <category term="mergetree" scheme="https://bohutang.me/tags/mergetree/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（5）存储引擎技术进化与MergeTree</title>
    <link href="https://bohutang.me/2020/06/20/clickhouse-and-friends-merge-tree-algo/"/>
    <id>https://bohutang.me/2020/06/20/clickhouse-and-friends-merge-tree-algo/</id>
    <published>2020-06-19T16:00:00.000Z</published>
    <updated>2020-08-11T13:37:05.365Z</updated>
    
    <content type="html"><![CDATA[<p>21 世纪的第二个 10 年，虎哥已经在存储引擎一线奋战近 10 年，由于强大的兴趣驱动，这么多年来几乎不放过 arXiv 上与存储相关的每一篇 paper。<br>尤其是看到带有 draft 的 paper 时，有一种乞丐听到“叮当”响时的愉悦。<br>看paper这玩意就像鉴宝，多数是“赝品”，需要你有“鉴真”的本领，否则今天是张三的算法超越xx，明儿又是王二的硬件提升了yy，让你永远跟不上节奏zz，湮灭在这些没有营养的技术垃圾中，浪费大好青春。</p><p>言归正传，接下来的3篇，跟 ClickHouse 的 MergeTree 引擎有关：<br><b>上篇介绍存储引擎的技术演进史</b>，从”远古”的 B-tree 出发推演到目前主流的技术架构。<br><b><a href="/2020/06/26/clickhouse-and-friends-merge-tree-disk-layout/">中篇会从存储结构介绍 MergeTree 原理</a></b>，对 ClickHouse MergeTree 有一个深入的认识，如何合理设计来进行科学加速。<br><b>下篇会从MergeTree代码出发</b>，看看 ClickHouse MergeTree 如何实现读、写。</p><p>本文为上篇，先来个热身，相信本篇大部分内容对大家来说都比较陌生，很少人写过。</p><h2 id="地位"><a href="#地位" class="headerlink" title="地位"></a><b>地位</b></h2><p>存储引擎(事务型)在一个数据库(DBMS)中的地位如何呢？</p><p>MySQL 的商业成功可以说大部分来自于 InnoDB 引擎，Oracle 收购 InnoDB 比 MySQL 早好几年呢！<br>20年前，能亲手撸一套 <a href="https://en.wikipedia.org/wiki/Algorithms_for_Recovery_and_Isolation_Exploiting_Semantics" target="_blank" rel="noopener">ARIES (Algorithms for Recovery and Isolation Exploiting Semantics)</a> 规范引擎，实力还是相当震撼的，相信 Oracle 收购的不仅是 InnoDB 这个引擎，更重要的是人， InnoDB 作者在哪里，在干什么？！<br>Fork 出来的 MariaDB 这么多年一直找不到自己的灵魂，在 Server 层磨磨蹭蹭可谓是江河日下，只能四处收购碰碰运气，当年 TokuDB 战斗过的 commit 依在，但这些已经是历史了。<br>另，WiredTiger 被 MongoDB 收购并使用，对整个生态所起的作用也是无可估量的，这些发动机引擎对于一辆汽车是非常重要的。</p><p>有人问道，都已经 2020 年了，开发一个存储引擎还这么难吗？不难，但是造出来的未必有 RocksDB 好用？！<br>如大家所见，很多的分布式存储引擎都是基于 RocksDB 研发，可谓短期内还算明智的选择。<br>从工程角度来看，一个 ACID 引擎要打磨的东西非常之多，到处充斥着人力、钱力、耐心的消耗，一种可能是写到一半就停滞了(如 <a href="https://github.com/BohuTANG/nessDB" target="_blank" rel="noopener">nessDB</a>)，还有一种可能是写着写着发现跟xx很像，沃茨法克。<br>当然，这里并不是鼓励大家都去基于 RocksDB 去构建自己的产品，而是要根据自己的情况去做选择。</p><h2 id="B-tree"><a href="#B-tree" class="headerlink" title="B-tree"></a><b>B-tree</b></h2><p>首先要尊称一声大爷，这个大爷年方 50，目前支撑着数据库产业的半壁江山。</p><p>50 年来不变而且人们还没有改变它的意向，这个大爷厉害的很！<br>鉴定一个算法的优劣，有一个学派叫 <b>IO复杂度分析</b>，简单推演真假便知。<br>下面就用此法分析下 B-tree(traditional b-tree) 的 IO 复杂度，对读、写 IO 一目了然，真正明白读为什么快，写为什么慢，如何优化。<br>为了可以愉快的阅读，本文不会做任何公式推导，复杂度分析怎么可能没有公式呢！</p><h3 id="读IO分析"><a href="#读IO分析" class="headerlink" title="读IO分析"></a><b>读IO分析</b></h3><p>这里有一个 3-level 的 B-tree，每个方块代表一个 page，数字代表 page ID。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-read.png" align="center" style="zoom:40%;" /><p>上图 B-tree 结构是<b>内存</b>的一个表现形式，如果我们要读取的记录在 leaf-8上，read-path 如蓝色箭头所示:<br>root-9 –&gt; branch-6 –&gt; leaf-8</p><p>下图是 B-tree 在<b>磁盘</b>上的存储形式，meta page 是起点:<br><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-read-disk.png" align="center" style="zoom:40%;" /></p><p>这样读取的随机 IO (假设内存里没有 page 缓存且 page 存储是随机的)总数就是(蓝色箭头):</p><p>1(meta-10)IO +  1(root-9)IO + 1(branch-6)IO + 1(leaf-8)IO = 4次 IO，这里忽略一直缓存的 meta 和 root，就是 <b>2</b> 次随机 IO。<br>如果磁盘 seek 是 1ms，读取延迟就是 <b>2ms</b>。</p><p>通过推演就会发现，B-tree 是一种读优化(Read-Optimized)的数据结构，无论 LSM-tree 还是 Fractal-tree 等在读上只能比它慢，因为读放大(Read Amplification)问题。<br>存储引擎算法可谓日新月异，但是大部分都是在跟写优化(Write-Optimized)做斗争，那怕是一个常数项的优化那就是突破，自从 Fractal-tree 突破后再无来者了！</p><h3 id="写IO分析"><a href="#写IO分析" class="headerlink" title="写IO分析"></a><b>写IO分析</b></h3><p>现在写一条记录到 leaf-8。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-update-raw.png" align="center" style="zoom:40%;" /><p>可以发现，每次写都需要先读取一遍，如上图蓝色路径所示。</p><p>假设这次写入导致 root, branch 都发生了变化，这种 in-place 的更新反映到磁盘上就是：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-update-raw-disk.png" align="center" style="zoom:40%;" /><p>基本是 <b>2</b> 次读 IO和写 <b>2</b> 次写 IO+WAL fsync，粗略为 <b>4</b> 次随机 IO。<br>通过分析发现，B-tree 对写操作不太友好，随机 IO 次数较多，而且 in-place 更新必须增加一个 page 级的 WAL 保证失败回滚，简直是要命。</p><h3 id="Write-Optimized-B-tree"><a href="#Write-Optimized-B-tree" class="headerlink" title="Write-Optimized B-tree"></a><b>Write-Optimized B-tree</b></h3><p>说到写优化，在机械盘的年代，大家的方向基本是把随机 IO 转换为顺序 IO，充分发挥磁盘的机械优势，于是出现一种 Append-only B-tree：<br><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-aof.png" align="center" style="zoom:40%;" /></p><ol><li>更新生成新的 page(蓝色)</li><li>page 回写磁盘时 append only 到文件末尾</li><li>无需 page WAL，数据不 overwrite，有写放大(Write Amplification)问题，需要做空洞重利用机制</li></ol><p>Append-only B-tree 节省了回写时的 2 次随机 IO，转换为常数级(constant)的1次顺序 IO，写性能大幅提升，总结起来就是：<br><b>随机变顺序，空间换时间</b><br>LSM-tree, Fractal-tree 等写优化算法的核心思想也是这个，只不过其实现机制不同。</p><h2 id="LSM-trees"><a href="#LSM-trees" class="headerlink" title="LSM-trees"></a><b>LSM-trees</b></h2><p>随着 LevelDB 的问世，LSM-tree 逐渐被大家所熟知。<br>LSM-tree 更像一种思想，模糊了 B-tree 里 tree 的严肃性，通过文件组织成一个更加松散的 tree。<br>这里不谈一个具体的 LSM-tree 是 Leveled 还是 Size-tiered，只谈大体思想。</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/lsm-tree.png" align="center" style="zoom:40%;" /><h3 id="写入"><a href="#写入" class="headerlink" title="写入"></a><b>写入</b></h3><ol><li><p>先写入内存的 C0</p></li><li><p>后台线程根据规则(Leveled/Sized)进行 merge，C0 –&gt; C1, C1 –&gt; C2 … CL</p></li><li><p>写入 C0 即可返回，IO 放到后台的 Merge 过程</p></li><li><p>每次 Merge 是硬伤，动作大就抖，动作小性能不好，每次 Merge 的数据流向不明确</p></li><li><p>写放大问题</p></li></ol><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a><b>读取</b></h3><ol><li>读取 C0</li><li>读取 C1 .. CL</li><li>合并记录返回</li><li>读放大问题</li></ol><h2 id="Fractal-tree"><a href="#Fractal-tree" class="headerlink" title="Fractal-tree"></a><b>Fractal-tree</b></h2><p>终于发展到了“终极”优化(目前最先进的索引算法)，Fractal-tree。<br>它是在 Append-only B-tree 的基础上，对每个 branch 节点增加了一个 message buffer 作为缓冲，可以看做是 LSM-tree 和 Append-only B-tree 完美合体。</p><p>相对于 LSM-tree 它的优势非常明显:<br>Merge 更加有序，数据流向非常分明，消除了 Merge 的抖动问题，大家一直寻找的 compaction 防抖方案一直存在的！</p><p>这个高科技目前只有 <a href="https://github.com/xelabs/tokudb" target="_blank" rel="noopener">TokuDB</a> 在使用，这个算法可以开篇新介，这里不做累述，感兴趣的可以参考原型实现 <a href="https://github.com/BohuTANG/nessDB" target="_blank" rel="noopener">nessDB</a>。</p><h2 id="Cache-oblivious"><a href="#Cache-oblivious" class="headerlink" title="Cache-oblivious"></a><b>Cache-oblivious</b></h2><p>这个词对于大部分人都是陌生的，不过别怕。<br>在存储引擎里，有一个数据结构非常非常重要，它负责 page 数据有序性维护，比如在一个 page 里怎么快速定位到我要的记录。<br>在 LevelDB 里使用 skiplist，但大部分引擎使用的是一个有序数组来表示，比如 [1, 2, 3, … 100]，然后使用二分查找。</p><p>大概 10 年前一位内核开发者发表了一篇 &lt;<a href="https://queue.acm.org/detail.cfm?id=1814327" target="_blank" rel="noopener">You’re Doing It Wrong</a>&gt;，这个小文讲了一个很有意思的事情:<br>数据的组织形式对性能有很大的影响，因为 CPU有 cache line。</p><p>抛开这篇文章不谈，咱们来看一张“神仙”图：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/veb-layout.png" align="center" style="zoom:40%;" /><p>这是一个 binary-tree 的 4 种 layout 表示形式，那么哪种 layout 对 CPU cache line 最友好？</p><p>也许你已经猜对了，那就是 van Emde Boas，简称 vEB。<br>因为它的相邻数据“扎堆”存储，point-query 和 range-query 的 cache line 可以最大化共享，skiplist 对 cache line 是非常不友好的，还可以更快！<br>对于 cache oblivious 数据结构，这里有一个简单的原型实现: <a href="https://github.com/BohuTANG/omt" target="_blank" rel="noopener">omt</a></p><h2 id="B-tree优化魔力象限"><a href="#B-tree优化魔力象限" class="headerlink" title="B-tree优化魔力象限"></a><b>B-tree优化魔力象限</b></h2><p>写优化算法从原生的 B-tree 到 Append-only B-tree(代表作 LMDB)，又到 LSM-tree(LevelDB/RocksDB 等)，最后进化到目前最先进的 Fractal-tree (TokuDB)。<br>这些算法耗费了很多年才在工程上实现并被认可，研发一款存储引擎缺的不是算法而是“鉴宝”的能力，这个“宝”可能已经躺了几十年了。</p><p>其实，”科学家”们已经总结出一个 B-tree 优化魔力象限:</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-optimal-curve.png" align="center" style="zoom:50%;" /><p>横坐标是写性能，纵坐标是读性能，B-tree 和 Logging 数据结构分布在曲线的两个极端。<br>B-tree 的读性能非常好，但是写性能差。<br>Logging 的写性能非常好，但是读性能差(想想我们每次写都把数据追加到文件末尾，是不是很快？但是读…)。</p><p>在它们中间有一个优化曲度(Optimal Curve)。<br>在这个曲度上，你可以通过增加/减少一个常数(1-epsilon)来做读和写优化组合，LSM-tree/Fractal-tree 都在这个曲度之上。  </p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/btree-epsilon.png" align="center" style="zoom:20%;" /><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>本文主要讨论事务性引擎的技术演进，其中包含了 IO 复杂度分析，其实这个分析是基于一个 DAM(Disk Access Machine)模型，这里不再展开。<br>这个模型要解决什么问题呢？<br>如果工程中涉及硬件层级关系，比如 Disk / Memory / CPU，数据在Disk，读取(以 block 为单位)到 Memory，查找计算(cache-line)在 CPU，<br>不同介质间性能差距又非常之大，我们怎么做才能让整体性能更优的问题。<br>和当今的硬件相融合，这个模型也一样适用。</p><p>最后回到 ClickHouse 的 MergeTree 引擎，它只使用了本文中的部分优化，实现也比较简洁、高效，毕竟没有事务，撸起来也没啥心理负担。<br><strong>随机变顺序，空间换时间</strong>， MergeTree 原理，请听下回分解。</p><h2 id="References"><a href="#References" class="headerlink" title=" References"></a><b> References</b></h2><p>[1] <a href="https://www.cs.au.dk/~gerth/papers/cacheoblivious05.pdf" target="_blank" rel="noopener">Cache-Oblivious Data Structures</a><br>[2] <a href="https://www3.cs.stonybrook.edu/~bender/talks/2013-BenderKuszmaul-xldb-tutorial.pdf" target="_blank" rel="noopener">Data Structures and Algorithms for Big Databases</a><br>[3] <a href="https://link.springer.com/chapter/10.1007%2F3-540-60220-8_74" target="_blank" rel="noopener">The buffer tree: A new technique for optimal I/O-algorithms</a><br>[4] <a href="http://www.bzero.se/ldapd/btree.html" target="_blank" rel="noopener">how the append-only btree works</a><br>[5] <a href="https://www.douban.com/note/269741273/" target="_blank" rel="noopener">写优化的数据结构(1):AOF和b-tree之间</a><br>[6] <a href="https://www.douban.com/note/269744617/" target="_blank" rel="noopener">写优化的数据结构(2):buffered tree</a><br>[7] <a href="https://www.douban.com/note/304123656/" target="_blank" rel="noopener">存储引擎数据结构优化(1):cpu bound</a><br>[8] <a href="https://www.douban.com/note/304349195/" target="_blank" rel="noopener">存储引擎数据结构优化(2):io bound</a><br>[9] <a href="https://github.com/BohuTANG/nessDB" target="_blank" rel="noopener">nessDB</a><br>[10] <a href="https://github.com/BohuTANG/omt" target="_blank" rel="noopener">omt</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;21 世纪的第二个 10 年，虎哥已经在存储引擎一线奋战近 10 年，由于强大的兴趣驱动，这么多年来几乎不放过 arXiv 上与存储相关的每一篇 paper。&lt;br&gt;尤其是看到带有 draft 的 paper 时，有一种乞丐听到“叮当”响时的愉悦。&lt;br&gt;看paper这玩意
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="lsm" scheme="https://bohutang.me/tags/lsm/"/>
    
      <category term="btree" scheme="https://bohutang.me/tags/btree/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（4）Pipeline处理器和调度器</title>
    <link href="https://bohutang.me/2020/06/11/clickhouse-and-friends-processor/"/>
    <id>https://bohutang.me/2020/06/11/clickhouse-and-friends-processor/</id>
    <published>2020-06-10T16:00:00.000Z</published>
    <updated>2020-08-22T02:41:40.084Z</updated>
    
    <content type="html"><![CDATA[<p><b>最后更新: 2020-08-15</b></p><p>本文谈下 ClickHouse 核心科技：处理器 Processor 和有向无环调度器 DAG Scheduler。</p><p>这些概念并不是 ClickHouse 首创，感兴趣的同学可以关注下 <a href="https://github.com/MaterializeInc/materialize" target="_blank" rel="noopener">materialize</a>的 <a href="https://github.com/TimelyDataflow/timely-dataflow" target="_blank" rel="noopener">timely-dataflow</a>，虎哥用golang 也写过一个<a href="https://github.com/vectorengine/vectorsql/tree/master/src/processors" target="_blank" rel="noopener">原型</a>。</p><p>拼的是实现细节，正是这些模块的精良设计，才有了 ClickHous e整体的高性能。</p><h2 id="Pipeline问题"><a href="#Pipeline问题" class="headerlink" title="Pipeline问题"></a><b>Pipeline问题</b></h2><p>在传统数据库系统中，一个 Query 处理流程大体是:<br><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/processor-plan.png" align="center" style="zoom:50%;" /></p><p>其中在Plan阶段，往往会增加一个 Pipeline 组装(一个 transformer 代表一次数据处理)：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/processor-transformer.png" align="center" style="zoom:50%;" /><p>所有 transformer 被编排成一个流水线(pipeline)，然后交给 executor 串行式执行，每执行一个 transformer 数据集就会被加工并输出，一直到下游的 sinker。<br>可以看到，这种模型的优点是<b>简单</b>，缺点是<b>性能低</b>，无法发挥 CPU 的<b>并行</b>能力，通常叫火山模型(<em>volcano</em>-style)，对于 OLTP 低延迟来说足够，对于计算密集的 OLAP 来说是远远不够的，CPU 不到 100% 就是犯罪！</p><p>对于上面的例子，如果 transformer1 和 transformer2 没有交集，那么它们就可以并行处理：</p><img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/processor-transformer2.png" align="center" style="zoom:50%;" /><p>这样就涉及到一些比较灵魂的问题：</p><ol><li>如何实现 transformer 的灵活编排？</li><li>如何实现 transformer 间的数据同步？</li><li>如何实现 transformer 间的并行调度？</li></ol><h2 id="Processor-和-DAG-Scheduler"><a href="#Processor-和-DAG-Scheduler" class="headerlink" title="Processor 和 DAG Scheduler"></a><b>Processor 和 DAG Scheduler</b></h2><h3 id="1-Transformer-编排"><a href="#1-Transformer-编排" class="headerlink" title="1. Transformer 编排"></a><b>1. Transformer 编排</b></h3><p>ClickHouse 实现了一系列基础 transformer 模块，见 <a href="https://github.com/ClickHouse/ClickHouse/tree/master/src/Processors/Transforms" target="_blank" rel="noopener">src/Processors/Transforms</a>，比如:</p><ul><li>FilterTransform       – WHERE 条件过滤</li><li>SortingTransform   – ORDER BY 排序</li><li>LimitByTransform  – LIMIT 裁剪</li></ul><p>当我们执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM t1 WHERE id&#x3D;1 ORDER BY time DESC LIMIT 10</span><br></pre></td></tr></table></figure><p>对于 ClickHouse 的 QueryPipeline 来说，它会按照以下方式进行编排组装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">QueryPipeline::addSimpleTransform(Source)</span><br><span class="line">QueryPipeline::addSimpleTransform(FilterTransform)</span><br><span class="line">QueryPipeline::addSimpleTransform(SortingTransform)</span><br><span class="line">QueryPipeline::addSimpleTransform(LimitByTransform)</span><br><span class="line">QueryPipeline::addSimpleTransform(Sinker)</span><br></pre></td></tr></table></figure><p>这样就实现了 Transformer 的编排，但是执行时数据如何进行同步呢？</p><h3 id="2-Transformer-数据同步"><a href="#2-Transformer-数据同步" class="headerlink" title="2. Transformer 数据同步"></a><b>2. Transformer 数据同步</b></h3><p>当 QueryPipeline 进行 transformer 编排时，我们还需要进行更加底层的 DAG 连通构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">connect(Source.OutPort, FilterTransform.InPort)</span><br><span class="line">connect(FilterTransform.OutPort, SortingTransform.InPort)</span><br><span class="line">connect(SortingTransform.OutPort, LimitByTransform.InPort)</span><br><span class="line">connect(LimitByTransform.OutPort, Sinker.InPort)</span><br></pre></td></tr></table></figure><p>这样就实现了数据的流向关系，一个 transformer 的 OutPort 对接另外一个的 InPort，就像我们现实中的水管管道一样，接口有 3 通甚至多通。</p><h3 id="3-Transformer-执行调度"><a href="#3-Transformer-执行调度" class="headerlink" title="3. Transformer 执行调度"></a><b>3. Transformer 执行调度</b></h3><p>现在管道组装起来了，那么管道内的水如何进行处理和给压流动呢？</p><p>ClickHouse 定义了一套 transform 状态，processor 根据这些状态来实现调度。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="class"><span class="keyword">class</span> <span class="title">Status</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    NeedData  <span class="comment">// 等待数据流进入</span></span><br><span class="line">    PortFull, <span class="comment">// 管道流出端阻塞</span></span><br><span class="line">    Finished, <span class="comment">// 完成状态，退出</span></span><br><span class="line">    Ready,    <span class="comment">// 切换到 work 函数，进行逻辑处理</span></span><br><span class="line">    Async,    <span class="comment">// 切换到 schedule 函数，进行异步处理</span></span><br><span class="line">    Wait,     <span class="comment">// 等待异步处理</span></span><br><span class="line">    ExpandPipeline,      <span class="comment">// Pipeline 需要裂变</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>当 source 生成数据后，它的状态会设置为 PortFull，意思是等着流入其他 transformer 的 InPort，processor 会开始调度 FilterTransformer(NeedData) 的 Prepare，进行 PullData，然后它的状态设置为 Ready，等待 processor 调度 Work 方法进行数据Filter处理，大家就这样靠状态让 processor 去感知，来调度和做状态迁移，直到 Finished 状态。</p><p>这里值得一提的是 ExpandPipeline 状态，它会根据 transformer 的实现，可以把一个 transformer 裂变出更多个 transformer 并行执行，达到一个爆炸效果。</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a><b>Example</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT number + 1 FROM t1;</span><br></pre></td></tr></table></figure><p>为了更加深入理解 ClickHouse 的 processor 和 scheduler 机制，我们来一个原生态的 example:</p><ol><li>一个 Source:{0,1,2,3,4}</li><li>AdderTransformer 对每个数字做加1操作</li><li>一个 Sinker，输出结果</li></ol><h3 id="1-Source"><a href="#1-Source" class="headerlink" title="1. Source"></a><b>1. Source</b></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySource</span> :</span> <span class="keyword">public</span> ISource</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">String</span> <span class="title">getName</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">"MySource"</span>; &#125;</span><br><span class="line"></span><br><span class="line">    MySource(UInt64 end_)</span><br><span class="line">        : ISource(Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)), <span class="built_in">end</span>(end_)</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    UInt64 <span class="built_in">end</span>;</span><br><span class="line">    <span class="keyword">bool</span> done = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">Chunk <span class="title">generate</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (done)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> Chunk();</span><br><span class="line">        &#125;</span><br><span class="line">        MutableColumns columns;</span><br><span class="line">        columns.emplace_back(ColumnUInt64::create());</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0U</span>; i &lt; <span class="built_in">end</span>; i++)</span><br><span class="line">            columns[<span class="number">0</span>]-&gt;insert(i);</span><br><span class="line"></span><br><span class="line">        done = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">return</span> Chunk(<span class="built_in">std</span>::<span class="built_in">move</span>(columns), <span class="built_in">end</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="2-MyAddTransform"><a href="#2-MyAddTransform" class="headerlink" title="2. MyAddTransform"></a><b>2. MyAddTransform</b></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAddTransformer</span> :</span> <span class="keyword">public</span> IProcessor</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">String</span> <span class="title">getName</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">"MyAddTransformer"</span>; &#125;</span><br><span class="line"></span><br><span class="line">    MyAddTransformer()</span><br><span class="line">        : IProcessor(</span><br><span class="line">            &#123;Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)&#125;,</span><br><span class="line">            &#123;Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)&#125;)</span><br><span class="line">        , input(inputs.front())</span><br><span class="line">        , output(outputs.front())</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">Status <span class="title">prepare</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (output.isFinished())</span><br><span class="line">        &#123;</span><br><span class="line">            input.<span class="built_in">close</span>();</span><br><span class="line">            <span class="keyword">return</span> Status::Finished;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!output.canPush())</span><br><span class="line">        &#123;</span><br><span class="line">            input.setNotNeeded();</span><br><span class="line">            <span class="keyword">return</span> Status::PortFull;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (has_process_data)</span><br><span class="line">        &#123;</span><br><span class="line">            output.push(<span class="built_in">std</span>::<span class="built_in">move</span>(current_chunk));</span><br><span class="line">            has_process_data = <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (input.isFinished())</span><br><span class="line">        &#123;</span><br><span class="line">            output.finish();</span><br><span class="line">            <span class="keyword">return</span> Status::Finished;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!input.hasData())</span><br><span class="line">        &#123;</span><br><span class="line">            input.setNeeded();</span><br><span class="line">            <span class="keyword">return</span> Status::NeedData;</span><br><span class="line">        &#125;</span><br><span class="line">        current_chunk = input.pull(<span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">return</span> Status::Ready;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">work</span><span class="params">()</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">auto</span> num_rows = current_chunk.getNumRows();</span><br><span class="line">        <span class="keyword">auto</span> result_columns = current_chunk.cloneEmptyColumns();</span><br><span class="line">        <span class="keyword">auto</span> columns = current_chunk.detachColumns();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> i = <span class="number">0U</span>; i &lt; num_rows; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">auto</span> val = columns[<span class="number">0</span>]-&gt;getUInt(i);</span><br><span class="line">            result_columns[<span class="number">0</span>]-&gt;insert(val+<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        current_chunk.setColumns(<span class="built_in">std</span>::<span class="built_in">move</span>(result_columns), num_rows);</span><br><span class="line">        has_process_data = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">InputPort &amp; <span class="title">getInputPort</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> input; &#125;</span><br><span class="line">    <span class="function">OutputPort &amp; <span class="title">getOutputPort</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> output; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="keyword">bool</span> has_input = <span class="literal">false</span>;</span><br><span class="line">    <span class="keyword">bool</span> has_process_data = <span class="literal">false</span>;</span><br><span class="line">    Chunk current_chunk;</span><br><span class="line">    InputPort &amp; input;</span><br><span class="line">    OutputPort &amp; output;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="3-MySink"><a href="#3-MySink" class="headerlink" title="3. MySink"></a><b>3. MySink</b></h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySink</span> :</span> <span class="keyword">public</span> ISink</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">String</span> <span class="title">getName</span><span class="params">()</span> <span class="keyword">const</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">"MySinker"</span>; &#125;</span><br><span class="line"></span><br><span class="line">    MySink() : ISink(Block(&#123;ColumnWithTypeAndName&#123;ColumnUInt64::create(), <span class="built_in">std</span>::make_shared&lt;DataTypeUInt64&gt;(), <span class="string">"number"</span>&#125;&#125;)) &#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    WriteBufferFromFileDescriptor out&#123;STDOUT_FILENO&#125;;</span><br><span class="line">    FormatSettings settings;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">consume</span><span class="params">(Chunk chunk)</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">size_t</span> rows = chunk.getNumRows();</span><br><span class="line">        <span class="keyword">size_t</span> columns = chunk.getNumColumns();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> row_num = <span class="number">0</span>; row_num &lt; rows; ++row_num)</span><br><span class="line">        &#123;</span><br><span class="line">            writeString(<span class="string">"prefix-"</span>, out);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">size_t</span> column_num = <span class="number">0</span>; column_num &lt; columns; ++column_num)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span> (column_num != <span class="number">0</span>)</span><br><span class="line">                    writeChar(<span class="string">'\t'</span>, out);</span><br><span class="line">                getPort()</span><br><span class="line">                    .getHeader()</span><br><span class="line">                    .getByPosition(column_num)</span><br><span class="line">                    .type-&gt;serializeAsText(*chunk.getColumns()[column_num], row_num, out, settings);</span><br><span class="line">            &#125;</span><br><span class="line">            writeChar(<span class="string">'\n'</span>, out);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        out.next();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h3 id="4-DAG-Scheduler"><a href="#4-DAG-Scheduler" class="headerlink" title="4. DAG Scheduler"></a><b>4. DAG Scheduler</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">int main(int, char **)</span><br><span class="line">&#123;</span><br><span class="line">    auto source0 &#x3D; std::make_shared&lt;MySource&gt;(5);</span><br><span class="line">    auto add0 &#x3D; std::make_shared&lt;MyAddTransformer&gt;();</span><br><span class="line">    auto sinker0 &#x3D; std::make_shared&lt;MySink&gt;();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;&#x2F; Connect.</span><br><span class="line">    connect(source0-&gt;getPort(), add0-&gt;getInputPort());</span><br><span class="line">    connect(add0-&gt;getOutputPort(), sinker0-&gt;getPort());</span><br><span class="line"></span><br><span class="line">    std::vector&lt;ProcessorPtr&gt; processors &#x3D; &#123;source0, add0, sinker0&#125;;</span><br><span class="line">    PipelineExecutor executor(processors);</span><br><span class="line">    executor.execute(1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从开发者角度看还是比较复杂，状态迁移还需要开发者自己控制，不过 upstream 已经做了大量的基础工作，比如对 source的封装 ISource，对 sink 的封装 ISink，还有一个基础的 ISimpleTransform，让开发者在上层使用 processor 时更加容易，可以积木式搭建出自己想要的 pipeline。</p><p>ClickHouse 的 transformer 数据单元是 Chunk，transformer 对上游 OutPort 流过来的 Chunk 进行加工，然后输出给下游的 InPort，图连通式的流水线并行工作，让 CPU 尽量满负荷工作。</p><p>当一个 SQL 被解析成 AST 后，ClickHouse 根据 AST 构建 Query Plan，然后根据 QueryPlan 构建出 pipeline，最后由  processor 负责调度和执行。<br>目前，ClickHouse 新版本已经默认开启 QueryPipeline，同时这块代码也在不停的迭代。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;b&gt;最后更新: 2020-08-15&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;本文谈下 ClickHouse 核心科技：处理器 Processor 和有向无环调度器 DAG Scheduler。&lt;/p&gt;
&lt;p&gt;这些概念并不是 ClickHouse 首创，感兴趣的同学可以关注下 &lt;a hre
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="processor" scheme="https://bohutang.me/tags/processor/"/>
    
      <category term="pipeline" scheme="https://bohutang.me/tags/pipeline/"/>
    
      <category term="dag scheduler" scheme="https://bohutang.me/tags/dag-scheduler/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（3）MySQL Protocol和Write调用栈</title>
    <link href="https://bohutang.me/2020/06/08/clickhouse-and-friends-mysql-protocol-write-stack/"/>
    <id>https://bohutang.me/2020/06/08/clickhouse-and-friends-mysql-protocol-write-stack/</id>
    <published>2020-06-07T16:00:00.000Z</published>
    <updated>2020-08-11T13:37:05.365Z</updated>
    
    <content type="html"><![CDATA[<p>上篇的<a href="/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/">MySQL Protocol和Read调用</a>里介绍了 ClickHouse 一条查询语句的调用栈，本文继续介绍写的调用栈，开整。</p><h2 id="Write请求"><a href="#Write请求" class="headerlink" title="Write请求"></a><b>Write请求</b></h2><ol><li><p>建表:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE test(a UInt8, b UInt8, c UInt8) ENGINE&#x3D;MergeTree() PARTITION BY (a, b) ORDER BY c;</span><br><span class="line">Query OK, 0 rows affected (0.03 sec)</span><br></pre></td></tr></table></figure></li><li><p>写入数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO test VALUES(1,1,1), (2,2,2);</span><br></pre></td></tr></table></figure></li></ol><h2 id="调用栈分析"><a href="#调用栈分析" class="headerlink" title="调用栈分析"></a><b>调用栈分析</b></h2><h3 id="1-获取存储引擎-OutputStream"><a href="#1-获取存储引擎-OutputStream" class="headerlink" title="1. 获取存储引擎 OutputStream"></a>1. 获取存储引擎 OutputStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DB::StorageMergeTree::write(std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, DB::Context const&amp;) StorageMergeTree.cpp:174</span><br><span class="line">DB::PushingToViewsBlockOutputStream::PushingToViewsBlockOutputStream(std::__1::shared_ptr&lt;DB::IStorage&gt; const&amp;, DB::Context const&amp;, std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, bool) PushingToViewsBlockOutputStream.cpp:110</span><br><span class="line">DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:229</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:364</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:696</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h3 id="2-从-SQL-组装-InputStream"><a href="#2-从-SQL-组装-InputStream" class="headerlink" title="2. 从 SQL 组装 InputStream"></a>2. 从 SQL 组装 InputStream</h3><p><code>(1,1,1), (2,2,2)</code> 如何组装成 inputstream 结构呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DB::InputStreamFromASTInsertQuery::InputStreamFromASTInsertQuery(std::__1::shared_ptr&lt;DB::IAST&gt; const&amp;, DB::ReadBuffer*, </span><br><span class="line">DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:300</span><br><span class="line">DB::executeQueryImpl(char const*, char const*, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) executeQuery.cpp:386</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:150</span><br></pre></td></tr></table></figure><p>然后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res.in &#x3D; std::make_shared&lt;InputStreamFromASTInsertQuery&gt;(query_ptr, nullptr, query_sample_block, context, nullptr);</span><br><span class="line">res.in &#x3D; std::make_shared&lt;NullAndDoCopyBlockInputStream&gt;(res.in, out_streams.at(0));</span><br></pre></td></tr></table></figure><p>通过 NullAndDoCopyBlockInputStream的 copyData 方法构造出 Block：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">DB::ValuesBlockInputFormat::readRow(std::__1::vector&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt;, std::__1::allocator&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt; &gt; &gt;&amp;, unsigned long) ValuesBlockInputFormat.cpp:93</span><br><span class="line">DB::ValuesBlockInputFormat::generate() ValuesBlockInputFormat.cpp:55</span><br><span class="line">DB::ISource::work() ISource.cpp:48</span><br><span class="line">DB::InputStreamFromInputFormat::readImpl() InputStreamFromInputFormat.h:48</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:57</span><br><span class="line">DB::InputStreamFromASTInsertQuery::readImpl() InputStreamFromASTInsertQuery.h:31</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:57</span><br><span class="line">void DB::copyDataImpl&lt;DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::$_0&amp;, void (&amp;)(DB::Block const&amp;)&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::$_0&amp;, void (&amp;)(DB::Block const&amp;)) copyData.cpp:26</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*) copyData.cpp:62</span><br><span class="line">DB::NullAndDoCopyBlockInputStream::readImpl() NullAndDoCopyBlockInputStream.h:47</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:57</span><br><span class="line">void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:26</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:73</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:785</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:313</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:150</span><br></pre></td></tr></table></figure><h3 id="3-组装-OutputStream"><a href="#3-组装-OutputStream" class="headerlink" title="3. 组装 OutputStream"></a>3. 组装 OutputStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DB::InterpreterInsertQuery::execute() InterpreterInsertQuery.cpp:107</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:364</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:696</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><p>组装顺序:</p><ol><li>NullAndDoCopyBlockInputStream</li><li>CountingBlockOutputStream</li><li>AddingDefaultBlockOutputStream</li><li>SquashingBlockOutputStream</li><li>PushingToViewsBlockOutputStream</li><li>MergeTreeBlockOutputStream</li></ol><h3 id="4-写入OutputStream"><a href="#4-写入OutputStream" class="headerlink" title="4. 写入OutputStream"></a>4. 写入OutputStream</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">DB::MergeTreeBlockOutputStream::write(DB::Block const&amp;) MergeTreeBlockOutputStream.cpp:17</span><br><span class="line">DB::PushingToViewsBlockOutputStream::write(DB::Block const&amp;) PushingToViewsBlockOutputStream.cpp:145</span><br><span class="line">DB::SquashingBlockOutputStream::finalize() SquashingBlockOutputStream.cpp:30</span><br><span class="line">DB::SquashingBlockOutputStream::writeSuffix() SquashingBlockOutputStream.cpp:50</span><br><span class="line">DB::AddingDefaultBlockOutputStream::writeSuffix() AddingDefaultBlockOutputStream.cpp:25</span><br><span class="line">DB::CountingBlockOutputStream::writeSuffix() CountingBlockOutputStream.h:37</span><br><span class="line">DB::copyDataImpl&lt;DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*)::&lt;lambda()&gt;&amp;, void (&amp;)(const DB::Block&amp;)&gt;(DB::IBlockInputStream &amp;, DB::IBlockOutputStream &amp;, &lt;lambda()&gt; &amp;, void (&amp;)(const DB::Block &amp;)) copyData.cpp:52</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::atomic&lt;bool&gt;*) copyData.cpp:138</span><br><span class="line">DB::NullAndDoCopyBlockInputStream::readImpl() NullAndDoCopyBlockInputStream.h:57</span><br><span class="line">DB::IBlockInputStream::read() IBlockInputStream.cpp:60</span><br><span class="line">void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:29</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:154</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:748</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><p>通过 copyData 方法，让数据在 OutputStream 间层层透传，一直到 MergeTreeBlockOutputStream。</p><h3 id="5-返回-Client"><a href="#5-返回-Client" class="headerlink" title="5. 返回 Client"></a>5. 返回 Client</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DB::MySQLOutputFormat::finalize() MySQLOutputFormat.cpp:62</span><br><span class="line">DB::IOutputFormat::doWriteSuffix() IOutputFormat.h:78</span><br><span class="line">DB::OutputStreamToOutputFormat::writeSuffix() OutputStreamToOutputFormat.cpp:18</span><br><span class="line">DB::MaterializingBlockOutputStream::writeSuffix() MaterializingBlockOutputStream.h:22</span><br><span class="line">void DB::copyDataImpl&lt;std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;&gt;(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:52</span><br><span class="line">DB::copyData(DB::IBlockInputStream&amp;, DB::IBlockOutputStream&amp;, std::__1::function&lt;bool ()&gt; const&amp;, std::__1::function&lt;void (DB::Block const&amp;)&gt; const&amp;) copyData.cpp:154</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:748</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO test VALUES(1,1,1), (2,2,2);</span><br></pre></td></tr></table></figure><p>首先内核解析 SQL 语句生成 AST，根据 AST 获取 Interpreter：InterpreterInsertQuery。<br>其次 Interpreter 依次添加相应的 OutputStream。<br>然后从 InputStream 读取数据，写入到 OutputStream，stream 会层层渗透，一直写到底层的存储引擎。<br>最后写入到 Socket Output，返回结果。</p><p>ClickHouse 的 OutputStream 编排还是比较复杂，缺少类似 Pipeline 的调度和编排，但是由于模式比较固化，目前看还算清晰。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上篇的&lt;a href=&quot;/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/&quot;&gt;MySQL Protocol和Read调用&lt;/a&gt;里介绍了 ClickHouse 一条查询语句的调用栈，本文继续介绍写的调用
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mysql" scheme="https://bohutang.me/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（2）MySQL Protocol和Read调用栈</title>
    <link href="https://bohutang.me/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/"/>
    <id>https://bohutang.me/2020/06/07/clickhouse-and-friends-mysql-protocol-read-stack/</id>
    <published>2020-06-06T16:00:00.000Z</published>
    <updated>2020-08-11T13:43:41.082Z</updated>
    
    <content type="html"><![CDATA[<p>作为一个 OLAP 的 DBMS 来说，有2个端非常重要：</p><ul><li><p>用户如何方便的链进来，这是入口端</p><ul><li>ClickHouse 除了自己的 client 外，还提供了 MySQL/PG/GRPC/HTTP 等接入方式</li></ul></li><li><p>数据如何方便的挂上去，这是数据源端</p><ul><li>ClickHouse 除了自己的引擎外，还可以挂载 MySQL/Kafka 等外部数据源</li></ul></li></ul><p>这样内外互通，多条朋友多条路，以实现“数据”级的编排能力。</p><p>今天谈的是入口端的 MySQL 协议，也是本系列 ClickHouse 的第一个好朋友，用户可通过 MySQL 客户端或相关 Driver 直接链接到 ClickHouse，进行数据读写等操作。</p><p>本文通过 MySQL的 Query 请求，借用调用栈来了解下 ClickHouse 的数据读取全过程。</p><h2 id="如何实现？"><a href="#如何实现？" class="headerlink" title=" 如何实现？"></a><b> 如何实现？</b></h2><p>入口文件在:<br><a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Server/MySQLHandler.cpp" target="_blank" rel="noopener">MySQLHandler.cpp</a></p><h3 id="握手协议"><a href="#握手协议" class="headerlink" title="握手协议"></a><b>握手协议</b></h3><ol><li>MySQLClient 发送 Greeting 数据报文到 MySQLHandler</li><li>MySQLHandler 回复一个 Greeting-Response 报文</li><li>MySQLClient 发送认证报文</li><li>MySQLHandler 对认证报文进行鉴权，并返回鉴权结果</li></ol><p>MySQL Protocol 实现在: <a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/MySQLProtocol.h" target="_blank" rel="noopener">Core/MySQLProtocol.h</a></p><h3 id="Query请求"><a href="#Query请求" class="headerlink" title="Query请求"></a><b>Query请求</b></h3><p>当认证通过后，就可以进行正常的数据交互了。</p><ol><li><p>当 MySQLClient 发送请求:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; SELECT * FROM system.numbers LIMIT 5;</span><br></pre></td></tr></table></figure></li><li><p>MySQLHandler 的调用栈：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-&gt;MySQLHandler::comQuery -&gt; executeQuery -&gt; pipeline-&gt;execute -&gt; MySQLOutputFormat::consume</span><br></pre></td></tr></table></figure></li><li><p>MySQLClient 接收到结果</p></li></ol><p>在步骤2里，executeQuery(executeQuery.cpp)非常重要。<br>它是所有前端 Server 和 ClickHouse 内核的接入口，第一个参数是 SQL 文本(‘select 1’)，第二个参数是结果集要发送到哪里去(socket net)。</p><h2 id="调用栈分析"><a href="#调用栈分析" class="headerlink" title="调用栈分析"></a><b>调用栈分析</b></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM system.numbers LIMIT 5</span><br></pre></td></tr></table></figure><h3 id="1-获取数据源"><a href="#1-获取数据源" class="headerlink" title="1. 获取数据源"></a>1. 获取数据源</h3><p>StorageSystemNumbers 数据源：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DB::StorageSystemNumbers::read(std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, std::__1::shared_ptr&lt;DB::StorageInMemoryMetadata const&gt; const&amp;, DB::SelectQueryInfo const&amp;, DB::Context const&amp;, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) StorageSystemNumbers.cpp:135</span><br><span class="line">DB::ReadFromStorageStep::ReadFromStorageStep(std::__1::shared_ptr&lt;DB::RWLockImpl::LockHolderImpl&gt;, std::__1::shared_ptr&lt;DB::StorageInMemoryMetadata const&gt;&amp;, DB::SelectQueryOptions, </span><br><span class="line">DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) memory:3028</span><br><span class="line">DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) InterpreterSelectQuery.cpp:1361</span><br><span class="line">DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&amp;, std::__1::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, std::__1::optional&lt;DB::Pipe&gt;) InterpreterSelectQuery.cpp:791</span><br><span class="line">DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&amp;) InterpreterSelectQuery.cpp:472</span><br><span class="line">DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&amp;) InterpreterSelectWithUnionQuery.cpp:183</span><br><span class="line">DB::InterpreterSelectWithUnionQuery::execute() InterpreterSelectWithUnionQuery.cpp:198</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:385</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, </span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><p>这里最主要的是 ReadFromStorageStep 函数，从不同 storage 里获取数据源 pipe:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pipes pipes &#x3D; storage-&gt;read(required_columns, metadata_snapshot, query_info, *context, processing_stage, max_block_size, max_streams);</span><br></pre></td></tr></table></figure><h3 id="2-Pipeline构造"><a href="#2-Pipeline构造" class="headerlink" title="2. Pipeline构造"></a>2. Pipeline构造</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DB::LimitTransform::LimitTransform(DB::Block const&amp;, unsigned long, unsigned long, unsigned long, bool, bool, std::__1::vector&lt;DB::SortColumnDescription, std::__1::allocator&lt;DB::SortColumnDescription&gt; &gt;) LimitTransform.cpp:21</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:2214</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:2299</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:3570</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) memory:4400</span><br><span class="line">DB::LimitStep::transformPipeline(DB::QueryPipeline&amp;) LimitStep.cpp:33</span><br><span class="line">DB::ITransformingStep::updatePipeline(std::__1::vector&lt;std::__1::unique_ptr&lt;DB::QueryPipeline, std::__1::default_delete&lt;DB::QueryPipeline&gt; &gt;, std::__1::allocator&lt;std::__1::unique_ptr&lt;DB::QueryPipeline, std::__1::default_delete&lt;DB::QueryPipeline&gt; &gt; &gt; &gt;) ITransformingStep.cpp:21</span><br><span class="line">DB::QueryPlan::buildQueryPipeline() QueryPlan.cpp:154</span><br><span class="line">DB::InterpreterSelectWithUnionQuery::execute() InterpreterSelectWithUnionQuery.cpp:200</span><br><span class="line">DB::executeQueryImpl(const char *, const char *, DB::Context &amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer *) executeQuery.cpp:385</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:722</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h3 id="3-Pipeline执行"><a href="#3-Pipeline执行" class="headerlink" title="3. Pipeline执行"></a>3. Pipeline执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">DB::LimitTransform::prepare(std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;, std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;) LimitTransform.cpp:67</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:291</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::PipelineExecutor::Edge&amp;, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, unsigned long) PipelineExecutor.cpp:264</span><br><span class="line">DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue&lt;DB::PipelineExecutor::ExecutionState*, std::__1::deque&lt;DB::PipelineExecutor::ExecutionState*, std::__1::allocator&lt;DB::PipelineExecutor::ExecutionState*&gt; &gt; &gt;&amp;, std::__1::unique_lock&lt;std::__1::mutex&gt;) PipelineExecutor.cpp:373</span><br><span class="line">DB::PipelineExecutor::initializeExecution(unsigned long) PipelineExecutor.cpp:747</span><br><span class="line">DB::PipelineExecutor::executeImpl(unsigned long) PipelineExecutor.cpp:764</span><br><span class="line">DB::PipelineExecutor::execute(unsigned long) PipelineExecutor.cpp:479</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:833</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:307</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h3 id="4-Output执行发送"><a href="#4-Output执行发送" class="headerlink" title="4. Output执行发送"></a>4. Output执行发送</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DB::MySQLOutputFormat::consume(DB::Chunk) MySQLOutputFormat.cpp:53</span><br><span class="line">DB::IOutputFormat::work() IOutputFormat.cpp:62</span><br><span class="line">DB::executeJob(DB::IProcessor *) PipelineExecutor.cpp:155</span><br><span class="line">operator() PipelineExecutor.cpp:172</span><br><span class="line">DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic&lt;bool&gt;*) PipelineExecutor.cpp:630</span><br><span class="line">DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) PipelineExecutor.cpp:546</span><br><span class="line">DB::PipelineExecutor::executeImpl(unsigned long) PipelineExecutor.cpp:812</span><br><span class="line">DB::PipelineExecutor::execute(unsigned long) PipelineExecutor.cpp:479</span><br><span class="line">DB::executeQuery(DB::ReadBuffer&amp;, DB::WriteBuffer&amp;, bool, DB::Context&amp;, std::__1::function&lt;void (std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;)&gt;) executeQuery.cpp:800</span><br><span class="line">DB::MySQLHandler::comQuery(DB::ReadBuffer&amp;) MySQLHandler.cpp:311</span><br><span class="line">DB::MySQLHandler::run() MySQLHandler.cpp:141</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><b>总结</b></h2><p>ClickHouse 的模块化比较清晰，像乐高积木一样可以组合拼装，当我们执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM system.numbers LIMIT 5</span><br></pre></td></tr></table></figure><p>首先内核解析 SQL 语句生成 AST，然后根据 AST 获取数据源 Source，pipeline.Add(Source)。<br>其次根据 AST 信息生成 QueryPlan，根据 QueryPlan 再生成相应的 Transform，pipeline.Add(LimitTransform)。<br>然后添加 Output Sink 作为数据发送对象，pipeline.Add(OutputSink)。<br>执行 pipeline, 各个 Transformer 开始工作。</p><p>ClickHouse 的 Transformer 调度系统叫做 Processor，也是决定性能的重要模块，详情见 <a href="/2020/06/11/clickhouse-and-friends-processor/">Pipeline 处理器和调度器</a>。<br>ClickHouse 是一辆手动挡的豪华跑车，免费拥有，海啸们！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作为一个 OLAP 的 DBMS 来说，有2个端非常重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;用户如何方便的链进来，这是入口端&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClickHouse 除了自己的 client 外，还提供了 MySQL/PG/GRPC/HTTP 等接入方式&lt;/li&gt;

      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
      <category term="mysql" scheme="https://bohutang.me/tags/mysql/"/>
    
  </entry>
  
  <entry>
    <title>ClickHouse和他的朋友们（1）编译、开发、测试</title>
    <link href="https://bohutang.me/2020/06/05/clickhouse-and-friends-development/"/>
    <id>https://bohutang.me/2020/06/05/clickhouse-and-friends-development/</id>
    <published>2020-06-04T16:00:00.000Z</published>
    <updated>2020-08-10T07:17:16.084Z</updated>
    
    <content type="html"><![CDATA[<p>一次偶然的机会，和ClickHouse团队做了一次线下沟通，Alexey提到ClickHouse的设计哲学:</p><ol><li>The product must solve actual problem</li><li>And do it better than others</li></ol><p>用工程思维解决商业问题的典范啊！</p><p>对用户来说，他们关心的不是什么天花乱坠、上天入地的高科技，只是需要一个能很好解决自己问题的方案，这在开源社区是非常难得的，靠实力“野蛮式”生长。</p><p>于是，我对这个散发着伏特加味道的利器充满了好奇，并参与到ClickHouse的社区中一探究竟，第一感觉是开放、友好、战斗力强(AK47 vs CK16, ClickHouse 2016年开源)。</p><p>本文先从编译和测试入手，再到如何为社区贡献Patch，希望对那些想参与CK社区的同学有所帮助。</p><h2 id="如何本地编译和测试ClickHouse？"><a href="#如何本地编译和测试ClickHouse？" class="headerlink" title=" 如何本地编译和测试ClickHouse？"></a><b> 如何本地编译和测试ClickHouse？</b></h2><h3 id="源码获取"><a href="#源码获取" class="headerlink" title="源码获取"></a><b>源码获取</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;ClickHouse&#x2F;ClickHouse</span><br></pre></td></tr></table></figure><h3 id="编译准备"><a href="#编译准备" class="headerlink" title="编译准备"></a><b>编译准备</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install build-essential</span><br><span class="line">sudo apt-get install software-properties-common</span><br><span class="line">sudo apt-add-repository ppa:ubuntu-toolchain-r&#x2F;test</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line">sudo apt-get install gcc-9 g++-9 git python ninja-build</span><br><span class="line">sudo snap install cmake</span><br></pre></td></tr></table></figure><h3 id="开始编译"><a href="#开始编译" class="headerlink" title="开始编译"></a><b>开始编译</b></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd ClickHouse</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">export CC&#x3D;gcc-9</span><br><span class="line">export CXX&#x3D;g++-9</span><br><span class="line">cmake ..</span><br><span class="line">ninja</span><br></pre></td></tr></table></figure><h3 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a><b>测试方法</b></h3><p>ClickHouse的测试在官方<a href="https://github.com/ClickHouse/ClickHouse/blob/master/docs/en/development/tests.md" target="_blank" rel="noopener">development/tests</a>文档里有详细的介绍，这里列举3个常用的测试模式：</p><h4 id="1-Functional-Tests"><a href="#1-Functional-Tests" class="headerlink" title="1. Functional Tests"></a>1. Functional Tests</h4><p>  功能测试，主要用于ClickHouse内部功能测试，方式：输入一个sql文件，输出一个result，类似MySQL里的mtr，<a href="https://github.com/ClickHouse/ClickHouse/tree/master/tests/queries" target="_blank" rel="noopener">测试集合</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd tests</span><br><span class="line">.&#x2F;clickhouse-test -c &quot;..&#x2F;build&#x2F;programs&#x2F;clickhouse-client&quot; 00001_select_1</span><br></pre></td></tr></table></figure><h4 id="2-Integration-Tests"><a href="#2-Integration-Tests" class="headerlink" title="2. Integration Tests"></a>2. Integration Tests</h4><p>  集成测试，主要用于涉及第三方服务的测试，比如MySQL/Postgres/MongoDB等，以容器化方式编排调度(pytest)运行，<a href="https://github.com/ClickHouse/ClickHouse/tree/master/tests/integration" target="_blank" rel="noopener">测试集合</a></p><p>  由于涉及模块较多，集成测试环境的搭建有一定的难度，建议使用官方的docker镜像。比如要跑test_mysql_protocol下的集成测试集：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd tests&#x2F;integration</span><br><span class="line">docker pull yandex&#x2F;clickhouse-integration-tests-runner</span><br><span class="line">.&#x2F;runner --binary &#x2F;your&#x2F;ClickHouse&#x2F;build&#x2F;programs&#x2F;clickhouse  --bridge-binary &#x2F;your&#x2F;ClickHouse&#x2F;build&#x2F;programs&#x2F;clickhouse-odbc-bridge --configs-dir &#x2F;your&#x2F;ClickHouse&#x2F;programs&#x2F;server&#x2F; &#39;test_mysql_protocol&#x2F;test.py::test_java_client -ss -vv&#39;</span><br></pre></td></tr></table></figure><h4 id="3-Unit-Tests"><a href="#3-Unit-Tests" class="headerlink" title="3. Unit Tests"></a>3. Unit Tests</h4><p>  单元测试，主要用于代码模块的测试，测试集在各个模块的tests目录，比如: <a href="https://github.com/ClickHouse/ClickHouse/tree/master/src/Core/tests" target="_blank" rel="noopener">Core/tests</a></p><p>  如果大家想了解某个模块是如何工作的，强烈建议去翻翻该模块的tests目录，比如想了解processor的工作机制，跟踪调试 <a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Processors/tests/processors_test.cpp" target="_blank" rel="noopener">Processors/tests/</a> 即可。</p><h2 id="如何给ClickHouse社区提Patch？"><a href="#如何给ClickHouse社区提Patch？" class="headerlink" title=" 如何给ClickHouse社区提Patch？"></a><b> 如何给ClickHouse社区提Patch？</b></h2><h4 id="1-fork"><a href="#1-fork" class="headerlink" title="1. fork"></a>1. fork</h4><p>  首先在自己的github上fork一份ClickHouse代码，比如 <a href="https://github.com/BohuTANG/ClickHouse" target="_blank" rel="noopener">https://github.com/BohuTANG/ClickHouse</a></p><h4 id="2-clone到本地"><a href="#2-clone到本地" class="headerlink" title="2. clone到本地"></a>2. clone到本地</h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;BohuTANG&#x2F;ClickHouse</span><br><span class="line">git checkout -B mysql_replica(branch名字)</span><br></pre></td></tr></table></figure><h4 id="3-创建新的分支"><a href="#3-创建新的分支" class="headerlink" title="3. 创建新的分支"></a>3. 创建新的分支</h4>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -B mysql_replica(branch名字)</span><br></pre></td></tr></table></figure><h4 id="4-功能开发"><a href="#4-功能开发" class="headerlink" title="4. 功能开发"></a>4. 功能开发</h4><p>  开发者可以提交一个Draft Pull Request到官方，github会显示这个Pull Request处于Draft状态，官方是无法Merge的</p><h4 id="5-can-be-testd标签"><a href="#5-can-be-testd标签" class="headerlink" title="5. can be testd标签"></a>5. can be testd标签</h4><p>  等待Upstream打[can be tested]标签，一旦被标记CI狂魔们就强势开跑，跑一轮大概需要几十个小时。<br>  协助开发者发现一些代码Style、编译以及测试等错误，这样开发者就可以在自己的分支不停的迭代、修正。<br>   <img src="https://bohutang-1253727613.cos.ap-beijing.myqcloud.com/posts/github-ck-ci.png"></p><p> 如果只是修改typo，这个标签Upstream通常不会添加。</p><h4 id="6-开发完毕"><a href="#6-开发完毕" class="headerlink" title="6. 开发完毕"></a>6. 开发完毕</h4><p> 开发完成，测试OK，把Draft提升为正式Pull Request，等待Upstraem Review。</p><h4 id="7-Merge到Master"><a href="#7-Merge到Master" class="headerlink" title="7. Merge到Master"></a>7. Merge到Master</h4><p> 如果Upstream通过，你的代码会被Merge到Master，恭喜你成为ClickHouse贡献者</p><h4 id="8-注意事项"><a href="#8-注意事项" class="headerlink" title="8. 注意事项"></a>8. 注意事项</h4><p> ClickHouse Upstream迭代非常快，一定要多关注master分支进度，尽量保持自己的分支代码与master同步。否则Upstream Docker更新，自己的test可能就过不了。</p><p> 建议把<a href="https://github.com/ClickHouse/ClickHouse/tree/master/docs/en/development" target="_blank" rel="noopener">doc/development</a>读一遍。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一次偶然的机会，和ClickHouse团队做了一次线下沟通，Alexey提到ClickHouse的设计哲学:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The product must solve actual problem&lt;/li&gt;
&lt;li&gt;And do it better than 
      
    
    </summary>
    
    
    
      <category term="clickhouse" scheme="https://bohutang.me/tags/clickhouse/"/>
    
      <category term="ClickHouse和他的朋友们" scheme="https://bohutang.me/tags/ClickHouse%E5%92%8C%E4%BB%96%E7%9A%84%E6%9C%8B%E5%8F%8B%E4%BB%AC/"/>
    
  </entry>
  
</feed>
